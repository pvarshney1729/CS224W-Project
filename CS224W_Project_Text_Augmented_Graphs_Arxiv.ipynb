{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS 224W Project: Text Augmented Graphs (OGBN-Arxiv)\n",
        "\n",
        "### This Colab contains code for experiments using various techniques for combining textual and graph information, tested on the OGBN-Arxiv dataset"
      ],
      "metadata": {
        "id": "iCdg3xHL2iC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install PyG and other required libraries"
      ],
      "metadata": {
        "id": "V2VKKHTawc7y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q68cWlLkvbxp",
        "outputId": "d7b10da0-2853-4b9c-8ccf-aebfb8c75ffa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.10/dist-packages (2.1.2+pt21cu121)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.10/dist-packages (0.6.18+pt21cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.23.5)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: ogb in /usr/local/lib/python3.10/dist-packages (1.3.6)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.23.5)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.66.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.7)\n",
            "Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (0.2.2)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (67.7.2)\n",
            "Requirement already satisfied: littleutils in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (0.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2023.3.post1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.10/dist-packages (1.7.2)\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "torch_version = str(torch.__version__)\n",
        "scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "!pip install torch-scatter -f $scatter_src\n",
        "!pip install torch-sparse -f $sparse_src\n",
        "!pip install torch-geometric\n",
        "!pip install ogb\n",
        "!pip install faiss-gpu\n",
        "import torch_geometric\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import GCNConv, SAGEConv\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "import numpy as np\n",
        "import pickle\n",
        "device = f'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = torch.device(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Link the Colab to your Google Drive\n",
        "\n",
        "We use Google Drive to load pre trained LM embeddings and logits. Instructions to download these embeddings are provided later in the Colab\n"
      ],
      "metadata": {
        "id": "_YUCOZKlHqPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ubd42LTCHpy8",
        "outputId": "4976656d-fda2-4eb9-b049-b934d2ef5f6a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add code to load the OGBN Arxiv dataset\n"
      ],
      "metadata": {
        "id": "10BSGFn4xAau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = \"ogbn-arxiv\"\n",
        "class LoadData:\n",
        "    \"\"\"\n",
        "    A class used to load and process graph data using the PyTorch Geometric (PyG) library.\n",
        "    \"\"\"\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"\n",
        "        Loads the graph dataset, applies necessary transformations, and retrieves split indices.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        data: PyG data object\n",
        "            The transformed graph data.\n",
        "        split_idx: dict\n",
        "            A dictionary containing the train, validation, and test split indices.\n",
        "        num_classes: int\n",
        "            The number of classes in the dataset.\n",
        "        \"\"\"\n",
        "        dataset = PygNodePropPredDataset(name=DATASET)\n",
        "        data = dataset[0]\n",
        "        transform = T.Compose([T.ToUndirected(), T.ToSparseTensor()])\n",
        "        data = transform(data)\n",
        "        split_idx = dataset.get_idx_split()\n",
        "        return data, split_idx, dataset.num_classes"
      ],
      "metadata": {
        "id": "-HO3dfR6xJYK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the model training and loss function\n",
        "\n"
      ],
      "metadata": {
        "id": "XqzkxMDZ0-QG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss:\n",
        "    \"\"\"\n",
        "    A class for defining loss computation in neural network training.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def get_loss(self, out, labels, train_idx):\n",
        "        \"\"\"\n",
        "        Computes the cross-entropy loss between the output predictions and the true labels.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        out : torch.Tensor\n",
        "            The output predictions from the neural network model, typically the logits.\n",
        "        labels : torch.Tensor\n",
        "            The true labels for the training data.\n",
        "        train_idx : torch.Tensor or list\n",
        "            The indices of the training data samples.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        torch.Tensor\n",
        "            The computed cross-entropy loss.\n",
        "        \"\"\"\n",
        "        return F.cross_entropy(out, labels[train_idx])\n",
        "\n",
        "\n",
        "def train(model, data, train_idx, optimizer, loss_obj):\n",
        "    \"\"\"\n",
        "    Trains a neural network model for one epoch.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : torch.nn.Module\n",
        "        The neural network model to be trained.\n",
        "    data : object\n",
        "        The data object containing features and adjacency information.\n",
        "    train_idx : torch.Tensor or list\n",
        "        The indices of the training data.\n",
        "    optimizer : torch.optim.Optimizer\n",
        "        The optimizer used for updating model weights.\n",
        "    loss_obj : Loss\n",
        "        An instance of the Loss class to compute the loss.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float\n",
        "        The loss value computed for this training epoch.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.adj_t, train_idx)[train_idx]\n",
        "    loss = loss_obj.get_loss(out, data.y.squeeze(1), train_idx)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "yYWYuh_W1F5t"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the model testing function\n"
      ],
      "metadata": {
        "id": "naBo4IQ01pmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def test(model, data, split_idx, evaluator):\n",
        "    \"\"\"\n",
        "    Evaluates the performance of a trained model on training, validation, and test datasets.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : torch.nn.Module\n",
        "        The neural network model to be evaluated.\n",
        "    data : object\n",
        "        The data object containing features, adjacency information, and labels.\n",
        "    split_idx : dict\n",
        "        A dictionary with keys 'train', 'valid', and 'test' mapping to the respective data indices.\n",
        "    evaluator : object\n",
        "        An object used to evaluate the model's predictions. It must have an 'eval' method.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple: (train_acc, valid_acc, test_acc)\n",
        "        A tuple containing the accuracy on the training, validation, and test sets.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    out = model(data.x, data.adj_t)\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True).cpu()\n",
        "\n",
        "    # Evaluate accuracy on training, validation, and test sets\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['train']],\n",
        "        'y_pred': y_pred[split_idx['train']],\n",
        "    })['acc']\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['valid']],\n",
        "        'y_pred': y_pred[split_idx['valid']],\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['test']],\n",
        "        'y_pred': y_pred[split_idx['test']],\n",
        "    })['acc']\n",
        "\n",
        "    return train_acc, valid_acc, test_acc\n"
      ],
      "metadata": {
        "id": "oPdpg7lm1WYh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Hyperparameters\n",
        "\n",
        "Our code support two models\n",
        "\n",
        "\n",
        "1.   GraphSAGE (torch_geometric.nn.models.GraphSAGE)\n",
        "2.   GCN (torch_geometric.nn.models.GCN)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3kAJ-BLv5d8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparams = {'model' : 'GraphSAGE', 'hidden_layer_size' : 128, 'num_layers' : 3, 'dropout' : 0.5, 'learning_rate' : 1e-3, 'epochs' : 1000}"
      ],
      "metadata": {
        "id": "dCZ9Sgsd5cv_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create an end-to-end training loop\n",
        "\n"
      ],
      "metadata": {
        "id": "QCXVcxlI4AX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(load_data_obj, loss_obj, hyperparams):\n",
        "  \"\"\"\n",
        "    Executes the training loop for a graph neural network using specified hyperparameters.\n",
        "\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    load_data_obj : LoadData\n",
        "        An instance of the LoadData class that is used to load the dataset.\n",
        "    loss_obj : Loss\n",
        "        An instance of the Loss class that defines the loss function to be used during training.\n",
        "    hyperparams : dict\n",
        "        A dictionary containing hyperparameters for the model. Expected keys are:\n",
        "        'model' (str), 'hidden_layer_size' (int), 'num_layers' (int), 'out_channels' (int),\n",
        "        'dropout' (float), 'learning_rate' (float), 'epochs' (int).\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple:\n",
        "        best_test_acc (float): The highest test accuracy achieved during training.\n",
        "        model: The trained model instance.\n",
        "  \"\"\"\n",
        "  data, split_idx, num_classes = load_data_obj.load_data()\n",
        "  if hyperparams['model'] == 'GraphSAGE':\n",
        "    model = torch_geometric.nn.models.GraphSAGE(data.x.shape[1], hidden_channels=hyperparams['hidden_layer_size'],\n",
        "                                              num_layers=hyperparams['num_layers'], out_channels=num_classes, dropout=hyperparams['dropout']).to(device)\n",
        "  else:\n",
        "    model = torch_geometric.nn.models.GCN(data.x.shape[1], hidden_channels=hyperparams['hidden_layer_size'],\n",
        "                                              num_layers=hyperparams['num_layers'], out_channels=num_classes, dropout=hyperparams['dropout']).to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=hyperparams['learning_rate'])\n",
        "  evaluator = Evaluator(name=DATASET)\n",
        "  train_idx = split_idx['train']\n",
        "  model.to(device)\n",
        "  data.to(device)\n",
        "  valid_accs=[]\n",
        "  test_accs=[]\n",
        "  for epoch in range(hyperparams['epochs']):\n",
        "    loss = train(model, data, train_idx, optimizer, loss_obj)\n",
        "    result = test(model, data, split_idx, evaluator)\n",
        "    train_acc, valid_acc, test_acc = result\n",
        "    valid_accs.append(valid_acc)\n",
        "    test_accs.append(test_acc)\n",
        "    print(f'Epoch: {epoch:02d}, '\n",
        "          f'Loss: {loss:.4f}, '\n",
        "          f'Train: {100 * train_acc:.2f}%, '\n",
        "          f'Valid: {100 * valid_acc:.2f}% '\n",
        "          f'Test: {100 * test_acc:.2f}%')\n",
        "  best_test_acc = test_accs[np.argmax(valid_accs)]\n",
        "  print(f'Best Test accuracy is {best_test_acc}')\n",
        "  return best_test_acc, model"
      ],
      "metadata": {
        "id": "YHNzhwxC4GYt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the training loop using the Base Model"
      ],
      "metadata": {
        "id": "yWbO32opnrBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_data=LoadData()\n",
        "loss_obj=Loss()\n",
        "standard_acc, base_model = train_loop(load_data, loss_obj, hyperparams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoPDc6jW956G",
        "outputId": "8ec94a24-f87c-4b3c-aa93-b209d6ceebc5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 00, Loss: 3.6682, Train: 11.00%, Valid: 22.97% Test: 21.56%\n",
            "Epoch: 01, Loss: 3.6248, Train: 11.09%, Valid: 23.00% Test: 21.56%\n",
            "Epoch: 02, Loss: 3.5821, Train: 12.13%, Valid: 23.21% Test: 21.71%\n",
            "Epoch: 03, Loss: 3.5354, Train: 15.53%, Valid: 24.33% Test: 22.50%\n",
            "Epoch: 04, Loss: 3.4821, Train: 21.51%, Valid: 27.00% Test: 24.45%\n",
            "Epoch: 05, Loss: 3.4190, Train: 25.42%, Valid: 28.87% Test: 25.96%\n",
            "Epoch: 06, Loss: 3.3461, Train: 26.96%, Valid: 29.55% Test: 26.56%\n",
            "Epoch: 07, Loss: 3.2713, Train: 27.44%, Valid: 29.69% Test: 26.67%\n",
            "Epoch: 08, Loss: 3.2037, Train: 27.43%, Valid: 29.46% Test: 26.48%\n",
            "Epoch: 09, Loss: 3.1528, Train: 24.90%, Valid: 23.63% Test: 20.96%\n",
            "Epoch: 10, Loss: 3.1300, Train: 18.13%, Valid: 8.02% Test: 6.15%\n",
            "Epoch: 11, Loss: 3.1302, Train: 17.91%, Valid: 7.63% Test: 5.86%\n",
            "Epoch: 12, Loss: 3.1323, Train: 17.91%, Valid: 7.63% Test: 5.86%\n",
            "Epoch: 13, Loss: 3.1219, Train: 17.91%, Valid: 7.63% Test: 5.86%\n",
            "Epoch: 14, Loss: 3.0938, Train: 17.91%, Valid: 7.63% Test: 5.86%\n",
            "Epoch: 15, Loss: 3.0669, Train: 17.95%, Valid: 7.69% Test: 5.90%\n",
            "Epoch: 16, Loss: 3.0378, Train: 19.11%, Valid: 9.83% Test: 7.73%\n",
            "Epoch: 17, Loss: 3.0100, Train: 23.70%, Valid: 20.26% Test: 17.79%\n",
            "Epoch: 18, Loss: 2.9910, Train: 26.46%, Valid: 27.46% Test: 24.91%\n",
            "Epoch: 19, Loss: 2.9759, Train: 27.41%, Valid: 29.37% Test: 26.55%\n",
            "Epoch: 20, Loss: 2.9545, Train: 27.78%, Valid: 29.90% Test: 26.90%\n",
            "Epoch: 21, Loss: 2.9318, Train: 27.94%, Valid: 30.02% Test: 26.98%\n",
            "Epoch: 22, Loss: 2.9061, Train: 27.97%, Valid: 30.04% Test: 27.01%\n",
            "Epoch: 23, Loss: 2.8774, Train: 27.97%, Valid: 30.05% Test: 27.01%\n",
            "Epoch: 24, Loss: 2.8470, Train: 27.95%, Valid: 30.03% Test: 27.00%\n",
            "Epoch: 25, Loss: 2.8199, Train: 27.95%, Valid: 30.02% Test: 27.00%\n",
            "Epoch: 26, Loss: 2.7884, Train: 28.00%, Valid: 30.06% Test: 27.03%\n",
            "Epoch: 27, Loss: 2.7575, Train: 28.10%, Valid: 30.17% Test: 27.08%\n",
            "Epoch: 28, Loss: 2.7275, Train: 28.33%, Valid: 30.30% Test: 27.18%\n",
            "Epoch: 29, Loss: 2.6966, Train: 28.90%, Valid: 30.56% Test: 27.46%\n",
            "Epoch: 30, Loss: 2.6640, Train: 29.99%, Valid: 31.19% Test: 28.05%\n",
            "Epoch: 31, Loss: 2.6315, Train: 31.20%, Valid: 31.92% Test: 28.94%\n",
            "Epoch: 32, Loss: 2.6001, Train: 32.10%, Valid: 32.38% Test: 29.75%\n",
            "Epoch: 33, Loss: 2.5662, Train: 32.68%, Valid: 32.78% Test: 30.24%\n",
            "Epoch: 34, Loss: 2.5358, Train: 33.15%, Valid: 33.15% Test: 30.63%\n",
            "Epoch: 35, Loss: 2.5037, Train: 33.56%, Valid: 33.47% Test: 30.99%\n",
            "Epoch: 36, Loss: 2.4726, Train: 34.10%, Valid: 34.02% Test: 31.47%\n",
            "Epoch: 37, Loss: 2.4475, Train: 34.85%, Valid: 34.74% Test: 32.15%\n",
            "Epoch: 38, Loss: 2.4153, Train: 35.63%, Valid: 35.57% Test: 33.22%\n",
            "Epoch: 39, Loss: 2.3928, Train: 36.38%, Valid: 36.38% Test: 34.35%\n",
            "Epoch: 40, Loss: 2.3634, Train: 37.13%, Valid: 37.28% Test: 35.45%\n",
            "Epoch: 41, Loss: 2.3392, Train: 37.89%, Valid: 38.02% Test: 36.27%\n",
            "Epoch: 42, Loss: 2.3078, Train: 38.79%, Valid: 38.80% Test: 36.95%\n",
            "Epoch: 43, Loss: 2.2858, Train: 39.70%, Valid: 39.51% Test: 37.70%\n",
            "Epoch: 44, Loss: 2.2607, Train: 40.77%, Valid: 40.71% Test: 38.72%\n",
            "Epoch: 45, Loss: 2.2375, Train: 41.83%, Valid: 41.88% Test: 40.02%\n",
            "Epoch: 46, Loss: 2.2115, Train: 42.83%, Valid: 43.01% Test: 41.24%\n",
            "Epoch: 47, Loss: 2.1890, Train: 43.72%, Valid: 44.02% Test: 42.16%\n",
            "Epoch: 48, Loss: 2.1659, Train: 44.57%, Valid: 44.77% Test: 43.12%\n",
            "Epoch: 49, Loss: 2.1435, Train: 45.17%, Valid: 45.35% Test: 44.07%\n",
            "Epoch: 50, Loss: 2.1233, Train: 45.76%, Valid: 46.25% Test: 45.06%\n",
            "Epoch: 51, Loss: 2.1020, Train: 46.41%, Valid: 47.21% Test: 46.29%\n",
            "Epoch: 52, Loss: 2.0815, Train: 47.10%, Valid: 48.36% Test: 47.61%\n",
            "Epoch: 53, Loss: 2.0591, Train: 47.73%, Valid: 49.38% Test: 48.76%\n",
            "Epoch: 54, Loss: 2.0401, Train: 48.15%, Valid: 49.87% Test: 49.28%\n",
            "Epoch: 55, Loss: 2.0212, Train: 48.49%, Valid: 50.28% Test: 49.52%\n",
            "Epoch: 56, Loss: 2.0060, Train: 48.82%, Valid: 50.56% Test: 49.67%\n",
            "Epoch: 57, Loss: 1.9874, Train: 49.18%, Valid: 50.90% Test: 50.07%\n",
            "Epoch: 58, Loss: 1.9689, Train: 49.56%, Valid: 51.40% Test: 50.61%\n",
            "Epoch: 59, Loss: 1.9527, Train: 49.82%, Valid: 51.64% Test: 50.99%\n",
            "Epoch: 60, Loss: 1.9362, Train: 50.04%, Valid: 51.87% Test: 51.19%\n",
            "Epoch: 61, Loss: 1.9265, Train: 50.29%, Valid: 52.14% Test: 51.48%\n",
            "Epoch: 62, Loss: 1.9070, Train: 50.57%, Valid: 52.48% Test: 51.83%\n",
            "Epoch: 63, Loss: 1.8939, Train: 50.82%, Valid: 52.81% Test: 52.19%\n",
            "Epoch: 64, Loss: 1.8813, Train: 51.00%, Valid: 52.98% Test: 52.45%\n",
            "Epoch: 65, Loss: 1.8684, Train: 51.17%, Valid: 53.12% Test: 52.76%\n",
            "Epoch: 66, Loss: 1.8509, Train: 51.43%, Valid: 53.35% Test: 53.06%\n",
            "Epoch: 67, Loss: 1.8410, Train: 51.79%, Valid: 53.73% Test: 53.55%\n",
            "Epoch: 68, Loss: 1.8318, Train: 52.10%, Valid: 54.15% Test: 53.85%\n",
            "Epoch: 69, Loss: 1.8196, Train: 52.38%, Valid: 54.39% Test: 54.01%\n",
            "Epoch: 70, Loss: 1.8035, Train: 52.64%, Valid: 54.63% Test: 54.16%\n",
            "Epoch: 71, Loss: 1.7969, Train: 52.91%, Valid: 54.79% Test: 54.36%\n",
            "Epoch: 72, Loss: 1.7840, Train: 53.21%, Valid: 55.04% Test: 54.70%\n",
            "Epoch: 73, Loss: 1.7704, Train: 53.54%, Valid: 55.46% Test: 55.15%\n",
            "Epoch: 74, Loss: 1.7584, Train: 53.81%, Valid: 55.79% Test: 55.54%\n",
            "Epoch: 75, Loss: 1.7486, Train: 54.07%, Valid: 55.91% Test: 55.68%\n",
            "Epoch: 76, Loss: 1.7383, Train: 54.34%, Valid: 56.07% Test: 55.79%\n",
            "Epoch: 77, Loss: 1.7289, Train: 54.62%, Valid: 56.37% Test: 56.01%\n",
            "Epoch: 78, Loss: 1.7188, Train: 54.90%, Valid: 56.62% Test: 56.40%\n",
            "Epoch: 79, Loss: 1.7074, Train: 55.24%, Valid: 56.98% Test: 56.80%\n",
            "Epoch: 80, Loss: 1.7013, Train: 55.59%, Valid: 57.30% Test: 57.13%\n",
            "Epoch: 81, Loss: 1.6870, Train: 55.83%, Valid: 57.47% Test: 57.27%\n",
            "Epoch: 82, Loss: 1.6823, Train: 56.13%, Valid: 57.66% Test: 57.45%\n",
            "Epoch: 83, Loss: 1.6684, Train: 56.41%, Valid: 57.85% Test: 57.59%\n",
            "Epoch: 84, Loss: 1.6573, Train: 56.67%, Valid: 58.11% Test: 57.84%\n",
            "Epoch: 85, Loss: 1.6470, Train: 56.96%, Valid: 58.38% Test: 58.08%\n",
            "Epoch: 86, Loss: 1.6429, Train: 57.14%, Valid: 58.52% Test: 58.27%\n",
            "Epoch: 87, Loss: 1.6295, Train: 57.31%, Valid: 58.71% Test: 58.42%\n",
            "Epoch: 88, Loss: 1.6245, Train: 57.52%, Valid: 58.83% Test: 58.49%\n",
            "Epoch: 89, Loss: 1.6144, Train: 57.75%, Valid: 59.06% Test: 58.70%\n",
            "Epoch: 90, Loss: 1.6047, Train: 58.02%, Valid: 59.36% Test: 59.00%\n",
            "Epoch: 91, Loss: 1.5997, Train: 58.24%, Valid: 59.61% Test: 59.29%\n",
            "Epoch: 92, Loss: 1.5889, Train: 58.39%, Valid: 59.74% Test: 59.47%\n",
            "Epoch: 93, Loss: 1.5849, Train: 58.57%, Valid: 59.77% Test: 59.56%\n",
            "Epoch: 94, Loss: 1.5829, Train: 58.75%, Valid: 59.92% Test: 59.69%\n",
            "Epoch: 95, Loss: 1.5720, Train: 58.96%, Valid: 60.20% Test: 59.77%\n",
            "Epoch: 96, Loss: 1.5600, Train: 59.14%, Valid: 60.38% Test: 59.88%\n",
            "Epoch: 97, Loss: 1.5560, Train: 59.30%, Valid: 60.42% Test: 60.03%\n",
            "Epoch: 98, Loss: 1.5471, Train: 59.43%, Valid: 60.56% Test: 60.17%\n",
            "Epoch: 99, Loss: 1.5410, Train: 59.60%, Valid: 60.77% Test: 60.46%\n",
            "Epoch: 100, Loss: 1.5353, Train: 59.73%, Valid: 60.94% Test: 60.55%\n",
            "Epoch: 101, Loss: 1.5271, Train: 59.88%, Valid: 61.02% Test: 60.65%\n",
            "Epoch: 102, Loss: 1.5196, Train: 60.02%, Valid: 61.19% Test: 60.67%\n",
            "Epoch: 103, Loss: 1.5136, Train: 60.16%, Valid: 61.27% Test: 60.86%\n",
            "Epoch: 104, Loss: 1.5118, Train: 60.28%, Valid: 61.42% Test: 61.19%\n",
            "Epoch: 105, Loss: 1.5059, Train: 60.47%, Valid: 61.58% Test: 61.32%\n",
            "Epoch: 106, Loss: 1.4954, Train: 60.58%, Valid: 61.74% Test: 61.32%\n",
            "Epoch: 107, Loss: 1.4948, Train: 60.71%, Valid: 61.73% Test: 61.23%\n",
            "Epoch: 108, Loss: 1.4871, Train: 60.82%, Valid: 61.88% Test: 61.47%\n",
            "Epoch: 109, Loss: 1.4800, Train: 60.94%, Valid: 62.09% Test: 61.73%\n",
            "Epoch: 110, Loss: 1.4774, Train: 61.06%, Valid: 62.21% Test: 61.81%\n",
            "Epoch: 111, Loss: 1.4731, Train: 61.17%, Valid: 62.30% Test: 61.82%\n",
            "Epoch: 112, Loss: 1.4655, Train: 61.18%, Valid: 62.33% Test: 61.76%\n",
            "Epoch: 113, Loss: 1.4558, Train: 61.23%, Valid: 62.46% Test: 61.87%\n",
            "Epoch: 114, Loss: 1.4615, Train: 61.39%, Valid: 62.61% Test: 62.02%\n",
            "Epoch: 115, Loss: 1.4498, Train: 61.51%, Valid: 62.85% Test: 62.16%\n",
            "Epoch: 116, Loss: 1.4442, Train: 61.58%, Valid: 62.82% Test: 62.27%\n",
            "Epoch: 117, Loss: 1.4418, Train: 61.68%, Valid: 62.76% Test: 62.23%\n",
            "Epoch: 118, Loss: 1.4339, Train: 61.73%, Valid: 62.86% Test: 62.28%\n",
            "Epoch: 119, Loss: 1.4338, Train: 61.79%, Valid: 63.05% Test: 62.47%\n",
            "Epoch: 120, Loss: 1.4284, Train: 61.84%, Valid: 63.15% Test: 62.62%\n",
            "Epoch: 121, Loss: 1.4264, Train: 61.92%, Valid: 63.12% Test: 62.55%\n",
            "Epoch: 122, Loss: 1.4218, Train: 62.02%, Valid: 63.18% Test: 62.61%\n",
            "Epoch: 123, Loss: 1.4147, Train: 62.09%, Valid: 63.35% Test: 62.73%\n",
            "Epoch: 124, Loss: 1.4083, Train: 62.14%, Valid: 63.48% Test: 62.86%\n",
            "Epoch: 125, Loss: 1.4084, Train: 62.18%, Valid: 63.57% Test: 62.97%\n",
            "Epoch: 126, Loss: 1.4048, Train: 62.25%, Valid: 63.44% Test: 62.85%\n",
            "Epoch: 127, Loss: 1.4022, Train: 62.34%, Valid: 63.42% Test: 62.79%\n",
            "Epoch: 128, Loss: 1.3993, Train: 62.42%, Valid: 63.65% Test: 62.99%\n",
            "Epoch: 129, Loss: 1.3952, Train: 62.42%, Valid: 63.73% Test: 63.07%\n",
            "Epoch: 130, Loss: 1.3901, Train: 62.46%, Valid: 63.75% Test: 63.15%\n",
            "Epoch: 131, Loss: 1.3890, Train: 62.56%, Valid: 63.69% Test: 63.11%\n",
            "Epoch: 132, Loss: 1.3871, Train: 62.63%, Valid: 63.71% Test: 63.14%\n",
            "Epoch: 133, Loss: 1.3837, Train: 62.77%, Valid: 63.88% Test: 63.19%\n",
            "Epoch: 134, Loss: 1.3773, Train: 62.78%, Valid: 63.94% Test: 63.31%\n",
            "Epoch: 135, Loss: 1.3749, Train: 62.78%, Valid: 63.92% Test: 63.44%\n",
            "Epoch: 136, Loss: 1.3706, Train: 62.81%, Valid: 63.89% Test: 63.37%\n",
            "Epoch: 137, Loss: 1.3720, Train: 62.95%, Valid: 63.95% Test: 63.43%\n",
            "Epoch: 138, Loss: 1.3658, Train: 63.06%, Valid: 64.02% Test: 63.48%\n",
            "Epoch: 139, Loss: 1.3626, Train: 63.12%, Valid: 64.06% Test: 63.52%\n",
            "Epoch: 140, Loss: 1.3669, Train: 63.11%, Valid: 64.08% Test: 63.50%\n",
            "Epoch: 141, Loss: 1.3597, Train: 63.13%, Valid: 64.11% Test: 63.56%\n",
            "Epoch: 142, Loss: 1.3555, Train: 63.17%, Valid: 64.18% Test: 63.60%\n",
            "Epoch: 143, Loss: 1.3543, Train: 63.25%, Valid: 64.24% Test: 63.72%\n",
            "Epoch: 144, Loss: 1.3523, Train: 63.31%, Valid: 64.32% Test: 63.78%\n",
            "Epoch: 145, Loss: 1.3504, Train: 63.32%, Valid: 64.41% Test: 63.86%\n",
            "Epoch: 146, Loss: 1.3491, Train: 63.35%, Valid: 64.35% Test: 63.80%\n",
            "Epoch: 147, Loss: 1.3433, Train: 63.36%, Valid: 64.39% Test: 63.76%\n",
            "Epoch: 148, Loss: 1.3440, Train: 63.46%, Valid: 64.48% Test: 63.93%\n",
            "Epoch: 149, Loss: 1.3422, Train: 63.50%, Valid: 64.55% Test: 64.09%\n",
            "Epoch: 150, Loss: 1.3365, Train: 63.55%, Valid: 64.67% Test: 64.27%\n",
            "Epoch: 151, Loss: 1.3370, Train: 63.60%, Valid: 64.71% Test: 64.23%\n",
            "Epoch: 152, Loss: 1.3353, Train: 63.66%, Valid: 64.65% Test: 63.93%\n",
            "Epoch: 153, Loss: 1.3320, Train: 63.71%, Valid: 64.63% Test: 63.85%\n",
            "Epoch: 154, Loss: 1.3315, Train: 63.74%, Valid: 64.68% Test: 64.03%\n",
            "Epoch: 155, Loss: 1.3245, Train: 63.79%, Valid: 64.76% Test: 64.27%\n",
            "Epoch: 156, Loss: 1.3234, Train: 63.82%, Valid: 64.74% Test: 64.29%\n",
            "Epoch: 157, Loss: 1.3242, Train: 63.89%, Valid: 64.81% Test: 64.04%\n",
            "Epoch: 158, Loss: 1.3247, Train: 63.86%, Valid: 64.81% Test: 63.80%\n",
            "Epoch: 159, Loss: 1.3155, Train: 63.94%, Valid: 64.90% Test: 64.13%\n",
            "Epoch: 160, Loss: 1.3163, Train: 64.00%, Valid: 65.06% Test: 64.49%\n",
            "Epoch: 161, Loss: 1.3148, Train: 64.05%, Valid: 65.04% Test: 64.52%\n",
            "Epoch: 162, Loss: 1.3113, Train: 64.13%, Valid: 65.19% Test: 64.46%\n",
            "Epoch: 163, Loss: 1.3082, Train: 64.17%, Valid: 65.13% Test: 64.37%\n",
            "Epoch: 164, Loss: 1.3086, Train: 64.22%, Valid: 65.16% Test: 64.49%\n",
            "Epoch: 165, Loss: 1.3075, Train: 64.26%, Valid: 65.26% Test: 64.70%\n",
            "Epoch: 166, Loss: 1.3043, Train: 64.30%, Valid: 65.26% Test: 64.69%\n",
            "Epoch: 167, Loss: 1.2980, Train: 64.35%, Valid: 65.29% Test: 64.57%\n",
            "Epoch: 168, Loss: 1.3002, Train: 64.38%, Valid: 65.27% Test: 64.49%\n",
            "Epoch: 169, Loss: 1.2993, Train: 64.40%, Valid: 65.25% Test: 64.50%\n",
            "Epoch: 170, Loss: 1.3019, Train: 64.47%, Valid: 65.34% Test: 64.59%\n",
            "Epoch: 171, Loss: 1.2983, Train: 64.48%, Valid: 65.47% Test: 64.83%\n",
            "Epoch: 172, Loss: 1.2955, Train: 64.54%, Valid: 65.45% Test: 64.82%\n",
            "Epoch: 173, Loss: 1.2937, Train: 64.60%, Valid: 65.46% Test: 64.64%\n",
            "Epoch: 174, Loss: 1.2938, Train: 64.68%, Valid: 65.46% Test: 64.57%\n",
            "Epoch: 175, Loss: 1.2906, Train: 64.73%, Valid: 65.48% Test: 64.58%\n",
            "Epoch: 176, Loss: 1.2871, Train: 64.78%, Valid: 65.60% Test: 64.80%\n",
            "Epoch: 177, Loss: 1.2862, Train: 64.75%, Valid: 65.66% Test: 65.07%\n",
            "Epoch: 178, Loss: 1.2870, Train: 64.84%, Valid: 65.71% Test: 65.10%\n",
            "Epoch: 179, Loss: 1.2836, Train: 64.92%, Valid: 65.75% Test: 64.94%\n",
            "Epoch: 180, Loss: 1.2825, Train: 64.98%, Valid: 65.71% Test: 64.76%\n",
            "Epoch: 181, Loss: 1.2818, Train: 64.96%, Valid: 65.71% Test: 64.83%\n",
            "Epoch: 182, Loss: 1.2764, Train: 65.00%, Valid: 65.72% Test: 65.04%\n",
            "Epoch: 183, Loss: 1.2774, Train: 65.08%, Valid: 65.82% Test: 65.14%\n",
            "Epoch: 184, Loss: 1.2735, Train: 65.11%, Valid: 65.94% Test: 65.26%\n",
            "Epoch: 185, Loss: 1.2747, Train: 65.18%, Valid: 65.99% Test: 65.32%\n",
            "Epoch: 186, Loss: 1.2733, Train: 65.21%, Valid: 66.04% Test: 65.24%\n",
            "Epoch: 187, Loss: 1.2718, Train: 65.19%, Valid: 65.94% Test: 65.17%\n",
            "Epoch: 188, Loss: 1.2715, Train: 65.24%, Valid: 65.95% Test: 65.15%\n",
            "Epoch: 189, Loss: 1.2682, Train: 65.30%, Valid: 65.94% Test: 65.14%\n",
            "Epoch: 190, Loss: 1.2677, Train: 65.33%, Valid: 66.08% Test: 65.25%\n",
            "Epoch: 191, Loss: 1.2649, Train: 65.37%, Valid: 66.16% Test: 65.35%\n",
            "Epoch: 192, Loss: 1.2649, Train: 65.37%, Valid: 66.18% Test: 65.37%\n",
            "Epoch: 193, Loss: 1.2652, Train: 65.43%, Valid: 66.16% Test: 65.29%\n",
            "Epoch: 194, Loss: 1.2616, Train: 65.50%, Valid: 66.18% Test: 65.32%\n",
            "Epoch: 195, Loss: 1.2602, Train: 65.53%, Valid: 66.27% Test: 65.43%\n",
            "Epoch: 196, Loss: 1.2598, Train: 65.60%, Valid: 66.32% Test: 65.49%\n",
            "Epoch: 197, Loss: 1.2580, Train: 65.58%, Valid: 66.31% Test: 65.45%\n",
            "Epoch: 198, Loss: 1.2595, Train: 65.66%, Valid: 66.36% Test: 65.50%\n",
            "Epoch: 199, Loss: 1.2500, Train: 65.69%, Valid: 66.41% Test: 65.61%\n",
            "Epoch: 200, Loss: 1.2492, Train: 65.72%, Valid: 66.36% Test: 65.64%\n",
            "Epoch: 201, Loss: 1.2522, Train: 65.74%, Valid: 66.35% Test: 65.43%\n",
            "Epoch: 202, Loss: 1.2509, Train: 65.78%, Valid: 66.33% Test: 65.41%\n",
            "Epoch: 203, Loss: 1.2437, Train: 65.87%, Valid: 66.46% Test: 65.56%\n",
            "Epoch: 204, Loss: 1.2490, Train: 65.86%, Valid: 66.56% Test: 65.75%\n",
            "Epoch: 205, Loss: 1.2462, Train: 65.86%, Valid: 66.61% Test: 65.76%\n",
            "Epoch: 206, Loss: 1.2459, Train: 65.94%, Valid: 66.55% Test: 65.72%\n",
            "Epoch: 207, Loss: 1.2438, Train: 65.99%, Valid: 66.57% Test: 65.65%\n",
            "Epoch: 208, Loss: 1.2416, Train: 65.97%, Valid: 66.64% Test: 65.64%\n",
            "Epoch: 209, Loss: 1.2420, Train: 66.04%, Valid: 66.72% Test: 65.81%\n",
            "Epoch: 210, Loss: 1.2388, Train: 66.07%, Valid: 66.78% Test: 65.94%\n",
            "Epoch: 211, Loss: 1.2383, Train: 66.07%, Valid: 66.74% Test: 65.90%\n",
            "Epoch: 212, Loss: 1.2353, Train: 66.13%, Valid: 66.73% Test: 65.80%\n",
            "Epoch: 213, Loss: 1.2348, Train: 66.17%, Valid: 66.63% Test: 65.70%\n",
            "Epoch: 214, Loss: 1.2339, Train: 66.25%, Valid: 66.69% Test: 65.75%\n",
            "Epoch: 215, Loss: 1.2327, Train: 66.26%, Valid: 66.83% Test: 65.95%\n",
            "Epoch: 216, Loss: 1.2263, Train: 66.26%, Valid: 66.83% Test: 65.97%\n",
            "Epoch: 217, Loss: 1.2281, Train: 66.30%, Valid: 66.85% Test: 65.94%\n",
            "Epoch: 218, Loss: 1.2230, Train: 66.34%, Valid: 66.86% Test: 65.94%\n",
            "Epoch: 219, Loss: 1.2250, Train: 66.31%, Valid: 66.89% Test: 65.95%\n",
            "Epoch: 220, Loss: 1.2273, Train: 66.36%, Valid: 66.93% Test: 66.00%\n",
            "Epoch: 221, Loss: 1.2260, Train: 66.42%, Valid: 66.97% Test: 66.06%\n",
            "Epoch: 222, Loss: 1.2230, Train: 66.47%, Valid: 67.00% Test: 66.04%\n",
            "Epoch: 223, Loss: 1.2251, Train: 66.53%, Valid: 66.87% Test: 65.88%\n",
            "Epoch: 224, Loss: 1.2234, Train: 66.52%, Valid: 67.02% Test: 66.02%\n",
            "Epoch: 225, Loss: 1.2221, Train: 66.51%, Valid: 67.13% Test: 66.21%\n",
            "Epoch: 226, Loss: 1.2166, Train: 66.53%, Valid: 67.19% Test: 66.23%\n",
            "Epoch: 227, Loss: 1.2161, Train: 66.61%, Valid: 67.04% Test: 66.09%\n",
            "Epoch: 228, Loss: 1.2177, Train: 66.63%, Valid: 66.99% Test: 66.00%\n",
            "Epoch: 229, Loss: 1.2176, Train: 66.63%, Valid: 67.10% Test: 66.04%\n",
            "Epoch: 230, Loss: 1.2119, Train: 66.63%, Valid: 67.18% Test: 66.23%\n",
            "Epoch: 231, Loss: 1.2155, Train: 66.64%, Valid: 67.19% Test: 66.24%\n",
            "Epoch: 232, Loss: 1.2133, Train: 66.67%, Valid: 67.16% Test: 66.13%\n",
            "Epoch: 233, Loss: 1.2117, Train: 66.73%, Valid: 67.11% Test: 66.06%\n",
            "Epoch: 234, Loss: 1.2085, Train: 66.76%, Valid: 67.17% Test: 66.14%\n",
            "Epoch: 235, Loss: 1.2087, Train: 66.79%, Valid: 67.29% Test: 66.37%\n",
            "Epoch: 236, Loss: 1.2096, Train: 66.79%, Valid: 67.36% Test: 66.51%\n",
            "Epoch: 237, Loss: 1.2092, Train: 66.83%, Valid: 67.32% Test: 66.42%\n",
            "Epoch: 238, Loss: 1.2104, Train: 66.87%, Valid: 67.23% Test: 66.34%\n",
            "Epoch: 239, Loss: 1.2036, Train: 66.87%, Valid: 67.29% Test: 66.28%\n",
            "Epoch: 240, Loss: 1.2040, Train: 66.88%, Valid: 67.32% Test: 66.38%\n",
            "Epoch: 241, Loss: 1.2046, Train: 66.94%, Valid: 67.37% Test: 66.43%\n",
            "Epoch: 242, Loss: 1.2041, Train: 66.99%, Valid: 67.34% Test: 66.48%\n",
            "Epoch: 243, Loss: 1.2048, Train: 67.04%, Valid: 67.36% Test: 66.51%\n",
            "Epoch: 244, Loss: 1.2019, Train: 67.07%, Valid: 67.39% Test: 66.46%\n",
            "Epoch: 245, Loss: 1.1972, Train: 67.06%, Valid: 67.46% Test: 66.44%\n",
            "Epoch: 246, Loss: 1.2010, Train: 67.10%, Valid: 67.44% Test: 66.50%\n",
            "Epoch: 247, Loss: 1.1953, Train: 67.12%, Valid: 67.45% Test: 66.55%\n",
            "Epoch: 248, Loss: 1.1975, Train: 67.16%, Valid: 67.50% Test: 66.55%\n",
            "Epoch: 249, Loss: 1.1958, Train: 67.21%, Valid: 67.50% Test: 66.44%\n",
            "Epoch: 250, Loss: 1.1952, Train: 67.18%, Valid: 67.49% Test: 66.45%\n",
            "Epoch: 251, Loss: 1.1948, Train: 67.17%, Valid: 67.47% Test: 66.55%\n",
            "Epoch: 252, Loss: 1.1903, Train: 67.21%, Valid: 67.54% Test: 66.60%\n",
            "Epoch: 253, Loss: 1.1924, Train: 67.28%, Valid: 67.66% Test: 66.66%\n",
            "Epoch: 254, Loss: 1.1930, Train: 67.30%, Valid: 67.70% Test: 66.75%\n",
            "Epoch: 255, Loss: 1.1923, Train: 67.26%, Valid: 67.74% Test: 66.79%\n",
            "Epoch: 256, Loss: 1.1875, Train: 67.27%, Valid: 67.65% Test: 66.72%\n",
            "Epoch: 257, Loss: 1.1877, Train: 67.35%, Valid: 67.56% Test: 66.58%\n",
            "Epoch: 258, Loss: 1.1864, Train: 67.42%, Valid: 67.64% Test: 66.65%\n",
            "Epoch: 259, Loss: 1.1861, Train: 67.45%, Valid: 67.78% Test: 66.80%\n",
            "Epoch: 260, Loss: 1.1860, Train: 67.46%, Valid: 67.78% Test: 66.90%\n",
            "Epoch: 261, Loss: 1.1854, Train: 67.46%, Valid: 67.71% Test: 66.84%\n",
            "Epoch: 262, Loss: 1.1845, Train: 67.51%, Valid: 67.63% Test: 66.61%\n",
            "Epoch: 263, Loss: 1.1845, Train: 67.51%, Valid: 67.66% Test: 66.65%\n",
            "Epoch: 264, Loss: 1.1823, Train: 67.50%, Valid: 67.82% Test: 66.83%\n",
            "Epoch: 265, Loss: 1.1822, Train: 67.57%, Valid: 67.88% Test: 66.92%\n",
            "Epoch: 266, Loss: 1.1818, Train: 67.56%, Valid: 67.81% Test: 66.86%\n",
            "Epoch: 267, Loss: 1.1784, Train: 67.59%, Valid: 67.76% Test: 66.88%\n",
            "Epoch: 268, Loss: 1.1811, Train: 67.59%, Valid: 67.83% Test: 66.91%\n",
            "Epoch: 269, Loss: 1.1774, Train: 67.62%, Valid: 67.82% Test: 66.91%\n",
            "Epoch: 270, Loss: 1.1799, Train: 67.66%, Valid: 67.83% Test: 66.84%\n",
            "Epoch: 271, Loss: 1.1799, Train: 67.71%, Valid: 67.83% Test: 66.84%\n",
            "Epoch: 272, Loss: 1.1726, Train: 67.72%, Valid: 67.91% Test: 66.93%\n",
            "Epoch: 273, Loss: 1.1771, Train: 67.71%, Valid: 68.05% Test: 67.06%\n",
            "Epoch: 274, Loss: 1.1761, Train: 67.72%, Valid: 68.00% Test: 67.02%\n",
            "Epoch: 275, Loss: 1.1788, Train: 67.76%, Valid: 67.91% Test: 66.95%\n",
            "Epoch: 276, Loss: 1.1708, Train: 67.82%, Valid: 67.90% Test: 66.78%\n",
            "Epoch: 277, Loss: 1.1699, Train: 67.83%, Valid: 67.92% Test: 66.82%\n",
            "Epoch: 278, Loss: 1.1716, Train: 67.82%, Valid: 67.96% Test: 66.89%\n",
            "Epoch: 279, Loss: 1.1692, Train: 67.87%, Valid: 68.02% Test: 67.02%\n",
            "Epoch: 280, Loss: 1.1700, Train: 67.90%, Valid: 67.99% Test: 67.05%\n",
            "Epoch: 281, Loss: 1.1674, Train: 67.92%, Valid: 68.06% Test: 67.04%\n",
            "Epoch: 282, Loss: 1.1719, Train: 67.89%, Valid: 68.11% Test: 67.16%\n",
            "Epoch: 283, Loss: 1.1675, Train: 67.92%, Valid: 68.17% Test: 67.12%\n",
            "Epoch: 284, Loss: 1.1653, Train: 67.96%, Valid: 67.97% Test: 66.97%\n",
            "Epoch: 285, Loss: 1.1669, Train: 68.03%, Valid: 67.96% Test: 66.89%\n",
            "Epoch: 286, Loss: 1.1658, Train: 68.04%, Valid: 68.12% Test: 67.15%\n",
            "Epoch: 287, Loss: 1.1631, Train: 68.01%, Valid: 68.14% Test: 67.22%\n",
            "Epoch: 288, Loss: 1.1630, Train: 68.04%, Valid: 68.20% Test: 67.18%\n",
            "Epoch: 289, Loss: 1.1643, Train: 68.09%, Valid: 68.06% Test: 67.02%\n",
            "Epoch: 290, Loss: 1.1615, Train: 68.11%, Valid: 68.07% Test: 66.89%\n",
            "Epoch: 291, Loss: 1.1647, Train: 68.08%, Valid: 68.09% Test: 67.01%\n",
            "Epoch: 292, Loss: 1.1607, Train: 68.09%, Valid: 68.17% Test: 67.21%\n",
            "Epoch: 293, Loss: 1.1593, Train: 68.14%, Valid: 68.17% Test: 67.12%\n",
            "Epoch: 294, Loss: 1.1633, Train: 68.15%, Valid: 68.14% Test: 67.09%\n",
            "Epoch: 295, Loss: 1.1583, Train: 68.16%, Valid: 68.25% Test: 67.23%\n",
            "Epoch: 296, Loss: 1.1589, Train: 68.15%, Valid: 68.26% Test: 67.29%\n",
            "Epoch: 297, Loss: 1.1582, Train: 68.18%, Valid: 68.34% Test: 67.35%\n",
            "Epoch: 298, Loss: 1.1582, Train: 68.23%, Valid: 68.31% Test: 67.25%\n",
            "Epoch: 299, Loss: 1.1557, Train: 68.24%, Valid: 68.29% Test: 67.31%\n",
            "Epoch: 300, Loss: 1.1580, Train: 68.26%, Valid: 68.34% Test: 67.27%\n",
            "Epoch: 301, Loss: 1.1578, Train: 68.28%, Valid: 68.32% Test: 67.28%\n",
            "Epoch: 302, Loss: 1.1554, Train: 68.30%, Valid: 68.32% Test: 67.23%\n",
            "Epoch: 303, Loss: 1.1563, Train: 68.29%, Valid: 68.23% Test: 67.16%\n",
            "Epoch: 304, Loss: 1.1564, Train: 68.32%, Valid: 68.32% Test: 67.20%\n",
            "Epoch: 305, Loss: 1.1543, Train: 68.32%, Valid: 68.35% Test: 67.25%\n",
            "Epoch: 306, Loss: 1.1546, Train: 68.34%, Valid: 68.31% Test: 67.23%\n",
            "Epoch: 307, Loss: 1.1532, Train: 68.37%, Valid: 68.48% Test: 67.35%\n",
            "Epoch: 308, Loss: 1.1488, Train: 68.34%, Valid: 68.43% Test: 67.38%\n",
            "Epoch: 309, Loss: 1.1503, Train: 68.33%, Valid: 68.43% Test: 67.41%\n",
            "Epoch: 310, Loss: 1.1517, Train: 68.34%, Valid: 68.34% Test: 67.37%\n",
            "Epoch: 311, Loss: 1.1482, Train: 68.39%, Valid: 68.40% Test: 67.29%\n",
            "Epoch: 312, Loss: 1.1520, Train: 68.46%, Valid: 68.45% Test: 67.34%\n",
            "Epoch: 313, Loss: 1.1479, Train: 68.46%, Valid: 68.46% Test: 67.36%\n",
            "Epoch: 314, Loss: 1.1498, Train: 68.44%, Valid: 68.50% Test: 67.53%\n",
            "Epoch: 315, Loss: 1.1480, Train: 68.45%, Valid: 68.44% Test: 67.48%\n",
            "Epoch: 316, Loss: 1.1498, Train: 68.44%, Valid: 68.40% Test: 67.40%\n",
            "Epoch: 317, Loss: 1.1474, Train: 68.51%, Valid: 68.48% Test: 67.33%\n",
            "Epoch: 318, Loss: 1.1457, Train: 68.54%, Valid: 68.50% Test: 67.37%\n",
            "Epoch: 319, Loss: 1.1484, Train: 68.53%, Valid: 68.50% Test: 67.50%\n",
            "Epoch: 320, Loss: 1.1464, Train: 68.49%, Valid: 68.44% Test: 67.47%\n",
            "Epoch: 321, Loss: 1.1460, Train: 68.55%, Valid: 68.49% Test: 67.45%\n",
            "Epoch: 322, Loss: 1.1416, Train: 68.57%, Valid: 68.49% Test: 67.40%\n",
            "Epoch: 323, Loss: 1.1429, Train: 68.58%, Valid: 68.59% Test: 67.46%\n",
            "Epoch: 324, Loss: 1.1414, Train: 68.58%, Valid: 68.55% Test: 67.55%\n",
            "Epoch: 325, Loss: 1.1401, Train: 68.57%, Valid: 68.55% Test: 67.62%\n",
            "Epoch: 326, Loss: 1.1426, Train: 68.62%, Valid: 68.56% Test: 67.54%\n",
            "Epoch: 327, Loss: 1.1388, Train: 68.64%, Valid: 68.43% Test: 67.38%\n",
            "Epoch: 328, Loss: 1.1399, Train: 68.64%, Valid: 68.57% Test: 67.54%\n",
            "Epoch: 329, Loss: 1.1380, Train: 68.65%, Valid: 68.55% Test: 67.68%\n",
            "Epoch: 330, Loss: 1.1363, Train: 68.66%, Valid: 68.57% Test: 67.71%\n",
            "Epoch: 331, Loss: 1.1390, Train: 68.73%, Valid: 68.52% Test: 67.62%\n",
            "Epoch: 332, Loss: 1.1363, Train: 68.74%, Valid: 68.54% Test: 67.52%\n",
            "Epoch: 333, Loss: 1.1359, Train: 68.76%, Valid: 68.61% Test: 67.57%\n",
            "Epoch: 334, Loss: 1.1333, Train: 68.76%, Valid: 68.62% Test: 67.64%\n",
            "Epoch: 335, Loss: 1.1377, Train: 68.73%, Valid: 68.64% Test: 67.71%\n",
            "Epoch: 336, Loss: 1.1357, Train: 68.76%, Valid: 68.59% Test: 67.54%\n",
            "Epoch: 337, Loss: 1.1354, Train: 68.80%, Valid: 68.58% Test: 67.53%\n",
            "Epoch: 338, Loss: 1.1363, Train: 68.80%, Valid: 68.62% Test: 67.60%\n",
            "Epoch: 339, Loss: 1.1329, Train: 68.84%, Valid: 68.62% Test: 67.63%\n",
            "Epoch: 340, Loss: 1.1277, Train: 68.84%, Valid: 68.73% Test: 67.74%\n",
            "Epoch: 341, Loss: 1.1332, Train: 68.85%, Valid: 68.75% Test: 67.78%\n",
            "Epoch: 342, Loss: 1.1293, Train: 68.86%, Valid: 68.65% Test: 67.69%\n",
            "Epoch: 343, Loss: 1.1296, Train: 68.86%, Valid: 68.61% Test: 67.60%\n",
            "Epoch: 344, Loss: 1.1296, Train: 68.91%, Valid: 68.70% Test: 67.62%\n",
            "Epoch: 345, Loss: 1.1331, Train: 68.89%, Valid: 68.70% Test: 67.72%\n",
            "Epoch: 346, Loss: 1.1277, Train: 68.93%, Valid: 68.77% Test: 67.74%\n",
            "Epoch: 347, Loss: 1.1294, Train: 68.96%, Valid: 68.71% Test: 67.68%\n",
            "Epoch: 348, Loss: 1.1308, Train: 68.93%, Valid: 68.71% Test: 67.66%\n",
            "Epoch: 349, Loss: 1.1262, Train: 68.96%, Valid: 68.75% Test: 67.77%\n",
            "Epoch: 350, Loss: 1.1306, Train: 69.00%, Valid: 68.81% Test: 67.80%\n",
            "Epoch: 351, Loss: 1.1228, Train: 69.00%, Valid: 68.83% Test: 67.80%\n",
            "Epoch: 352, Loss: 1.1296, Train: 68.97%, Valid: 68.85% Test: 67.88%\n",
            "Epoch: 353, Loss: 1.1249, Train: 68.98%, Valid: 68.77% Test: 67.81%\n",
            "Epoch: 354, Loss: 1.1266, Train: 69.02%, Valid: 68.81% Test: 67.79%\n",
            "Epoch: 355, Loss: 1.1258, Train: 69.05%, Valid: 68.77% Test: 67.70%\n",
            "Epoch: 356, Loss: 1.1234, Train: 69.06%, Valid: 68.81% Test: 67.77%\n",
            "Epoch: 357, Loss: 1.1225, Train: 69.06%, Valid: 68.77% Test: 67.77%\n",
            "Epoch: 358, Loss: 1.1248, Train: 69.06%, Valid: 68.85% Test: 67.92%\n",
            "Epoch: 359, Loss: 1.1257, Train: 69.11%, Valid: 68.90% Test: 68.00%\n",
            "Epoch: 360, Loss: 1.1228, Train: 69.13%, Valid: 68.86% Test: 67.86%\n",
            "Epoch: 361, Loss: 1.1226, Train: 69.17%, Valid: 68.85% Test: 67.84%\n",
            "Epoch: 362, Loss: 1.1206, Train: 69.09%, Valid: 68.80% Test: 67.92%\n",
            "Epoch: 363, Loss: 1.1234, Train: 69.12%, Valid: 68.81% Test: 67.87%\n",
            "Epoch: 364, Loss: 1.1206, Train: 69.20%, Valid: 68.78% Test: 67.65%\n",
            "Epoch: 365, Loss: 1.1179, Train: 69.18%, Valid: 68.76% Test: 67.62%\n",
            "Epoch: 366, Loss: 1.1196, Train: 69.18%, Valid: 68.83% Test: 67.78%\n",
            "Epoch: 367, Loss: 1.1172, Train: 69.14%, Valid: 68.88% Test: 67.99%\n",
            "Epoch: 368, Loss: 1.1218, Train: 69.19%, Valid: 68.89% Test: 67.99%\n",
            "Epoch: 369, Loss: 1.1205, Train: 69.22%, Valid: 68.95% Test: 67.93%\n",
            "Epoch: 370, Loss: 1.1202, Train: 69.27%, Valid: 68.95% Test: 67.91%\n",
            "Epoch: 371, Loss: 1.1148, Train: 69.27%, Valid: 68.99% Test: 67.99%\n",
            "Epoch: 372, Loss: 1.1147, Train: 69.21%, Valid: 68.83% Test: 68.01%\n",
            "Epoch: 373, Loss: 1.1176, Train: 69.30%, Valid: 68.93% Test: 67.86%\n",
            "Epoch: 374, Loss: 1.1199, Train: 69.32%, Valid: 68.92% Test: 67.76%\n",
            "Epoch: 375, Loss: 1.1148, Train: 69.29%, Valid: 68.97% Test: 67.91%\n",
            "Epoch: 376, Loss: 1.1193, Train: 69.29%, Valid: 69.06% Test: 68.19%\n",
            "Epoch: 377, Loss: 1.1140, Train: 69.27%, Valid: 68.94% Test: 68.21%\n",
            "Epoch: 378, Loss: 1.1108, Train: 69.30%, Valid: 68.99% Test: 68.04%\n",
            "Epoch: 379, Loss: 1.1145, Train: 69.31%, Valid: 68.97% Test: 67.76%\n",
            "Epoch: 380, Loss: 1.1136, Train: 69.34%, Valid: 68.93% Test: 67.82%\n",
            "Epoch: 381, Loss: 1.1121, Train: 69.35%, Valid: 68.98% Test: 67.93%\n",
            "Epoch: 382, Loss: 1.1113, Train: 69.36%, Valid: 68.93% Test: 68.06%\n",
            "Epoch: 383, Loss: 1.1120, Train: 69.37%, Valid: 69.00% Test: 68.02%\n",
            "Epoch: 384, Loss: 1.1092, Train: 69.40%, Valid: 69.02% Test: 67.95%\n",
            "Epoch: 385, Loss: 1.1099, Train: 69.44%, Valid: 69.05% Test: 67.98%\n",
            "Epoch: 386, Loss: 1.1086, Train: 69.40%, Valid: 68.98% Test: 68.08%\n",
            "Epoch: 387, Loss: 1.1097, Train: 69.41%, Valid: 69.11% Test: 68.16%\n",
            "Epoch: 388, Loss: 1.1113, Train: 69.42%, Valid: 69.09% Test: 68.01%\n",
            "Epoch: 389, Loss: 1.1060, Train: 69.43%, Valid: 69.03% Test: 68.00%\n",
            "Epoch: 390, Loss: 1.1083, Train: 69.48%, Valid: 69.06% Test: 68.17%\n",
            "Epoch: 391, Loss: 1.1050, Train: 69.49%, Valid: 69.10% Test: 68.16%\n",
            "Epoch: 392, Loss: 1.1064, Train: 69.51%, Valid: 69.09% Test: 68.10%\n",
            "Epoch: 393, Loss: 1.1102, Train: 69.49%, Valid: 69.04% Test: 68.09%\n",
            "Epoch: 394, Loss: 1.1075, Train: 69.49%, Valid: 69.15% Test: 68.21%\n",
            "Epoch: 395, Loss: 1.1055, Train: 69.51%, Valid: 69.13% Test: 68.21%\n",
            "Epoch: 396, Loss: 1.1038, Train: 69.56%, Valid: 69.10% Test: 68.12%\n",
            "Epoch: 397, Loss: 1.1070, Train: 69.54%, Valid: 69.04% Test: 67.97%\n",
            "Epoch: 398, Loss: 1.1020, Train: 69.55%, Valid: 69.03% Test: 68.00%\n",
            "Epoch: 399, Loss: 1.1034, Train: 69.51%, Valid: 69.18% Test: 68.16%\n",
            "Epoch: 400, Loss: 1.1025, Train: 69.54%, Valid: 69.18% Test: 68.28%\n",
            "Epoch: 401, Loss: 1.1060, Train: 69.57%, Valid: 69.18% Test: 68.24%\n",
            "Epoch: 402, Loss: 1.1053, Train: 69.58%, Valid: 69.17% Test: 68.10%\n",
            "Epoch: 403, Loss: 1.1035, Train: 69.58%, Valid: 69.15% Test: 68.17%\n",
            "Epoch: 404, Loss: 1.1038, Train: 69.63%, Valid: 69.19% Test: 68.13%\n",
            "Epoch: 405, Loss: 1.1015, Train: 69.65%, Valid: 69.19% Test: 68.14%\n",
            "Epoch: 406, Loss: 1.1046, Train: 69.65%, Valid: 69.21% Test: 68.12%\n",
            "Epoch: 407, Loss: 1.0989, Train: 69.60%, Valid: 69.11% Test: 68.14%\n",
            "Epoch: 408, Loss: 1.1024, Train: 69.57%, Valid: 69.14% Test: 68.25%\n",
            "Epoch: 409, Loss: 1.1009, Train: 69.62%, Valid: 69.19% Test: 68.27%\n",
            "Epoch: 410, Loss: 1.1016, Train: 69.68%, Valid: 69.16% Test: 68.09%\n",
            "Epoch: 411, Loss: 1.0973, Train: 69.73%, Valid: 69.12% Test: 68.10%\n",
            "Epoch: 412, Loss: 1.1011, Train: 69.65%, Valid: 69.21% Test: 68.27%\n",
            "Epoch: 413, Loss: 1.1001, Train: 69.69%, Valid: 69.26% Test: 68.29%\n",
            "Epoch: 414, Loss: 1.0988, Train: 69.73%, Valid: 69.26% Test: 68.24%\n",
            "Epoch: 415, Loss: 1.0972, Train: 69.76%, Valid: 69.18% Test: 68.07%\n",
            "Epoch: 416, Loss: 1.1005, Train: 69.79%, Valid: 69.18% Test: 68.10%\n",
            "Epoch: 417, Loss: 1.0993, Train: 69.78%, Valid: 69.24% Test: 68.29%\n",
            "Epoch: 418, Loss: 1.0983, Train: 69.72%, Valid: 69.36% Test: 68.46%\n",
            "Epoch: 419, Loss: 1.0954, Train: 69.72%, Valid: 69.30% Test: 68.37%\n",
            "Epoch: 420, Loss: 1.0939, Train: 69.81%, Valid: 69.17% Test: 68.13%\n",
            "Epoch: 421, Loss: 1.0954, Train: 69.82%, Valid: 69.14% Test: 68.12%\n",
            "Epoch: 422, Loss: 1.0943, Train: 69.77%, Valid: 69.21% Test: 68.22%\n",
            "Epoch: 423, Loss: 1.0903, Train: 69.72%, Valid: 69.26% Test: 68.25%\n",
            "Epoch: 424, Loss: 1.0927, Train: 69.79%, Valid: 69.31% Test: 68.33%\n",
            "Epoch: 425, Loss: 1.0922, Train: 69.84%, Valid: 69.28% Test: 68.41%\n",
            "Epoch: 426, Loss: 1.0955, Train: 69.83%, Valid: 69.27% Test: 68.39%\n",
            "Epoch: 427, Loss: 1.0924, Train: 69.83%, Valid: 69.32% Test: 68.44%\n",
            "Epoch: 428, Loss: 1.0944, Train: 69.83%, Valid: 69.33% Test: 68.34%\n",
            "Epoch: 429, Loss: 1.0957, Train: 69.86%, Valid: 69.30% Test: 68.31%\n",
            "Epoch: 430, Loss: 1.0918, Train: 69.90%, Valid: 69.33% Test: 68.38%\n",
            "Epoch: 431, Loss: 1.0944, Train: 69.92%, Valid: 69.38% Test: 68.45%\n",
            "Epoch: 432, Loss: 1.0901, Train: 69.85%, Valid: 69.34% Test: 68.42%\n",
            "Epoch: 433, Loss: 1.0933, Train: 69.89%, Valid: 69.32% Test: 68.27%\n",
            "Epoch: 434, Loss: 1.0896, Train: 69.93%, Valid: 69.29% Test: 68.32%\n",
            "Epoch: 435, Loss: 1.0913, Train: 69.96%, Valid: 69.36% Test: 68.34%\n",
            "Epoch: 436, Loss: 1.0909, Train: 69.92%, Valid: 69.41% Test: 68.47%\n",
            "Epoch: 437, Loss: 1.0888, Train: 69.90%, Valid: 69.46% Test: 68.46%\n",
            "Epoch: 438, Loss: 1.0885, Train: 69.96%, Valid: 69.34% Test: 68.36%\n",
            "Epoch: 439, Loss: 1.0888, Train: 69.95%, Valid: 69.31% Test: 68.35%\n",
            "Epoch: 440, Loss: 1.0871, Train: 70.00%, Valid: 69.40% Test: 68.39%\n",
            "Epoch: 441, Loss: 1.0899, Train: 69.99%, Valid: 69.42% Test: 68.42%\n",
            "Epoch: 442, Loss: 1.0833, Train: 69.99%, Valid: 69.34% Test: 68.35%\n",
            "Epoch: 443, Loss: 1.0889, Train: 70.03%, Valid: 69.35% Test: 68.32%\n",
            "Epoch: 444, Loss: 1.0834, Train: 70.06%, Valid: 69.36% Test: 68.39%\n",
            "Epoch: 445, Loss: 1.0872, Train: 70.03%, Valid: 69.38% Test: 68.47%\n",
            "Epoch: 446, Loss: 1.0842, Train: 69.99%, Valid: 69.48% Test: 68.52%\n",
            "Epoch: 447, Loss: 1.0860, Train: 70.03%, Valid: 69.45% Test: 68.49%\n",
            "Epoch: 448, Loss: 1.0840, Train: 70.05%, Valid: 69.37% Test: 68.47%\n",
            "Epoch: 449, Loss: 1.0784, Train: 70.07%, Valid: 69.38% Test: 68.44%\n",
            "Epoch: 450, Loss: 1.0834, Train: 70.10%, Valid: 69.42% Test: 68.40%\n",
            "Epoch: 451, Loss: 1.0828, Train: 70.11%, Valid: 69.39% Test: 68.43%\n",
            "Epoch: 452, Loss: 1.0818, Train: 70.09%, Valid: 69.46% Test: 68.55%\n",
            "Epoch: 453, Loss: 1.0819, Train: 70.10%, Valid: 69.46% Test: 68.53%\n",
            "Epoch: 454, Loss: 1.0861, Train: 70.10%, Valid: 69.46% Test: 68.50%\n",
            "Epoch: 455, Loss: 1.0858, Train: 70.05%, Valid: 69.50% Test: 68.53%\n",
            "Epoch: 456, Loss: 1.0815, Train: 70.13%, Valid: 69.49% Test: 68.50%\n",
            "Epoch: 457, Loss: 1.0808, Train: 70.16%, Valid: 69.43% Test: 68.37%\n",
            "Epoch: 458, Loss: 1.0831, Train: 70.10%, Valid: 69.45% Test: 68.36%\n",
            "Epoch: 459, Loss: 1.0838, Train: 70.13%, Valid: 69.48% Test: 68.46%\n",
            "Epoch: 460, Loss: 1.0800, Train: 70.15%, Valid: 69.46% Test: 68.45%\n",
            "Epoch: 461, Loss: 1.0783, Train: 70.18%, Valid: 69.52% Test: 68.58%\n",
            "Epoch: 462, Loss: 1.0829, Train: 70.18%, Valid: 69.55% Test: 68.56%\n",
            "Epoch: 463, Loss: 1.0798, Train: 70.25%, Valid: 69.53% Test: 68.52%\n",
            "Epoch: 464, Loss: 1.0797, Train: 70.25%, Valid: 69.45% Test: 68.55%\n",
            "Epoch: 465, Loss: 1.0812, Train: 70.27%, Valid: 69.52% Test: 68.52%\n",
            "Epoch: 466, Loss: 1.0818, Train: 70.23%, Valid: 69.53% Test: 68.56%\n",
            "Epoch: 467, Loss: 1.0844, Train: 70.24%, Valid: 69.50% Test: 68.48%\n",
            "Epoch: 468, Loss: 1.0798, Train: 70.22%, Valid: 69.54% Test: 68.44%\n",
            "Epoch: 469, Loss: 1.0777, Train: 70.26%, Valid: 69.55% Test: 68.46%\n",
            "Epoch: 470, Loss: 1.0765, Train: 70.29%, Valid: 69.59% Test: 68.59%\n",
            "Epoch: 471, Loss: 1.0768, Train: 70.24%, Valid: 69.65% Test: 68.67%\n",
            "Epoch: 472, Loss: 1.0746, Train: 70.29%, Valid: 69.61% Test: 68.59%\n",
            "Epoch: 473, Loss: 1.0765, Train: 70.27%, Valid: 69.57% Test: 68.52%\n",
            "Epoch: 474, Loss: 1.0778, Train: 70.26%, Valid: 69.61% Test: 68.61%\n",
            "Epoch: 475, Loss: 1.0765, Train: 70.31%, Valid: 69.59% Test: 68.68%\n",
            "Epoch: 476, Loss: 1.0761, Train: 70.30%, Valid: 69.65% Test: 68.66%\n",
            "Epoch: 477, Loss: 1.0740, Train: 70.35%, Valid: 69.60% Test: 68.48%\n",
            "Epoch: 478, Loss: 1.0753, Train: 70.36%, Valid: 69.61% Test: 68.46%\n",
            "Epoch: 479, Loss: 1.0718, Train: 70.32%, Valid: 69.66% Test: 68.59%\n",
            "Epoch: 480, Loss: 1.0756, Train: 70.35%, Valid: 69.70% Test: 68.70%\n",
            "Epoch: 481, Loss: 1.0760, Train: 70.34%, Valid: 69.72% Test: 68.76%\n",
            "Epoch: 482, Loss: 1.0743, Train: 70.39%, Valid: 69.64% Test: 68.51%\n",
            "Epoch: 483, Loss: 1.0750, Train: 70.35%, Valid: 69.65% Test: 68.62%\n",
            "Epoch: 484, Loss: 1.0755, Train: 70.29%, Valid: 69.59% Test: 68.74%\n",
            "Epoch: 485, Loss: 1.0714, Train: 70.40%, Valid: 69.61% Test: 68.51%\n",
            "Epoch: 486, Loss: 1.0754, Train: 70.44%, Valid: 69.59% Test: 68.37%\n",
            "Epoch: 487, Loss: 1.0722, Train: 70.45%, Valid: 69.71% Test: 68.57%\n",
            "Epoch: 488, Loss: 1.0699, Train: 70.37%, Valid: 69.76% Test: 68.75%\n",
            "Epoch: 489, Loss: 1.0729, Train: 70.40%, Valid: 69.70% Test: 68.71%\n",
            "Epoch: 490, Loss: 1.0700, Train: 70.42%, Valid: 69.61% Test: 68.49%\n",
            "Epoch: 491, Loss: 1.0683, Train: 70.46%, Valid: 69.66% Test: 68.52%\n",
            "Epoch: 492, Loss: 1.0726, Train: 70.43%, Valid: 69.71% Test: 68.71%\n",
            "Epoch: 493, Loss: 1.0658, Train: 70.40%, Valid: 69.68% Test: 68.70%\n",
            "Epoch: 494, Loss: 1.0681, Train: 70.43%, Valid: 69.63% Test: 68.59%\n",
            "Epoch: 495, Loss: 1.0671, Train: 70.43%, Valid: 69.62% Test: 68.49%\n",
            "Epoch: 496, Loss: 1.0717, Train: 70.46%, Valid: 69.73% Test: 68.65%\n",
            "Epoch: 497, Loss: 1.0702, Train: 70.46%, Valid: 69.73% Test: 68.74%\n",
            "Epoch: 498, Loss: 1.0684, Train: 70.49%, Valid: 69.76% Test: 68.74%\n",
            "Epoch: 499, Loss: 1.0667, Train: 70.52%, Valid: 69.74% Test: 68.66%\n",
            "Epoch: 500, Loss: 1.0679, Train: 70.47%, Valid: 69.74% Test: 68.65%\n",
            "Epoch: 501, Loss: 1.0646, Train: 70.49%, Valid: 69.73% Test: 68.68%\n",
            "Epoch: 502, Loss: 1.0647, Train: 70.49%, Valid: 69.80% Test: 68.67%\n",
            "Epoch: 503, Loss: 1.0672, Train: 70.54%, Valid: 69.79% Test: 68.76%\n",
            "Epoch: 504, Loss: 1.0668, Train: 70.58%, Valid: 69.82% Test: 68.80%\n",
            "Epoch: 505, Loss: 1.0640, Train: 70.55%, Valid: 69.84% Test: 68.83%\n",
            "Epoch: 506, Loss: 1.0655, Train: 70.55%, Valid: 69.87% Test: 68.80%\n",
            "Epoch: 507, Loss: 1.0649, Train: 70.57%, Valid: 69.73% Test: 68.61%\n",
            "Epoch: 508, Loss: 1.0682, Train: 70.61%, Valid: 69.54% Test: 68.50%\n",
            "Epoch: 509, Loss: 1.0647, Train: 70.59%, Valid: 69.70% Test: 68.69%\n",
            "Epoch: 510, Loss: 1.0618, Train: 70.56%, Valid: 69.88% Test: 68.89%\n",
            "Epoch: 511, Loss: 1.0652, Train: 70.54%, Valid: 69.92% Test: 69.00%\n",
            "Epoch: 512, Loss: 1.0635, Train: 70.59%, Valid: 69.81% Test: 68.80%\n",
            "Epoch: 513, Loss: 1.0664, Train: 70.62%, Valid: 69.78% Test: 68.66%\n",
            "Epoch: 514, Loss: 1.0645, Train: 70.60%, Valid: 69.73% Test: 68.80%\n",
            "Epoch: 515, Loss: 1.0619, Train: 70.58%, Valid: 69.79% Test: 68.82%\n",
            "Epoch: 516, Loss: 1.0658, Train: 70.60%, Valid: 69.82% Test: 68.78%\n",
            "Epoch: 517, Loss: 1.0609, Train: 70.66%, Valid: 69.70% Test: 68.65%\n",
            "Epoch: 518, Loss: 1.0635, Train: 70.63%, Valid: 69.79% Test: 68.72%\n",
            "Epoch: 519, Loss: 1.0605, Train: 70.64%, Valid: 69.83% Test: 68.85%\n",
            "Epoch: 520, Loss: 1.0607, Train: 70.63%, Valid: 69.86% Test: 68.91%\n",
            "Epoch: 521, Loss: 1.0639, Train: 70.67%, Valid: 69.84% Test: 68.76%\n",
            "Epoch: 522, Loss: 1.0605, Train: 70.70%, Valid: 69.78% Test: 68.67%\n",
            "Epoch: 523, Loss: 1.0600, Train: 70.69%, Valid: 69.87% Test: 68.84%\n",
            "Epoch: 524, Loss: 1.0599, Train: 70.61%, Valid: 69.85% Test: 68.92%\n",
            "Epoch: 525, Loss: 1.0631, Train: 70.64%, Valid: 69.80% Test: 68.84%\n",
            "Epoch: 526, Loss: 1.0580, Train: 70.70%, Valid: 69.74% Test: 68.58%\n",
            "Epoch: 527, Loss: 1.0612, Train: 70.73%, Valid: 69.73% Test: 68.56%\n",
            "Epoch: 528, Loss: 1.0583, Train: 70.69%, Valid: 69.86% Test: 68.84%\n",
            "Epoch: 529, Loss: 1.0591, Train: 70.66%, Valid: 69.89% Test: 69.02%\n",
            "Epoch: 530, Loss: 1.0619, Train: 70.73%, Valid: 69.99% Test: 68.99%\n",
            "Epoch: 531, Loss: 1.0573, Train: 70.77%, Valid: 69.87% Test: 68.79%\n",
            "Epoch: 532, Loss: 1.0597, Train: 70.74%, Valid: 69.80% Test: 68.80%\n",
            "Epoch: 533, Loss: 1.0594, Train: 70.73%, Valid: 69.89% Test: 68.90%\n",
            "Epoch: 534, Loss: 1.0572, Train: 70.72%, Valid: 69.91% Test: 68.92%\n",
            "Epoch: 535, Loss: 1.0597, Train: 70.77%, Valid: 69.79% Test: 68.71%\n",
            "Epoch: 536, Loss: 1.0583, Train: 70.77%, Valid: 69.79% Test: 68.73%\n",
            "Epoch: 537, Loss: 1.0598, Train: 70.77%, Valid: 69.85% Test: 68.89%\n",
            "Epoch: 538, Loss: 1.0584, Train: 70.73%, Valid: 69.96% Test: 68.96%\n",
            "Epoch: 539, Loss: 1.0534, Train: 70.78%, Valid: 69.82% Test: 68.78%\n",
            "Epoch: 540, Loss: 1.0575, Train: 70.80%, Valid: 69.87% Test: 68.80%\n",
            "Epoch: 541, Loss: 1.0542, Train: 70.78%, Valid: 69.87% Test: 68.89%\n",
            "Epoch: 542, Loss: 1.0535, Train: 70.81%, Valid: 69.98% Test: 69.00%\n",
            "Epoch: 543, Loss: 1.0547, Train: 70.79%, Valid: 69.94% Test: 68.91%\n",
            "Epoch: 544, Loss: 1.0560, Train: 70.80%, Valid: 69.85% Test: 68.82%\n",
            "Epoch: 545, Loss: 1.0544, Train: 70.82%, Valid: 69.86% Test: 68.84%\n",
            "Epoch: 546, Loss: 1.0540, Train: 70.86%, Valid: 69.92% Test: 68.76%\n",
            "Epoch: 547, Loss: 1.0521, Train: 70.86%, Valid: 69.91% Test: 68.96%\n",
            "Epoch: 548, Loss: 1.0535, Train: 70.85%, Valid: 69.95% Test: 69.06%\n",
            "Epoch: 549, Loss: 1.0564, Train: 70.88%, Valid: 70.00% Test: 69.00%\n",
            "Epoch: 550, Loss: 1.0555, Train: 70.89%, Valid: 69.89% Test: 68.81%\n",
            "Epoch: 551, Loss: 1.0533, Train: 70.95%, Valid: 69.92% Test: 68.77%\n",
            "Epoch: 552, Loss: 1.0570, Train: 70.86%, Valid: 69.96% Test: 68.97%\n",
            "Epoch: 553, Loss: 1.0528, Train: 70.84%, Valid: 69.99% Test: 69.02%\n",
            "Epoch: 554, Loss: 1.0529, Train: 70.87%, Valid: 70.03% Test: 68.99%\n",
            "Epoch: 555, Loss: 1.0509, Train: 70.88%, Valid: 69.89% Test: 68.77%\n",
            "Epoch: 556, Loss: 1.0496, Train: 70.88%, Valid: 69.91% Test: 68.85%\n",
            "Epoch: 557, Loss: 1.0536, Train: 70.85%, Valid: 69.88% Test: 68.97%\n",
            "Epoch: 558, Loss: 1.0496, Train: 70.91%, Valid: 70.02% Test: 69.10%\n",
            "Epoch: 559, Loss: 1.0511, Train: 70.88%, Valid: 70.09% Test: 68.95%\n",
            "Epoch: 560, Loss: 1.0518, Train: 70.91%, Valid: 69.93% Test: 68.85%\n",
            "Epoch: 561, Loss: 1.0521, Train: 70.94%, Valid: 69.82% Test: 68.71%\n",
            "Epoch: 562, Loss: 1.0512, Train: 70.97%, Valid: 69.95% Test: 68.89%\n",
            "Epoch: 563, Loss: 1.0491, Train: 70.95%, Valid: 70.02% Test: 68.96%\n",
            "Epoch: 564, Loss: 1.0502, Train: 70.98%, Valid: 70.05% Test: 68.98%\n",
            "Epoch: 565, Loss: 1.0502, Train: 70.97%, Valid: 69.95% Test: 68.98%\n",
            "Epoch: 566, Loss: 1.0475, Train: 70.98%, Valid: 69.94% Test: 68.99%\n",
            "Epoch: 567, Loss: 1.0469, Train: 70.94%, Valid: 69.98% Test: 68.95%\n",
            "Epoch: 568, Loss: 1.0503, Train: 70.92%, Valid: 69.97% Test: 68.84%\n",
            "Epoch: 569, Loss: 1.0474, Train: 70.95%, Valid: 69.95% Test: 68.89%\n",
            "Epoch: 570, Loss: 1.0469, Train: 70.92%, Valid: 69.99% Test: 69.04%\n",
            "Epoch: 571, Loss: 1.0492, Train: 70.95%, Valid: 70.03% Test: 69.13%\n",
            "Epoch: 572, Loss: 1.0471, Train: 71.00%, Valid: 70.01% Test: 68.88%\n",
            "Epoch: 573, Loss: 1.0478, Train: 71.02%, Valid: 69.87% Test: 68.60%\n",
            "Epoch: 574, Loss: 1.0471, Train: 71.00%, Valid: 69.97% Test: 68.85%\n",
            "Epoch: 575, Loss: 1.0462, Train: 70.94%, Valid: 70.01% Test: 69.14%\n",
            "Epoch: 576, Loss: 1.0455, Train: 70.98%, Valid: 70.07% Test: 69.09%\n",
            "Epoch: 577, Loss: 1.0463, Train: 71.06%, Valid: 69.91% Test: 68.72%\n",
            "Epoch: 578, Loss: 1.0444, Train: 71.06%, Valid: 69.95% Test: 68.81%\n",
            "Epoch: 579, Loss: 1.0469, Train: 70.99%, Valid: 69.95% Test: 69.12%\n",
            "Epoch: 580, Loss: 1.0436, Train: 71.00%, Valid: 70.12% Test: 69.21%\n",
            "Epoch: 581, Loss: 1.0450, Train: 71.03%, Valid: 70.10% Test: 69.02%\n",
            "Epoch: 582, Loss: 1.0445, Train: 71.12%, Valid: 69.92% Test: 68.72%\n",
            "Epoch: 583, Loss: 1.0458, Train: 71.10%, Valid: 69.92% Test: 68.83%\n",
            "Epoch: 584, Loss: 1.0413, Train: 71.05%, Valid: 70.02% Test: 69.07%\n",
            "Epoch: 585, Loss: 1.0439, Train: 71.07%, Valid: 70.12% Test: 69.17%\n",
            "Epoch: 586, Loss: 1.0448, Train: 71.07%, Valid: 70.01% Test: 68.97%\n",
            "Epoch: 587, Loss: 1.0444, Train: 71.13%, Valid: 70.11% Test: 68.99%\n",
            "Epoch: 588, Loss: 1.0455, Train: 71.06%, Valid: 70.05% Test: 69.13%\n",
            "Epoch: 589, Loss: 1.0422, Train: 71.03%, Valid: 70.17% Test: 69.21%\n",
            "Epoch: 590, Loss: 1.0424, Train: 71.11%, Valid: 70.11% Test: 68.96%\n",
            "Epoch: 591, Loss: 1.0434, Train: 71.16%, Valid: 69.97% Test: 68.78%\n",
            "Epoch: 592, Loss: 1.0430, Train: 71.11%, Valid: 70.01% Test: 68.93%\n",
            "Epoch: 593, Loss: 1.0432, Train: 71.10%, Valid: 70.07% Test: 69.14%\n",
            "Epoch: 594, Loss: 1.0425, Train: 71.11%, Valid: 70.18% Test: 69.25%\n",
            "Epoch: 595, Loss: 1.0418, Train: 71.20%, Valid: 70.17% Test: 69.10%\n",
            "Epoch: 596, Loss: 1.0408, Train: 71.20%, Valid: 70.07% Test: 68.96%\n",
            "Epoch: 597, Loss: 1.0428, Train: 71.14%, Valid: 70.14% Test: 69.05%\n",
            "Epoch: 598, Loss: 1.0399, Train: 71.13%, Valid: 70.03% Test: 69.11%\n",
            "Epoch: 599, Loss: 1.0425, Train: 71.19%, Valid: 70.05% Test: 69.00%\n",
            "Epoch: 600, Loss: 1.0378, Train: 71.19%, Valid: 70.09% Test: 68.93%\n",
            "Epoch: 601, Loss: 1.0454, Train: 71.18%, Valid: 70.14% Test: 69.01%\n",
            "Epoch: 602, Loss: 1.0389, Train: 71.14%, Valid: 70.17% Test: 69.19%\n",
            "Epoch: 603, Loss: 1.0417, Train: 71.16%, Valid: 70.21% Test: 69.18%\n",
            "Epoch: 604, Loss: 1.0388, Train: 71.24%, Valid: 70.12% Test: 69.07%\n",
            "Epoch: 605, Loss: 1.0391, Train: 71.18%, Valid: 70.07% Test: 68.99%\n",
            "Epoch: 606, Loss: 1.0377, Train: 71.15%, Valid: 70.14% Test: 69.18%\n",
            "Epoch: 607, Loss: 1.0395, Train: 71.16%, Valid: 70.05% Test: 69.21%\n",
            "Epoch: 608, Loss: 1.0356, Train: 71.22%, Valid: 70.04% Test: 69.08%\n",
            "Epoch: 609, Loss: 1.0408, Train: 71.24%, Valid: 70.03% Test: 68.94%\n",
            "Epoch: 610, Loss: 1.0402, Train: 71.25%, Valid: 70.13% Test: 69.12%\n",
            "Epoch: 611, Loss: 1.0374, Train: 71.20%, Valid: 70.27% Test: 69.29%\n",
            "Epoch: 612, Loss: 1.0413, Train: 71.21%, Valid: 70.27% Test: 69.21%\n",
            "Epoch: 613, Loss: 1.0369, Train: 71.30%, Valid: 70.18% Test: 69.07%\n",
            "Epoch: 614, Loss: 1.0395, Train: 71.30%, Valid: 70.05% Test: 68.96%\n",
            "Epoch: 615, Loss: 1.0389, Train: 71.27%, Valid: 70.13% Test: 69.08%\n",
            "Epoch: 616, Loss: 1.0362, Train: 71.21%, Valid: 70.22% Test: 69.23%\n",
            "Epoch: 617, Loss: 1.0378, Train: 71.25%, Valid: 70.16% Test: 69.19%\n",
            "Epoch: 618, Loss: 1.0356, Train: 71.27%, Valid: 70.07% Test: 69.04%\n",
            "Epoch: 619, Loss: 1.0348, Train: 71.31%, Valid: 70.01% Test: 68.88%\n",
            "Epoch: 620, Loss: 1.0382, Train: 71.26%, Valid: 70.18% Test: 69.22%\n",
            "Epoch: 621, Loss: 1.0368, Train: 71.28%, Valid: 70.27% Test: 69.40%\n",
            "Epoch: 622, Loss: 1.0359, Train: 71.28%, Valid: 70.20% Test: 69.31%\n",
            "Epoch: 623, Loss: 1.0340, Train: 71.30%, Valid: 70.14% Test: 69.13%\n",
            "Epoch: 624, Loss: 1.0334, Train: 71.30%, Valid: 70.15% Test: 69.05%\n",
            "Epoch: 625, Loss: 1.0310, Train: 71.29%, Valid: 70.25% Test: 69.15%\n",
            "Epoch: 626, Loss: 1.0345, Train: 71.28%, Valid: 70.18% Test: 69.23%\n",
            "Epoch: 627, Loss: 1.0375, Train: 71.31%, Valid: 70.21% Test: 69.21%\n",
            "Epoch: 628, Loss: 1.0352, Train: 71.36%, Valid: 70.16% Test: 69.07%\n",
            "Epoch: 629, Loss: 1.0338, Train: 71.35%, Valid: 70.18% Test: 69.10%\n",
            "Epoch: 630, Loss: 1.0320, Train: 71.32%, Valid: 70.24% Test: 69.23%\n",
            "Epoch: 631, Loss: 1.0342, Train: 71.33%, Valid: 70.23% Test: 69.30%\n",
            "Epoch: 632, Loss: 1.0336, Train: 71.33%, Valid: 70.20% Test: 69.22%\n",
            "Epoch: 633, Loss: 1.0327, Train: 71.39%, Valid: 70.14% Test: 68.94%\n",
            "Epoch: 634, Loss: 1.0343, Train: 71.39%, Valid: 70.14% Test: 69.03%\n",
            "Epoch: 635, Loss: 1.0316, Train: 71.33%, Valid: 70.22% Test: 69.24%\n",
            "Epoch: 636, Loss: 1.0342, Train: 71.34%, Valid: 70.26% Test: 69.33%\n",
            "Epoch: 637, Loss: 1.0326, Train: 71.43%, Valid: 70.20% Test: 69.19%\n",
            "Epoch: 638, Loss: 1.0299, Train: 71.39%, Valid: 70.30% Test: 69.31%\n",
            "Epoch: 639, Loss: 1.0346, Train: 71.34%, Valid: 70.29% Test: 69.33%\n",
            "Epoch: 640, Loss: 1.0318, Train: 71.37%, Valid: 70.31% Test: 69.21%\n",
            "Epoch: 641, Loss: 1.0363, Train: 71.38%, Valid: 70.16% Test: 69.07%\n",
            "Epoch: 642, Loss: 1.0321, Train: 71.42%, Valid: 70.13% Test: 69.11%\n",
            "Epoch: 643, Loss: 1.0307, Train: 71.43%, Valid: 70.27% Test: 69.33%\n",
            "Epoch: 644, Loss: 1.0328, Train: 71.40%, Valid: 70.30% Test: 69.35%\n",
            "Epoch: 645, Loss: 1.0315, Train: 71.45%, Valid: 70.27% Test: 69.28%\n",
            "Epoch: 646, Loss: 1.0292, Train: 71.47%, Valid: 70.17% Test: 69.13%\n",
            "Epoch: 647, Loss: 1.0319, Train: 71.44%, Valid: 70.22% Test: 69.14%\n",
            "Epoch: 648, Loss: 1.0293, Train: 71.40%, Valid: 70.36% Test: 69.28%\n",
            "Epoch: 649, Loss: 1.0290, Train: 71.38%, Valid: 70.30% Test: 69.31%\n",
            "Epoch: 650, Loss: 1.0318, Train: 71.47%, Valid: 70.24% Test: 69.25%\n",
            "Epoch: 651, Loss: 1.0313, Train: 71.51%, Valid: 70.28% Test: 69.23%\n",
            "Epoch: 652, Loss: 1.0298, Train: 71.47%, Valid: 70.28% Test: 69.32%\n",
            "Epoch: 653, Loss: 1.0272, Train: 71.44%, Valid: 70.29% Test: 69.26%\n",
            "Epoch: 654, Loss: 1.0323, Train: 71.47%, Valid: 70.26% Test: 69.27%\n",
            "Epoch: 655, Loss: 1.0275, Train: 71.47%, Valid: 70.26% Test: 69.23%\n",
            "Epoch: 656, Loss: 1.0267, Train: 71.47%, Valid: 70.24% Test: 69.27%\n",
            "Epoch: 657, Loss: 1.0265, Train: 71.46%, Valid: 70.28% Test: 69.26%\n",
            "Epoch: 658, Loss: 1.0313, Train: 71.50%, Valid: 70.29% Test: 69.21%\n",
            "Epoch: 659, Loss: 1.0288, Train: 71.51%, Valid: 70.30% Test: 69.27%\n",
            "Epoch: 660, Loss: 1.0245, Train: 71.49%, Valid: 70.32% Test: 69.33%\n",
            "Epoch: 661, Loss: 1.0286, Train: 71.53%, Valid: 70.24% Test: 69.29%\n",
            "Epoch: 662, Loss: 1.0287, Train: 71.54%, Valid: 70.27% Test: 69.22%\n",
            "Epoch: 663, Loss: 1.0281, Train: 71.52%, Valid: 70.27% Test: 69.26%\n",
            "Epoch: 664, Loss: 1.0299, Train: 71.53%, Valid: 70.24% Test: 69.28%\n",
            "Epoch: 665, Loss: 1.0269, Train: 71.53%, Valid: 70.21% Test: 69.16%\n",
            "Epoch: 666, Loss: 1.0251, Train: 71.59%, Valid: 70.29% Test: 69.17%\n",
            "Epoch: 667, Loss: 1.0231, Train: 71.58%, Valid: 70.26% Test: 69.21%\n",
            "Epoch: 668, Loss: 1.0263, Train: 71.53%, Valid: 70.36% Test: 69.34%\n",
            "Epoch: 669, Loss: 1.0243, Train: 71.52%, Valid: 70.29% Test: 69.37%\n",
            "Epoch: 670, Loss: 1.0241, Train: 71.55%, Valid: 70.23% Test: 69.27%\n",
            "Epoch: 671, Loss: 1.0265, Train: 71.59%, Valid: 70.26% Test: 69.23%\n",
            "Epoch: 672, Loss: 1.0249, Train: 71.61%, Valid: 70.25% Test: 69.21%\n",
            "Epoch: 673, Loss: 1.0255, Train: 71.57%, Valid: 70.24% Test: 69.22%\n",
            "Epoch: 674, Loss: 1.0251, Train: 71.58%, Valid: 70.34% Test: 69.36%\n",
            "Epoch: 675, Loss: 1.0232, Train: 71.61%, Valid: 70.32% Test: 69.35%\n",
            "Epoch: 676, Loss: 1.0279, Train: 71.58%, Valid: 70.38% Test: 69.38%\n",
            "Epoch: 677, Loss: 1.0215, Train: 71.62%, Valid: 70.28% Test: 69.23%\n",
            "Epoch: 678, Loss: 1.0264, Train: 71.64%, Valid: 70.32% Test: 69.25%\n",
            "Epoch: 679, Loss: 1.0218, Train: 71.61%, Valid: 70.33% Test: 69.37%\n",
            "Epoch: 680, Loss: 1.0230, Train: 71.58%, Valid: 70.32% Test: 69.37%\n",
            "Epoch: 681, Loss: 1.0214, Train: 71.60%, Valid: 70.30% Test: 69.35%\n",
            "Epoch: 682, Loss: 1.0231, Train: 71.61%, Valid: 70.30% Test: 69.27%\n",
            "Epoch: 683, Loss: 1.0229, Train: 71.63%, Valid: 70.34% Test: 69.32%\n",
            "Epoch: 684, Loss: 1.0237, Train: 71.66%, Valid: 70.31% Test: 69.29%\n",
            "Epoch: 685, Loss: 1.0223, Train: 71.65%, Valid: 70.36% Test: 69.29%\n",
            "Epoch: 686, Loss: 1.0235, Train: 71.64%, Valid: 70.32% Test: 69.34%\n",
            "Epoch: 687, Loss: 1.0206, Train: 71.62%, Valid: 70.30% Test: 69.33%\n",
            "Epoch: 688, Loss: 1.0240, Train: 71.67%, Valid: 70.30% Test: 69.21%\n",
            "Epoch: 689, Loss: 1.0233, Train: 71.69%, Valid: 70.28% Test: 69.13%\n",
            "Epoch: 690, Loss: 1.0193, Train: 71.65%, Valid: 70.32% Test: 69.28%\n",
            "Epoch: 691, Loss: 1.0191, Train: 71.62%, Valid: 70.40% Test: 69.36%\n",
            "Epoch: 692, Loss: 1.0216, Train: 71.64%, Valid: 70.32% Test: 69.33%\n",
            "Epoch: 693, Loss: 1.0208, Train: 71.72%, Valid: 70.26% Test: 69.20%\n",
            "Epoch: 694, Loss: 1.0220, Train: 71.76%, Valid: 70.31% Test: 69.13%\n",
            "Epoch: 695, Loss: 1.0216, Train: 71.74%, Valid: 70.45% Test: 69.44%\n",
            "Epoch: 696, Loss: 1.0203, Train: 71.66%, Valid: 70.39% Test: 69.54%\n",
            "Epoch: 697, Loss: 1.0212, Train: 71.67%, Valid: 70.39% Test: 69.43%\n",
            "Epoch: 698, Loss: 1.0181, Train: 71.72%, Valid: 70.28% Test: 69.30%\n",
            "Epoch: 699, Loss: 1.0189, Train: 71.75%, Valid: 70.37% Test: 69.20%\n",
            "Epoch: 700, Loss: 1.0190, Train: 71.71%, Valid: 70.46% Test: 69.47%\n",
            "Epoch: 701, Loss: 1.0192, Train: 71.68%, Valid: 70.55% Test: 69.45%\n",
            "Epoch: 702, Loss: 1.0196, Train: 71.76%, Valid: 70.28% Test: 69.19%\n",
            "Epoch: 703, Loss: 1.0196, Train: 71.78%, Valid: 70.28% Test: 69.21%\n",
            "Epoch: 704, Loss: 1.0183, Train: 71.73%, Valid: 70.50% Test: 69.51%\n",
            "Epoch: 705, Loss: 1.0178, Train: 71.72%, Valid: 70.56% Test: 69.56%\n",
            "Epoch: 706, Loss: 1.0173, Train: 71.76%, Valid: 70.42% Test: 69.38%\n",
            "Epoch: 707, Loss: 1.0183, Train: 71.81%, Valid: 70.36% Test: 69.16%\n",
            "Epoch: 708, Loss: 1.0180, Train: 71.78%, Valid: 70.38% Test: 69.36%\n",
            "Epoch: 709, Loss: 1.0162, Train: 71.68%, Valid: 70.45% Test: 69.55%\n",
            "Epoch: 710, Loss: 1.0186, Train: 71.73%, Valid: 70.42% Test: 69.44%\n",
            "Epoch: 711, Loss: 1.0183, Train: 71.80%, Valid: 70.37% Test: 69.34%\n",
            "Epoch: 712, Loss: 1.0133, Train: 71.81%, Valid: 70.35% Test: 69.29%\n",
            "Epoch: 713, Loss: 1.0174, Train: 71.77%, Valid: 70.47% Test: 69.45%\n",
            "Epoch: 714, Loss: 1.0154, Train: 71.76%, Valid: 70.48% Test: 69.50%\n",
            "Epoch: 715, Loss: 1.0173, Train: 71.84%, Valid: 70.42% Test: 69.29%\n",
            "Epoch: 716, Loss: 1.0180, Train: 71.80%, Valid: 70.31% Test: 69.24%\n",
            "Epoch: 717, Loss: 1.0143, Train: 71.79%, Valid: 70.46% Test: 69.40%\n",
            "Epoch: 718, Loss: 1.0155, Train: 71.83%, Valid: 70.51% Test: 69.44%\n",
            "Epoch: 719, Loss: 1.0141, Train: 71.83%, Valid: 70.41% Test: 69.29%\n",
            "Epoch: 720, Loss: 1.0141, Train: 71.85%, Valid: 70.29% Test: 69.30%\n",
            "Epoch: 721, Loss: 1.0158, Train: 71.82%, Valid: 70.37% Test: 69.48%\n",
            "Epoch: 722, Loss: 1.0118, Train: 71.81%, Valid: 70.45% Test: 69.51%\n",
            "Epoch: 723, Loss: 1.0129, Train: 71.85%, Valid: 70.47% Test: 69.45%\n",
            "Epoch: 724, Loss: 1.0131, Train: 71.87%, Valid: 70.44% Test: 69.25%\n",
            "Epoch: 725, Loss: 1.0185, Train: 71.84%, Valid: 70.43% Test: 69.29%\n",
            "Epoch: 726, Loss: 1.0144, Train: 71.87%, Valid: 70.42% Test: 69.37%\n",
            "Epoch: 727, Loss: 1.0131, Train: 71.85%, Valid: 70.56% Test: 69.49%\n",
            "Epoch: 728, Loss: 1.0140, Train: 71.87%, Valid: 70.45% Test: 69.36%\n",
            "Epoch: 729, Loss: 1.0163, Train: 71.87%, Valid: 70.38% Test: 69.22%\n",
            "Epoch: 730, Loss: 1.0160, Train: 71.90%, Valid: 70.46% Test: 69.30%\n",
            "Epoch: 731, Loss: 1.0149, Train: 71.88%, Valid: 70.46% Test: 69.43%\n",
            "Epoch: 732, Loss: 1.0115, Train: 71.86%, Valid: 70.51% Test: 69.57%\n",
            "Epoch: 733, Loss: 1.0116, Train: 71.86%, Valid: 70.54% Test: 69.58%\n",
            "Epoch: 734, Loss: 1.0138, Train: 71.89%, Valid: 70.42% Test: 69.34%\n",
            "Epoch: 735, Loss: 1.0116, Train: 71.90%, Valid: 70.38% Test: 69.26%\n",
            "Epoch: 736, Loss: 1.0127, Train: 71.92%, Valid: 70.52% Test: 69.36%\n",
            "Epoch: 737, Loss: 1.0104, Train: 71.93%, Valid: 70.57% Test: 69.46%\n",
            "Epoch: 738, Loss: 1.0101, Train: 71.87%, Valid: 70.54% Test: 69.51%\n",
            "Epoch: 739, Loss: 1.0110, Train: 71.95%, Valid: 70.46% Test: 69.39%\n",
            "Epoch: 740, Loss: 1.0118, Train: 71.94%, Valid: 70.51% Test: 69.34%\n",
            "Epoch: 741, Loss: 1.0127, Train: 71.93%, Valid: 70.50% Test: 69.53%\n",
            "Epoch: 742, Loss: 1.0131, Train: 71.91%, Valid: 70.57% Test: 69.56%\n",
            "Epoch: 743, Loss: 1.0130, Train: 71.91%, Valid: 70.55% Test: 69.59%\n",
            "Epoch: 744, Loss: 1.0103, Train: 71.95%, Valid: 70.50% Test: 69.42%\n",
            "Epoch: 745, Loss: 1.0109, Train: 71.96%, Valid: 70.53% Test: 69.38%\n",
            "Epoch: 746, Loss: 1.0112, Train: 71.93%, Valid: 70.53% Test: 69.43%\n",
            "Epoch: 747, Loss: 1.0145, Train: 71.94%, Valid: 70.54% Test: 69.54%\n",
            "Epoch: 748, Loss: 1.0099, Train: 71.92%, Valid: 70.51% Test: 69.53%\n",
            "Epoch: 749, Loss: 1.0080, Train: 71.99%, Valid: 70.49% Test: 69.37%\n",
            "Epoch: 750, Loss: 1.0094, Train: 71.94%, Valid: 70.56% Test: 69.45%\n",
            "Epoch: 751, Loss: 1.0102, Train: 71.94%, Valid: 70.52% Test: 69.45%\n",
            "Epoch: 752, Loss: 1.0119, Train: 71.94%, Valid: 70.53% Test: 69.48%\n",
            "Epoch: 753, Loss: 1.0115, Train: 71.97%, Valid: 70.52% Test: 69.57%\n",
            "Epoch: 754, Loss: 1.0118, Train: 71.98%, Valid: 70.50% Test: 69.54%\n",
            "Epoch: 755, Loss: 1.0087, Train: 72.00%, Valid: 70.50% Test: 69.47%\n",
            "Epoch: 756, Loss: 1.0092, Train: 71.99%, Valid: 70.57% Test: 69.48%\n",
            "Epoch: 757, Loss: 1.0084, Train: 71.95%, Valid: 70.65% Test: 69.62%\n",
            "Epoch: 758, Loss: 1.0090, Train: 72.00%, Valid: 70.55% Test: 69.60%\n",
            "Epoch: 759, Loss: 1.0086, Train: 72.01%, Valid: 70.48% Test: 69.44%\n",
            "Epoch: 760, Loss: 1.0116, Train: 72.01%, Valid: 70.49% Test: 69.44%\n",
            "Epoch: 761, Loss: 1.0073, Train: 72.00%, Valid: 70.53% Test: 69.57%\n",
            "Epoch: 762, Loss: 1.0079, Train: 72.00%, Valid: 70.54% Test: 69.53%\n",
            "Epoch: 763, Loss: 1.0083, Train: 72.02%, Valid: 70.59% Test: 69.49%\n",
            "Epoch: 764, Loss: 1.0063, Train: 72.02%, Valid: 70.50% Test: 69.41%\n",
            "Epoch: 765, Loss: 1.0056, Train: 72.03%, Valid: 70.52% Test: 69.46%\n",
            "Epoch: 766, Loss: 1.0082, Train: 71.99%, Valid: 70.58% Test: 69.59%\n",
            "Epoch: 767, Loss: 1.0084, Train: 72.02%, Valid: 70.55% Test: 69.60%\n",
            "Epoch: 768, Loss: 1.0049, Train: 72.07%, Valid: 70.55% Test: 69.50%\n",
            "Epoch: 769, Loss: 1.0069, Train: 72.01%, Valid: 70.65% Test: 69.60%\n",
            "Epoch: 770, Loss: 1.0071, Train: 72.02%, Valid: 70.68% Test: 69.72%\n",
            "Epoch: 771, Loss: 1.0052, Train: 72.01%, Valid: 70.63% Test: 69.69%\n",
            "Epoch: 772, Loss: 1.0045, Train: 72.08%, Valid: 70.55% Test: 69.47%\n",
            "Epoch: 773, Loss: 1.0062, Train: 72.07%, Valid: 70.51% Test: 69.34%\n",
            "Epoch: 774, Loss: 1.0059, Train: 72.05%, Valid: 70.58% Test: 69.51%\n",
            "Epoch: 775, Loss: 1.0058, Train: 72.01%, Valid: 70.64% Test: 69.71%\n",
            "Epoch: 776, Loss: 1.0030, Train: 72.05%, Valid: 70.67% Test: 69.63%\n",
            "Epoch: 777, Loss: 1.0066, Train: 72.08%, Valid: 70.54% Test: 69.35%\n",
            "Epoch: 778, Loss: 1.0074, Train: 72.11%, Valid: 70.56% Test: 69.33%\n",
            "Epoch: 779, Loss: 1.0037, Train: 72.08%, Valid: 70.58% Test: 69.63%\n",
            "Epoch: 780, Loss: 1.0060, Train: 72.06%, Valid: 70.59% Test: 69.72%\n",
            "Epoch: 781, Loss: 1.0061, Train: 72.08%, Valid: 70.65% Test: 69.66%\n",
            "Epoch: 782, Loss: 1.0033, Train: 72.12%, Valid: 70.58% Test: 69.62%\n",
            "Epoch: 783, Loss: 1.0047, Train: 72.12%, Valid: 70.58% Test: 69.55%\n",
            "Epoch: 784, Loss: 1.0037, Train: 72.11%, Valid: 70.55% Test: 69.53%\n",
            "Epoch: 785, Loss: 1.0012, Train: 72.10%, Valid: 70.57% Test: 69.58%\n",
            "Epoch: 786, Loss: 1.0034, Train: 72.12%, Valid: 70.54% Test: 69.58%\n",
            "Epoch: 787, Loss: 1.0060, Train: 72.10%, Valid: 70.53% Test: 69.48%\n",
            "Epoch: 788, Loss: 1.0041, Train: 72.14%, Valid: 70.57% Test: 69.46%\n",
            "Epoch: 789, Loss: 1.0033, Train: 72.16%, Valid: 70.57% Test: 69.48%\n",
            "Epoch: 790, Loss: 1.0013, Train: 72.17%, Valid: 70.59% Test: 69.57%\n",
            "Epoch: 791, Loss: 0.9997, Train: 72.11%, Valid: 70.60% Test: 69.60%\n",
            "Epoch: 792, Loss: 1.0015, Train: 72.15%, Valid: 70.61% Test: 69.68%\n",
            "Epoch: 793, Loss: 1.0034, Train: 72.14%, Valid: 70.66% Test: 69.67%\n",
            "Epoch: 794, Loss: 1.0032, Train: 72.14%, Valid: 70.61% Test: 69.52%\n",
            "Epoch: 795, Loss: 1.0023, Train: 72.18%, Valid: 70.54% Test: 69.52%\n",
            "Epoch: 796, Loss: 0.9987, Train: 72.18%, Valid: 70.61% Test: 69.48%\n",
            "Epoch: 797, Loss: 0.9997, Train: 72.12%, Valid: 70.73% Test: 69.63%\n",
            "Epoch: 798, Loss: 1.0039, Train: 72.18%, Valid: 70.70% Test: 69.60%\n",
            "Epoch: 799, Loss: 1.0019, Train: 72.19%, Valid: 70.59% Test: 69.55%\n",
            "Epoch: 800, Loss: 1.0001, Train: 72.20%, Valid: 70.56% Test: 69.42%\n",
            "Epoch: 801, Loss: 1.0036, Train: 72.19%, Valid: 70.60% Test: 69.67%\n",
            "Epoch: 802, Loss: 0.9995, Train: 72.19%, Valid: 70.69% Test: 69.75%\n",
            "Epoch: 803, Loss: 1.0012, Train: 72.19%, Valid: 70.64% Test: 69.70%\n",
            "Epoch: 804, Loss: 1.0001, Train: 72.28%, Valid: 70.57% Test: 69.40%\n",
            "Epoch: 805, Loss: 1.0036, Train: 72.21%, Valid: 70.52% Test: 69.32%\n",
            "Epoch: 806, Loss: 1.0001, Train: 72.23%, Valid: 70.55% Test: 69.57%\n",
            "Epoch: 807, Loss: 0.9956, Train: 72.17%, Valid: 70.64% Test: 69.77%\n",
            "Epoch: 808, Loss: 1.0016, Train: 72.21%, Valid: 70.66% Test: 69.76%\n",
            "Epoch: 809, Loss: 0.9989, Train: 72.23%, Valid: 70.58% Test: 69.46%\n",
            "Epoch: 810, Loss: 1.0007, Train: 72.24%, Valid: 70.53% Test: 69.35%\n",
            "Epoch: 811, Loss: 1.0013, Train: 72.24%, Valid: 70.58% Test: 69.58%\n",
            "Epoch: 812, Loss: 0.9998, Train: 72.21%, Valid: 70.60% Test: 69.65%\n",
            "Epoch: 813, Loss: 1.0001, Train: 72.27%, Valid: 70.59% Test: 69.50%\n",
            "Epoch: 814, Loss: 0.9974, Train: 72.28%, Valid: 70.59% Test: 69.53%\n",
            "Epoch: 815, Loss: 0.9979, Train: 72.25%, Valid: 70.63% Test: 69.70%\n",
            "Epoch: 816, Loss: 1.0003, Train: 72.24%, Valid: 70.62% Test: 69.64%\n",
            "Epoch: 817, Loss: 0.9997, Train: 72.28%, Valid: 70.54% Test: 69.59%\n",
            "Epoch: 818, Loss: 1.0001, Train: 72.29%, Valid: 70.62% Test: 69.52%\n",
            "Epoch: 819, Loss: 0.9966, Train: 72.30%, Valid: 70.61% Test: 69.61%\n",
            "Epoch: 820, Loss: 0.9963, Train: 72.27%, Valid: 70.69% Test: 69.66%\n",
            "Epoch: 821, Loss: 0.9964, Train: 72.28%, Valid: 70.65% Test: 69.69%\n",
            "Epoch: 822, Loss: 0.9969, Train: 72.27%, Valid: 70.69% Test: 69.78%\n",
            "Epoch: 823, Loss: 0.9961, Train: 72.31%, Valid: 70.74% Test: 69.81%\n",
            "Epoch: 824, Loss: 0.9978, Train: 72.31%, Valid: 70.69% Test: 69.73%\n",
            "Epoch: 825, Loss: 0.9960, Train: 72.35%, Valid: 70.59% Test: 69.56%\n",
            "Epoch: 826, Loss: 0.9977, Train: 72.31%, Valid: 70.59% Test: 69.64%\n",
            "Epoch: 827, Loss: 0.9992, Train: 72.31%, Valid: 70.68% Test: 69.68%\n",
            "Epoch: 828, Loss: 1.0005, Train: 72.30%, Valid: 70.64% Test: 69.77%\n",
            "Epoch: 829, Loss: 0.9976, Train: 72.31%, Valid: 70.64% Test: 69.73%\n",
            "Epoch: 830, Loss: 0.9953, Train: 72.32%, Valid: 70.64% Test: 69.72%\n",
            "Epoch: 831, Loss: 0.9991, Train: 72.32%, Valid: 70.67% Test: 69.76%\n",
            "Epoch: 832, Loss: 0.9964, Train: 72.37%, Valid: 70.66% Test: 69.68%\n",
            "Epoch: 833, Loss: 0.9941, Train: 72.36%, Valid: 70.70% Test: 69.59%\n",
            "Epoch: 834, Loss: 0.9983, Train: 72.37%, Valid: 70.62% Test: 69.53%\n",
            "Epoch: 835, Loss: 0.9946, Train: 72.37%, Valid: 70.66% Test: 69.70%\n",
            "Epoch: 836, Loss: 0.9964, Train: 72.33%, Valid: 70.71% Test: 69.68%\n",
            "Epoch: 837, Loss: 0.9960, Train: 72.36%, Valid: 70.68% Test: 69.66%\n",
            "Epoch: 838, Loss: 0.9941, Train: 72.37%, Valid: 70.65% Test: 69.59%\n",
            "Epoch: 839, Loss: 0.9959, Train: 72.34%, Valid: 70.65% Test: 69.62%\n",
            "Epoch: 840, Loss: 0.9967, Train: 72.38%, Valid: 70.67% Test: 69.77%\n",
            "Epoch: 841, Loss: 0.9976, Train: 72.36%, Valid: 70.63% Test: 69.69%\n",
            "Epoch: 842, Loss: 0.9942, Train: 72.36%, Valid: 70.58% Test: 69.53%\n",
            "Epoch: 843, Loss: 0.9933, Train: 72.38%, Valid: 70.60% Test: 69.62%\n",
            "Epoch: 844, Loss: 0.9953, Train: 72.40%, Valid: 70.72% Test: 69.77%\n",
            "Epoch: 845, Loss: 0.9963, Train: 72.41%, Valid: 70.72% Test: 69.67%\n",
            "Epoch: 846, Loss: 0.9947, Train: 72.45%, Valid: 70.57% Test: 69.50%\n",
            "Epoch: 847, Loss: 0.9953, Train: 72.45%, Valid: 70.59% Test: 69.56%\n",
            "Epoch: 848, Loss: 0.9938, Train: 72.39%, Valid: 70.76% Test: 69.82%\n",
            "Epoch: 849, Loss: 0.9941, Train: 72.40%, Valid: 70.75% Test: 69.84%\n",
            "Epoch: 850, Loss: 0.9934, Train: 72.44%, Valid: 70.61% Test: 69.71%\n",
            "Epoch: 851, Loss: 0.9948, Train: 72.44%, Valid: 70.60% Test: 69.53%\n",
            "Epoch: 852, Loss: 0.9893, Train: 72.39%, Valid: 70.65% Test: 69.81%\n",
            "Epoch: 853, Loss: 0.9912, Train: 72.41%, Valid: 70.72% Test: 69.86%\n",
            "Epoch: 854, Loss: 0.9913, Train: 72.43%, Valid: 70.64% Test: 69.71%\n",
            "Epoch: 855, Loss: 0.9942, Train: 72.45%, Valid: 70.59% Test: 69.53%\n",
            "Epoch: 856, Loss: 0.9899, Train: 72.43%, Valid: 70.64% Test: 69.67%\n",
            "Epoch: 857, Loss: 0.9920, Train: 72.45%, Valid: 70.68% Test: 69.86%\n",
            "Epoch: 858, Loss: 0.9898, Train: 72.43%, Valid: 70.76% Test: 69.90%\n",
            "Epoch: 859, Loss: 0.9918, Train: 72.45%, Valid: 70.76% Test: 69.84%\n",
            "Epoch: 860, Loss: 0.9935, Train: 72.44%, Valid: 70.62% Test: 69.68%\n",
            "Epoch: 861, Loss: 0.9903, Train: 72.43%, Valid: 70.68% Test: 69.72%\n",
            "Epoch: 862, Loss: 0.9888, Train: 72.41%, Valid: 70.66% Test: 69.71%\n",
            "Epoch: 863, Loss: 0.9920, Train: 72.42%, Valid: 70.69% Test: 69.76%\n",
            "Epoch: 864, Loss: 0.9912, Train: 72.47%, Valid: 70.72% Test: 69.77%\n",
            "Epoch: 865, Loss: 0.9916, Train: 72.46%, Valid: 70.67% Test: 69.73%\n",
            "Epoch: 866, Loss: 0.9906, Train: 72.47%, Valid: 70.59% Test: 69.54%\n",
            "Epoch: 867, Loss: 0.9930, Train: 72.49%, Valid: 70.65% Test: 69.73%\n",
            "Epoch: 868, Loss: 0.9923, Train: 72.46%, Valid: 70.72% Test: 69.67%\n",
            "Epoch: 869, Loss: 0.9903, Train: 72.45%, Valid: 70.81% Test: 69.82%\n",
            "Epoch: 870, Loss: 0.9922, Train: 72.45%, Valid: 70.70% Test: 69.71%\n",
            "Epoch: 871, Loss: 0.9876, Train: 72.51%, Valid: 70.68% Test: 69.80%\n",
            "Epoch: 872, Loss: 0.9921, Train: 72.50%, Valid: 70.72% Test: 69.78%\n",
            "Epoch: 873, Loss: 0.9894, Train: 72.53%, Valid: 70.71% Test: 69.79%\n",
            "Epoch: 874, Loss: 0.9873, Train: 72.47%, Valid: 70.74% Test: 69.65%\n",
            "Epoch: 875, Loss: 0.9925, Train: 72.52%, Valid: 70.72% Test: 69.72%\n",
            "Epoch: 876, Loss: 0.9879, Train: 72.49%, Valid: 70.68% Test: 69.79%\n",
            "Epoch: 877, Loss: 0.9913, Train: 72.48%, Valid: 70.70% Test: 69.88%\n",
            "Epoch: 878, Loss: 0.9869, Train: 72.52%, Valid: 70.71% Test: 69.85%\n",
            "Epoch: 879, Loss: 0.9896, Train: 72.58%, Valid: 70.72% Test: 69.53%\n",
            "Epoch: 880, Loss: 0.9889, Train: 72.58%, Valid: 70.69% Test: 69.61%\n",
            "Epoch: 881, Loss: 0.9901, Train: 72.51%, Valid: 70.76% Test: 69.85%\n",
            "Epoch: 882, Loss: 0.9913, Train: 72.51%, Valid: 70.75% Test: 69.98%\n",
            "Epoch: 883, Loss: 0.9863, Train: 72.54%, Valid: 70.70% Test: 69.75%\n",
            "Epoch: 884, Loss: 0.9875, Train: 72.56%, Valid: 70.57% Test: 69.39%\n",
            "Epoch: 885, Loss: 0.9888, Train: 72.55%, Valid: 70.65% Test: 69.60%\n",
            "Epoch: 886, Loss: 0.9866, Train: 72.51%, Valid: 70.80% Test: 70.01%\n",
            "Epoch: 887, Loss: 0.9885, Train: 72.50%, Valid: 70.79% Test: 69.99%\n",
            "Epoch: 888, Loss: 0.9899, Train: 72.58%, Valid: 70.68% Test: 69.66%\n",
            "Epoch: 889, Loss: 0.9866, Train: 72.63%, Valid: 70.74% Test: 69.63%\n",
            "Epoch: 890, Loss: 0.9914, Train: 72.57%, Valid: 70.76% Test: 69.79%\n",
            "Epoch: 891, Loss: 0.9858, Train: 72.58%, Valid: 70.70% Test: 69.85%\n",
            "Epoch: 892, Loss: 0.9859, Train: 72.60%, Valid: 70.74% Test: 69.86%\n",
            "Epoch: 893, Loss: 0.9847, Train: 72.61%, Valid: 70.68% Test: 69.64%\n",
            "Epoch: 894, Loss: 0.9886, Train: 72.58%, Valid: 70.70% Test: 69.78%\n",
            "Epoch: 895, Loss: 0.9870, Train: 72.56%, Valid: 70.71% Test: 69.85%\n",
            "Epoch: 896, Loss: 0.9887, Train: 72.59%, Valid: 70.69% Test: 69.72%\n",
            "Epoch: 897, Loss: 0.9879, Train: 72.64%, Valid: 70.73% Test: 69.59%\n",
            "Epoch: 898, Loss: 0.9865, Train: 72.64%, Valid: 70.74% Test: 69.77%\n",
            "Epoch: 899, Loss: 0.9873, Train: 72.59%, Valid: 70.86% Test: 69.94%\n",
            "Epoch: 900, Loss: 0.9828, Train: 72.62%, Valid: 70.77% Test: 69.80%\n",
            "Epoch: 901, Loss: 0.9854, Train: 72.64%, Valid: 70.69% Test: 69.66%\n",
            "Epoch: 902, Loss: 0.9868, Train: 72.67%, Valid: 70.73% Test: 69.76%\n",
            "Epoch: 903, Loss: 0.9870, Train: 72.65%, Valid: 70.81% Test: 69.94%\n",
            "Epoch: 904, Loss: 0.9869, Train: 72.63%, Valid: 70.79% Test: 69.92%\n",
            "Epoch: 905, Loss: 0.9812, Train: 72.65%, Valid: 70.80% Test: 69.77%\n",
            "Epoch: 906, Loss: 0.9799, Train: 72.67%, Valid: 70.80% Test: 69.64%\n",
            "Epoch: 907, Loss: 0.9833, Train: 72.66%, Valid: 70.81% Test: 69.75%\n",
            "Epoch: 908, Loss: 0.9824, Train: 72.62%, Valid: 70.89% Test: 69.99%\n",
            "Epoch: 909, Loss: 0.9816, Train: 72.65%, Valid: 70.84% Test: 69.89%\n",
            "Epoch: 910, Loss: 0.9824, Train: 72.68%, Valid: 70.78% Test: 69.65%\n",
            "Epoch: 911, Loss: 0.9837, Train: 72.70%, Valid: 70.78% Test: 69.64%\n",
            "Epoch: 912, Loss: 0.9796, Train: 72.68%, Valid: 70.84% Test: 69.87%\n",
            "Epoch: 913, Loss: 0.9839, Train: 72.66%, Valid: 70.82% Test: 69.94%\n",
            "Epoch: 914, Loss: 0.9840, Train: 72.69%, Valid: 70.88% Test: 69.83%\n",
            "Epoch: 915, Loss: 0.9823, Train: 72.72%, Valid: 70.79% Test: 69.80%\n",
            "Epoch: 916, Loss: 0.9840, Train: 72.68%, Valid: 70.85% Test: 69.92%\n",
            "Epoch: 917, Loss: 0.9809, Train: 72.69%, Valid: 70.80% Test: 69.95%\n",
            "Epoch: 918, Loss: 0.9816, Train: 72.74%, Valid: 70.84% Test: 69.83%\n",
            "Epoch: 919, Loss: 0.9822, Train: 72.72%, Valid: 70.89% Test: 69.86%\n",
            "Epoch: 920, Loss: 0.9795, Train: 72.71%, Valid: 70.83% Test: 69.90%\n",
            "Epoch: 921, Loss: 0.9817, Train: 72.68%, Valid: 70.82% Test: 69.77%\n",
            "Epoch: 922, Loss: 0.9811, Train: 72.70%, Valid: 70.76% Test: 69.61%\n",
            "Epoch: 923, Loss: 0.9807, Train: 72.77%, Valid: 70.87% Test: 69.78%\n",
            "Epoch: 924, Loss: 0.9842, Train: 72.74%, Valid: 70.91% Test: 69.95%\n",
            "Epoch: 925, Loss: 0.9817, Train: 72.72%, Valid: 70.83% Test: 69.96%\n",
            "Epoch: 926, Loss: 0.9807, Train: 72.70%, Valid: 70.77% Test: 69.89%\n",
            "Epoch: 927, Loss: 0.9830, Train: 72.74%, Valid: 70.79% Test: 69.87%\n",
            "Epoch: 928, Loss: 0.9804, Train: 72.76%, Valid: 70.87% Test: 70.08%\n",
            "Epoch: 929, Loss: 0.9827, Train: 72.74%, Valid: 70.91% Test: 70.12%\n",
            "Epoch: 930, Loss: 0.9811, Train: 72.76%, Valid: 70.82% Test: 69.81%\n",
            "Epoch: 931, Loss: 0.9800, Train: 72.77%, Valid: 70.72% Test: 69.50%\n",
            "Epoch: 932, Loss: 0.9815, Train: 72.71%, Valid: 70.78% Test: 69.72%\n",
            "Epoch: 933, Loss: 0.9789, Train: 72.67%, Valid: 70.93% Test: 70.00%\n",
            "Epoch: 934, Loss: 0.9825, Train: 72.71%, Valid: 70.86% Test: 70.00%\n",
            "Epoch: 935, Loss: 0.9809, Train: 72.76%, Valid: 70.70% Test: 69.49%\n",
            "Epoch: 936, Loss: 0.9815, Train: 72.78%, Valid: 70.65% Test: 69.46%\n",
            "Epoch: 937, Loss: 0.9798, Train: 72.81%, Valid: 70.82% Test: 69.91%\n",
            "Epoch: 938, Loss: 0.9787, Train: 72.78%, Valid: 70.96% Test: 70.11%\n",
            "Epoch: 939, Loss: 0.9823, Train: 72.75%, Valid: 70.89% Test: 70.01%\n",
            "Epoch: 940, Loss: 0.9773, Train: 72.80%, Valid: 70.80% Test: 69.76%\n",
            "Epoch: 941, Loss: 0.9795, Train: 72.75%, Valid: 70.86% Test: 69.83%\n",
            "Epoch: 942, Loss: 0.9822, Train: 72.78%, Valid: 70.85% Test: 69.87%\n",
            "Epoch: 943, Loss: 0.9788, Train: 72.84%, Valid: 70.83% Test: 69.79%\n",
            "Epoch: 944, Loss: 0.9755, Train: 72.79%, Valid: 70.79% Test: 69.87%\n",
            "Epoch: 945, Loss: 0.9790, Train: 72.80%, Valid: 70.82% Test: 69.92%\n",
            "Epoch: 946, Loss: 0.9771, Train: 72.82%, Valid: 70.90% Test: 69.96%\n",
            "Epoch: 947, Loss: 0.9787, Train: 72.83%, Valid: 70.78% Test: 69.72%\n",
            "Epoch: 948, Loss: 0.9797, Train: 72.86%, Valid: 70.73% Test: 69.58%\n",
            "Epoch: 949, Loss: 0.9801, Train: 72.80%, Valid: 70.84% Test: 69.90%\n",
            "Epoch: 950, Loss: 0.9780, Train: 72.77%, Valid: 70.87% Test: 69.98%\n",
            "Epoch: 951, Loss: 0.9789, Train: 72.82%, Valid: 70.79% Test: 69.74%\n",
            "Epoch: 952, Loss: 0.9763, Train: 72.88%, Valid: 70.83% Test: 69.75%\n",
            "Epoch: 953, Loss: 0.9785, Train: 72.85%, Valid: 70.87% Test: 69.93%\n",
            "Epoch: 954, Loss: 0.9756, Train: 72.83%, Valid: 70.88% Test: 69.97%\n",
            "Epoch: 955, Loss: 0.9759, Train: 72.81%, Valid: 70.84% Test: 70.06%\n",
            "Epoch: 956, Loss: 0.9777, Train: 72.88%, Valid: 70.84% Test: 69.81%\n",
            "Epoch: 957, Loss: 0.9779, Train: 72.87%, Valid: 70.89% Test: 69.83%\n",
            "Epoch: 958, Loss: 0.9748, Train: 72.87%, Valid: 70.93% Test: 69.90%\n",
            "Epoch: 959, Loss: 0.9782, Train: 72.82%, Valid: 70.98% Test: 70.04%\n",
            "Epoch: 960, Loss: 0.9762, Train: 72.85%, Valid: 70.97% Test: 70.01%\n",
            "Epoch: 961, Loss: 0.9774, Train: 72.85%, Valid: 70.92% Test: 70.05%\n",
            "Epoch: 962, Loss: 0.9788, Train: 72.84%, Valid: 70.88% Test: 69.95%\n",
            "Epoch: 963, Loss: 0.9769, Train: 72.85%, Valid: 70.88% Test: 69.94%\n",
            "Epoch: 964, Loss: 0.9749, Train: 72.85%, Valid: 70.84% Test: 69.86%\n",
            "Epoch: 965, Loss: 0.9750, Train: 72.86%, Valid: 70.83% Test: 69.72%\n",
            "Epoch: 966, Loss: 0.9744, Train: 72.88%, Valid: 70.89% Test: 69.85%\n",
            "Epoch: 967, Loss: 0.9742, Train: 72.87%, Valid: 70.95% Test: 70.07%\n",
            "Epoch: 968, Loss: 0.9754, Train: 72.87%, Valid: 70.93% Test: 70.14%\n",
            "Epoch: 969, Loss: 0.9783, Train: 72.93%, Valid: 70.82% Test: 69.84%\n",
            "Epoch: 970, Loss: 0.9762, Train: 72.91%, Valid: 70.88% Test: 69.79%\n",
            "Epoch: 971, Loss: 0.9762, Train: 72.94%, Valid: 70.88% Test: 69.88%\n",
            "Epoch: 972, Loss: 0.9743, Train: 72.86%, Valid: 70.93% Test: 70.06%\n",
            "Epoch: 973, Loss: 0.9768, Train: 72.88%, Valid: 70.98% Test: 70.05%\n",
            "Epoch: 974, Loss: 0.9759, Train: 72.93%, Valid: 70.87% Test: 69.81%\n",
            "Epoch: 975, Loss: 0.9743, Train: 72.93%, Valid: 70.93% Test: 69.92%\n",
            "Epoch: 976, Loss: 0.9727, Train: 72.91%, Valid: 70.92% Test: 70.03%\n",
            "Epoch: 977, Loss: 0.9753, Train: 72.92%, Valid: 70.96% Test: 70.07%\n",
            "Epoch: 978, Loss: 0.9706, Train: 72.92%, Valid: 70.90% Test: 69.94%\n",
            "Epoch: 979, Loss: 0.9745, Train: 72.94%, Valid: 70.94% Test: 69.93%\n",
            "Epoch: 980, Loss: 0.9745, Train: 72.96%, Valid: 70.89% Test: 69.76%\n",
            "Epoch: 981, Loss: 0.9731, Train: 72.95%, Valid: 70.96% Test: 69.81%\n",
            "Epoch: 982, Loss: 0.9742, Train: 72.93%, Valid: 70.96% Test: 69.94%\n",
            "Epoch: 983, Loss: 0.9745, Train: 72.97%, Valid: 70.91% Test: 69.89%\n",
            "Epoch: 984, Loss: 0.9734, Train: 72.99%, Valid: 70.90% Test: 69.92%\n",
            "Epoch: 985, Loss: 0.9727, Train: 72.93%, Valid: 70.88% Test: 69.95%\n",
            "Epoch: 986, Loss: 0.9743, Train: 72.99%, Valid: 70.97% Test: 70.00%\n",
            "Epoch: 987, Loss: 0.9733, Train: 72.96%, Valid: 70.94% Test: 69.94%\n",
            "Epoch: 988, Loss: 0.9725, Train: 72.96%, Valid: 70.92% Test: 69.85%\n",
            "Epoch: 989, Loss: 0.9720, Train: 72.97%, Valid: 70.90% Test: 69.91%\n",
            "Epoch: 990, Loss: 0.9727, Train: 72.98%, Valid: 71.02% Test: 70.05%\n",
            "Epoch: 991, Loss: 0.9718, Train: 73.00%, Valid: 71.00% Test: 70.08%\n",
            "Epoch: 992, Loss: 0.9717, Train: 73.01%, Valid: 70.93% Test: 69.92%\n",
            "Epoch: 993, Loss: 0.9715, Train: 72.97%, Valid: 70.90% Test: 69.86%\n",
            "Epoch: 994, Loss: 0.9714, Train: 72.96%, Valid: 70.96% Test: 70.04%\n",
            "Epoch: 995, Loss: 0.9735, Train: 72.98%, Valid: 70.97% Test: 70.06%\n",
            "Epoch: 996, Loss: 0.9707, Train: 73.02%, Valid: 70.93% Test: 70.00%\n",
            "Epoch: 997, Loss: 0.9728, Train: 73.03%, Valid: 71.06% Test: 69.98%\n",
            "Epoch: 998, Loss: 0.9700, Train: 72.97%, Valid: 71.01% Test: 70.15%\n",
            "Epoch: 999, Loss: 0.9703, Train: 72.98%, Valid: 71.02% Test: 70.27%\n",
            "Best Test accuracy is 0.6998333436207641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataset with LM initialized embeddings\n",
        "\n",
        "We finetune an MPNet model on the ogbn-arxiv task using the title+abstract as an input feature. We then use the embeddings generated by this model as our initial node features for the GNN.\n",
        "\n",
        "The embeddings are available at https://drive.google.com/file/d/184qquWQuXbSog2PDZMG5xuWZU043hn3u/view?usp=sharing, please create a copy of this file in your Google Drive account and update the filepath in the code accordingly\n",
        "\n"
      ],
      "metadata": {
        "id": "894s0kfsAwTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filepath= \"/content/drive/Shareddrives/CS224W Project/graph_embeddings/finetuned/mpnet_arxiv.pkl\"\n",
        "embs = pickle.load(open(filepath, \"rb\"))\n"
      ],
      "metadata": {
        "id": "eRBezUs8Hfkk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LoadDataLMInit(LoadData):\n",
        "    \"\"\"\n",
        "    A class extending LoadData to load graph data with initial node embeddings.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    embs : array-like\n",
        "        An array of node embeddings used to initialize the node features in the graph dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embs):\n",
        "        \"\"\"\n",
        "        The constructor for LoadDataLMInit class.\n",
        "\n",
        "        Initializes the LoadDataLMInit instance with the provided node embeddings.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        embs : array-like\n",
        "            An array-like structure containing node embeddings. Each element in the array\n",
        "            represents the embedding of a node in the graph.\n",
        "        \"\"\"\n",
        "        self.embs = embs\n",
        "        super(LoadDataLMInit, self).__init__()\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"\n",
        "        Loads the graph dataset with initial node embeddings, applies transformations, and retrieves split indices.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        data : PyG data object\n",
        "            The graph data object with initialized node features.\n",
        "        split_idx : dict\n",
        "            A dictionary containing the indices for train, validation, and test splits.\n",
        "        num_classes : int\n",
        "            The number of classes in the dataset.\n",
        "        \"\"\"\n",
        "        dataset = PygNodePropPredDataset(name=DATASET)\n",
        "        data = dataset[0]\n",
        "        embs = torch.nn.functional.normalize(torch.tensor(self.embs), dim=-1)\n",
        "        data.x = torch.tensor(self.embs)\n",
        "        split_idx = dataset.get_idx_split()\n",
        "        transform = T.Compose([T.ToUndirected(), T.ToSparseTensor()])\n",
        "        data = transform(data)\n",
        "        num_classes = dataset.num_classes\n",
        "        return data, split_idx, num_classes\n"
      ],
      "metadata": {
        "id": "orA5gxchAduL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the pipeline with LM initialized embeddings"
      ],
      "metadata": {
        "id": "GED4eoFrIZY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_data=LoadDataLMInit(embs)\n",
        "lminit_acc, lminit_model = train_loop(load_data, loss_obj, hyperparams)"
      ],
      "metadata": {
        "id": "q__q0nOdIeIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a428e9e-39fa-4de2-c713-6922d0bcde2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 00, Loss: 3.6924, Train: 1.83%, Valid: 3.60% Test: 4.25%\n",
            "Epoch: 01, Loss: 3.6616, Train: 4.53%, Valid: 9.02% Test: 9.91%\n",
            "Epoch: 02, Loss: 3.6309, Train: 30.15%, Valid: 39.30% Test: 37.35%\n",
            "Epoch: 03, Loss: 3.5958, Train: 46.82%, Valid: 51.64% Test: 50.88%\n",
            "Epoch: 04, Loss: 3.5544, Train: 50.87%, Valid: 54.65% Test: 54.58%\n",
            "Epoch: 05, Loss: 3.5045, Train: 52.80%, Valid: 56.15% Test: 56.55%\n",
            "Epoch: 06, Loss: 3.4433, Train: 52.78%, Valid: 56.52% Test: 57.17%\n",
            "Epoch: 07, Loss: 3.3713, Train: 52.14%, Valid: 56.24% Test: 57.15%\n",
            "Epoch: 08, Loss: 3.2855, Train: 51.21%, Valid: 55.72% Test: 56.90%\n",
            "Epoch: 09, Loss: 3.1847, Train: 50.23%, Valid: 55.10% Test: 56.39%\n",
            "Epoch: 10, Loss: 3.0684, Train: 49.26%, Valid: 54.51% Test: 56.00%\n",
            "Epoch: 11, Loss: 2.9403, Train: 48.30%, Valid: 54.01% Test: 55.61%\n",
            "Epoch: 12, Loss: 2.8038, Train: 47.47%, Valid: 53.41% Test: 55.29%\n",
            "Epoch: 13, Loss: 2.6633, Train: 46.81%, Valid: 52.72% Test: 54.75%\n",
            "Epoch: 14, Loss: 2.5287, Train: 46.13%, Valid: 51.73% Test: 53.99%\n",
            "Epoch: 15, Loss: 2.4108, Train: 45.43%, Valid: 50.53% Test: 52.82%\n",
            "Epoch: 16, Loss: 2.3195, Train: 44.88%, Valid: 49.63% Test: 51.59%\n",
            "Epoch: 17, Loss: 2.2556, Train: 44.73%, Valid: 49.02% Test: 50.42%\n",
            "Epoch: 18, Loss: 2.2093, Train: 45.00%, Valid: 48.59% Test: 49.11%\n",
            "Epoch: 19, Loss: 2.1702, Train: 45.56%, Valid: 48.16% Test: 47.76%\n",
            "Epoch: 20, Loss: 2.1333, Train: 46.59%, Valid: 48.42% Test: 47.44%\n",
            "Epoch: 21, Loss: 2.0910, Train: 48.04%, Valid: 49.53% Test: 48.23%\n",
            "Epoch: 22, Loss: 2.0434, Train: 50.02%, Valid: 51.33% Test: 50.01%\n",
            "Epoch: 23, Loss: 1.9928, Train: 52.42%, Valid: 53.45% Test: 52.31%\n",
            "Epoch: 24, Loss: 1.9393, Train: 54.81%, Valid: 55.89% Test: 55.01%\n",
            "Epoch: 25, Loss: 1.8812, Train: 57.42%, Valid: 58.51% Test: 57.70%\n",
            "Epoch: 26, Loss: 1.8262, Train: 59.50%, Valid: 60.56% Test: 60.10%\n",
            "Epoch: 27, Loss: 1.7806, Train: 60.90%, Valid: 62.08% Test: 61.66%\n",
            "Epoch: 28, Loss: 1.7358, Train: 62.02%, Valid: 63.24% Test: 62.83%\n",
            "Epoch: 29, Loss: 1.6931, Train: 63.09%, Valid: 64.15% Test: 63.63%\n",
            "Epoch: 30, Loss: 1.6558, Train: 64.10%, Valid: 64.86% Test: 64.25%\n",
            "Epoch: 31, Loss: 1.6163, Train: 64.74%, Valid: 65.28% Test: 64.69%\n",
            "Epoch: 32, Loss: 1.5874, Train: 65.17%, Valid: 65.62% Test: 64.98%\n",
            "Epoch: 33, Loss: 1.5461, Train: 65.58%, Valid: 65.97% Test: 65.23%\n",
            "Epoch: 34, Loss: 1.5143, Train: 65.97%, Valid: 66.24% Test: 65.52%\n",
            "Epoch: 35, Loss: 1.4817, Train: 66.45%, Valid: 66.52% Test: 65.77%\n",
            "Epoch: 36, Loss: 1.4459, Train: 66.79%, Valid: 66.85% Test: 66.00%\n",
            "Epoch: 37, Loss: 1.4152, Train: 67.09%, Valid: 67.16% Test: 66.31%\n",
            "Epoch: 38, Loss: 1.3895, Train: 67.36%, Valid: 67.46% Test: 66.65%\n",
            "Epoch: 39, Loss: 1.3587, Train: 67.61%, Valid: 67.69% Test: 66.90%\n",
            "Epoch: 40, Loss: 1.3354, Train: 67.88%, Valid: 67.87% Test: 67.21%\n",
            "Epoch: 41, Loss: 1.3099, Train: 68.11%, Valid: 68.14% Test: 67.54%\n",
            "Epoch: 42, Loss: 1.2862, Train: 68.38%, Valid: 68.39% Test: 67.76%\n",
            "Epoch: 43, Loss: 1.2621, Train: 68.77%, Valid: 68.61% Test: 67.98%\n",
            "Epoch: 44, Loss: 1.2393, Train: 69.14%, Valid: 68.88% Test: 68.27%\n",
            "Epoch: 45, Loss: 1.2211, Train: 69.48%, Valid: 69.17% Test: 68.50%\n",
            "Epoch: 46, Loss: 1.2021, Train: 69.76%, Valid: 69.44% Test: 68.73%\n",
            "Epoch: 47, Loss: 1.1839, Train: 70.05%, Valid: 69.70% Test: 68.87%\n",
            "Epoch: 48, Loss: 1.1679, Train: 70.35%, Valid: 69.94% Test: 69.08%\n",
            "Epoch: 49, Loss: 1.1542, Train: 70.65%, Valid: 70.12% Test: 69.26%\n",
            "Epoch: 50, Loss: 1.1407, Train: 70.95%, Valid: 70.38% Test: 69.54%\n",
            "Epoch: 51, Loss: 1.1238, Train: 71.23%, Valid: 70.65% Test: 69.82%\n",
            "Epoch: 52, Loss: 1.1137, Train: 71.56%, Valid: 70.89% Test: 70.06%\n",
            "Epoch: 53, Loss: 1.0981, Train: 71.83%, Valid: 71.14% Test: 70.29%\n",
            "Epoch: 54, Loss: 1.0873, Train: 72.36%, Valid: 71.50% Test: 70.53%\n",
            "Epoch: 55, Loss: 1.0776, Train: 72.82%, Valid: 71.75% Test: 70.79%\n",
            "Epoch: 56, Loss: 1.0698, Train: 73.13%, Valid: 71.88% Test: 70.93%\n",
            "Epoch: 57, Loss: 1.0556, Train: 73.42%, Valid: 71.99% Test: 71.06%\n",
            "Epoch: 58, Loss: 1.0479, Train: 73.59%, Valid: 72.02% Test: 71.20%\n",
            "Epoch: 59, Loss: 1.0399, Train: 73.74%, Valid: 72.14% Test: 71.33%\n",
            "Epoch: 60, Loss: 1.0323, Train: 73.90%, Valid: 72.23% Test: 71.42%\n",
            "Epoch: 61, Loss: 1.0238, Train: 74.01%, Valid: 72.30% Test: 71.48%\n",
            "Epoch: 62, Loss: 1.0163, Train: 74.13%, Valid: 72.41% Test: 71.62%\n",
            "Epoch: 63, Loss: 1.0047, Train: 74.25%, Valid: 72.51% Test: 71.71%\n",
            "Epoch: 64, Loss: 1.0003, Train: 74.34%, Valid: 72.59% Test: 71.79%\n",
            "Epoch: 65, Loss: 0.9951, Train: 74.45%, Valid: 72.65% Test: 71.88%\n",
            "Epoch: 66, Loss: 0.9886, Train: 74.55%, Valid: 72.71% Test: 71.94%\n",
            "Epoch: 67, Loss: 0.9826, Train: 74.67%, Valid: 72.76% Test: 72.03%\n",
            "Epoch: 68, Loss: 0.9767, Train: 74.77%, Valid: 72.89% Test: 72.12%\n",
            "Epoch: 69, Loss: 0.9710, Train: 74.88%, Valid: 72.96% Test: 72.20%\n",
            "Epoch: 70, Loss: 0.9677, Train: 74.99%, Valid: 73.05% Test: 72.27%\n",
            "Epoch: 71, Loss: 0.9594, Train: 75.12%, Valid: 73.10% Test: 72.35%\n",
            "Epoch: 72, Loss: 0.9558, Train: 75.26%, Valid: 73.17% Test: 72.45%\n",
            "Epoch: 73, Loss: 0.9503, Train: 75.36%, Valid: 73.26% Test: 72.50%\n",
            "Epoch: 74, Loss: 0.9447, Train: 75.48%, Valid: 73.32% Test: 72.61%\n",
            "Epoch: 75, Loss: 0.9412, Train: 75.56%, Valid: 73.39% Test: 72.72%\n",
            "Epoch: 76, Loss: 0.9388, Train: 75.61%, Valid: 73.49% Test: 72.82%\n",
            "Epoch: 77, Loss: 0.9331, Train: 75.68%, Valid: 73.53% Test: 72.93%\n",
            "Epoch: 78, Loss: 0.9292, Train: 75.79%, Valid: 73.64% Test: 72.97%\n",
            "Epoch: 79, Loss: 0.9270, Train: 75.89%, Valid: 73.70% Test: 73.07%\n",
            "Epoch: 80, Loss: 0.9230, Train: 75.97%, Valid: 73.72% Test: 73.12%\n",
            "Epoch: 81, Loss: 0.9167, Train: 76.06%, Valid: 73.79% Test: 73.21%\n",
            "Epoch: 82, Loss: 0.9138, Train: 76.12%, Valid: 73.84% Test: 73.32%\n",
            "Epoch: 83, Loss: 0.9112, Train: 76.19%, Valid: 73.93% Test: 73.38%\n",
            "Epoch: 84, Loss: 0.9045, Train: 76.27%, Valid: 73.95% Test: 73.43%\n",
            "Epoch: 85, Loss: 0.9030, Train: 76.35%, Valid: 74.00% Test: 73.48%\n",
            "Epoch: 86, Loss: 0.8969, Train: 76.43%, Valid: 74.06% Test: 73.53%\n",
            "Epoch: 87, Loss: 0.8968, Train: 76.51%, Valid: 74.12% Test: 73.60%\n",
            "Epoch: 88, Loss: 0.8899, Train: 76.58%, Valid: 74.19% Test: 73.67%\n",
            "Epoch: 89, Loss: 0.8873, Train: 76.65%, Valid: 74.27% Test: 73.73%\n",
            "Epoch: 90, Loss: 0.8851, Train: 76.71%, Valid: 74.32% Test: 73.80%\n",
            "Epoch: 91, Loss: 0.8792, Train: 76.77%, Valid: 74.35% Test: 73.86%\n",
            "Epoch: 92, Loss: 0.8742, Train: 76.83%, Valid: 74.37% Test: 73.93%\n",
            "Epoch: 93, Loss: 0.8778, Train: 76.90%, Valid: 74.45% Test: 73.97%\n",
            "Epoch: 94, Loss: 0.8735, Train: 76.95%, Valid: 74.54% Test: 73.99%\n",
            "Epoch: 95, Loss: 0.8695, Train: 77.00%, Valid: 74.60% Test: 74.00%\n",
            "Epoch: 96, Loss: 0.8677, Train: 77.07%, Valid: 74.61% Test: 74.04%\n",
            "Epoch: 97, Loss: 0.8644, Train: 77.12%, Valid: 74.69% Test: 74.09%\n",
            "Epoch: 98, Loss: 0.8608, Train: 77.18%, Valid: 74.79% Test: 74.12%\n",
            "Epoch: 99, Loss: 0.8579, Train: 77.23%, Valid: 74.84% Test: 74.16%\n",
            "Epoch: 100, Loss: 0.8533, Train: 77.29%, Valid: 74.89% Test: 74.19%\n",
            "Epoch: 101, Loss: 0.8511, Train: 77.36%, Valid: 74.89% Test: 74.25%\n",
            "Epoch: 102, Loss: 0.8482, Train: 77.41%, Valid: 74.92% Test: 74.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add virtual edges to the Dataset\n",
        "\n",
        "We attempt data augmentation by adding virtual edges, to capture relationships between nodes that have similar titles and abstracts. We add edges between each node and its k nearest neighbours based on the LLM embeddings of the MPNet model explained above. We use the [FAISS](https://github.com/facebookresearch/faiss) library for fast calculation of nearest neighbours."
      ],
      "metadata": {
        "id": "aJab3L_WTQTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "class LoadDataVirtualEdges(LoadData):\n",
        "  \"\"\"\n",
        "    A class extending LoadData to load graph data with additional virtual edges.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    embs : array-like\n",
        "        An array of node embeddings used to find nearest neighbors and create virtual edges.\n",
        "    k : int\n",
        "        The number of nearest neighbors to consider for creating virtual edges.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, embs, k):\n",
        "    self.embs=embs\n",
        "    self.k=k\n",
        "    super(LoadDataVirtualEdges, self).__init__()\n",
        "\n",
        "  def get_nearest_neighbours(self, embs, k):\n",
        "      \"\"\"\n",
        "      Computes the k-nearest neighbors for each node in the graph based on embeddings.\n",
        "\n",
        "      Parameters:\n",
        "      -----------\n",
        "      embs : array-like\n",
        "          An array-like structure containing node embeddings.\n",
        "      k : int\n",
        "          The number of nearest neighbors to find for each node.\n",
        "\n",
        "      Returns:\n",
        "      --------\n",
        "      numpy.ndarray\n",
        "          An array of indices representing the k-nearest neighbors for each node.\n",
        "      \"\"\"\n",
        "      res = faiss.StandardGpuResources()\n",
        "      index = faiss.IndexFlatL2(embs.shape[1])   # build the index\n",
        "      gpu_index_flat = faiss.index_cpu_to_gpu(res, 0, index)\n",
        "      gpu_index_flat.add(embs)                  # add vectors to the index\n",
        "      D, I = gpu_index_flat.search(embs, k+1)     # actual search\n",
        "      return I\n",
        "\n",
        "  def convert_to_edge_index(self, I, k):\n",
        "    \"\"\"\n",
        "    Converts nearest neighbor information into PyTorch Geometric edge indices.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    I : numpy.ndarray\n",
        "        An array of indices representing the nearest neighbors for each node.\n",
        "    k : int\n",
        "        The number of nearest neighbors for each node.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple: (src_nodes, dst_nodes)\n",
        "        Two tensors representing the source and destination nodes of each virtual edge.\n",
        "    \"\"\"\n",
        "    num_nodes = I.shape[0]\n",
        "    src_nodes = torch.arange(num_nodes).repeat_interleave(k).to(device)\n",
        "    dst_nodes = torch.tensor(I[:,1:]).flatten().to(device)\n",
        "    return src_nodes, dst_nodes\n",
        "\n",
        "  def load_data(self):\n",
        "    \"\"\"\n",
        "    Loads the graph dataset, adds virtual edges based on nearest neighbors, applies transformations,\n",
        "    and retrieves split indices.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    data : PyG data object\n",
        "        The graph data object with virtual edges added.\n",
        "    split_idx : dict\n",
        "        A dictionary containing the indices for train, validation, and test splits.\n",
        "    num_classes : int\n",
        "        The number of classes in the dataset.\n",
        "    \"\"\"\n",
        "    dataset = PygNodePropPredDataset(name='ogbn-arxiv')\n",
        "    data = dataset[0]\n",
        "    I=self.get_nearest_neighbours(self.embs,self.k)\n",
        "    S,D=self.convert_to_edge_index(I,k)\n",
        "    data.edge_index=torch.cat((data.edge_index,torch.stack((S,D)).to('cpu')),dim=1)\n",
        "    transform=T.Compose([T.ToUndirected(), T.ToSparseTensor()])\n",
        "    data=transform(data)\n",
        "    num_classes = dataset.num_classes\n",
        "    split_idx = dataset.get_idx_split()\n",
        "    return data, split_idx, num_classes"
      ],
      "metadata": {
        "id": "G9XSkQwHUtbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the pipeline with Virtual Edges added"
      ],
      "metadata": {
        "id": "ryrzlxSaWDCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k=4\n",
        "load_data=LoadDataVirtualEdges(embs,k)\n",
        "virtual_edge_acc, virtual_edge_model = train_loop(load_data, loss_obj, hyperparams)"
      ],
      "metadata": {
        "id": "IkDoMDTHV_9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add KL Divergence Regularization\n",
        "\n",
        "KLLoss extends the basic loss computation (like cross-entropy) by adding a regularization term\n",
        "    based on KL divergence. This is useful in scenarios where one wishes to penalize the divergence\n",
        "    between the model's output distribution and a target distribution, which in this case is provided\n",
        "    by 'lm_logits'. The lambda (lmbda) parameter controls the weight of this regularization term.\n",
        "\n",
        "\n",
        "The MPNET logits are available at https://drive.google.com/file/d/13yY-y7FEpFhe2lOOVX-LMZy58oEFxTS4/view?usp=sharing, please create a copy of this file in your Google Drive account and update the filepath in the code accordingly\n"
      ],
      "metadata": {
        "id": "sN79xLSueMbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filepath_logits='/content/drive/Shareddrives/CS224W Project/graph_embeddings/finetuned/mpnet_logits_arxiv.pkl'\n",
        "lm_logits=pickle.load(open(filepath_logits, \"rb\"))"
      ],
      "metadata": {
        "id": "jlN3AA6IeiyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KLLoss(Loss):\n",
        "    \"\"\"\n",
        "    A class extending Loss to incorporate Kullback-Leibler (KL) divergence as a regularization term.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    lm_logits : torch.Tensor\n",
        "        The logits from a language model or a pre-defined target distribution.\n",
        "    lmbda : float\n",
        "        The weight (lambda) of the KL divergence regularization term in the overall loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lm_logits, lmbda):\n",
        "        \"\"\"\n",
        "        The constructor for KLLoss class.\n",
        "\n",
        "        Initializes the KLLoss instance with provided language model logits and lambda value.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        lm_logits : torch.Tensor\n",
        "            The logits from a language model or a pre-defined target distribution.\n",
        "        lmbda : float\n",
        "            The weight (lambda) of the KL divergence regularization term in the overall loss.\n",
        "        \"\"\"\n",
        "        self.lmbda = lmbda\n",
        "        self.lm_logits = lm_logits\n",
        "        super(KLLoss, self).__init__()\n",
        "\n",
        "    def get_loss(self, out, labels, train_idx):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        -----------\n",
        "        out : torch.Tensor\n",
        "            The output predictions from the neural network model, typically the logits.\n",
        "        labels : torch.Tensor\n",
        "            The true labels for the training data.\n",
        "        train_idx : torch.Tensor or list\n",
        "            The indices of the training data samples.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        torch.Tensor\n",
        "            The computed loss, combining cross-entropy and KL divergence.\n",
        "        \"\"\"\n",
        "        loss = F.cross_entropy(out, labels[train_idx])\n",
        "        reg_penalty = torch.nn.KLDivLoss(log_target=True)\n",
        "        reg = reg_penalty(F.log_softmax(out, dim=1), F.log_softmax(torch.tensor(self.lm_logits[train_idx]).to(device), dim=1))\n",
        "        loss += reg * self.lmbda\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "yyJFavYJeZb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lmbda=1\n",
        "load_data=LoadData()\n",
        "kl_loss_obj=KLLoss(lm_logits,lmbda)\n",
        "kl_best_acc = train_loop(load_data, kl_loss_obj, hyperparams)"
      ],
      "metadata": {
        "id": "pYdJ2QhafNQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensembling\n",
        "\n",
        "We demonstrate an ensemble approach combining a base graph neural network model with scaled logits from a language model.\n",
        "\n"
      ],
      "metadata": {
        "id": "DvdH3QLEiSFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data,split_idx,_=LoadData().load_data()\n",
        "data.to(device)\n",
        "base_model.to(device)\n",
        "final_out = base_model(data.x, data.adj_t)\n",
        "lm_logits = torch.tensor(lm_logits).to(device)\n",
        "max_test_acc=0\n",
        "max_i=0\n",
        "for i in [0.2,0.4,0.6,0.8,1]:\n",
        "  final_out = final_out + i*lm_logits\n",
        "  y_pred = final_out.argmax(dim=-1, keepdim=True)\n",
        "  evaluator = Evaluator(name=DATASET)\n",
        "  test_acc = evaluator.eval({\n",
        "          'y_true': data.y[split_idx['test']],\n",
        "          'y_pred': y_pred[split_idx['test']],\n",
        "      })['acc']\n",
        "  if test_acc>max_test_acc:\n",
        "    max_test_acc=test_acc\n",
        "    max_i=i\n",
        "print(f'Max test acc: {max_test_acc}')"
      ],
      "metadata": {
        "id": "W5oWwEBMiVSZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}