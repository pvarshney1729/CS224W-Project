{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS 224W Project: Text Augmented Graphs (OGBN-Arxiv)\n",
        "\n",
        "### This Colab contains code for experiments using various techniques for combining textual and graph information, tested on the OGBN-Arxiv dataset\n",
        "\n",
        "### For the rest of the code of our project, please refer to https://github.com/pvarshney1729/CS224W-Project"
      ],
      "metadata": {
        "id": "iCdg3xHL2iC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install PyG and other required libraries"
      ],
      "metadata": {
        "id": "V2VKKHTawc7y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q68cWlLkvbxp",
        "outputId": "bbcaf0b6-66ed-4981-f28c-dc3c9294fe42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_scatter-2.1.2%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt21cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_sparse-0.6.18%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.23.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt21cu121\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.2.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.4.0\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.23.5)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.66.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.7)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2023.3.post1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7026 sha256=87bb225ab22637b0c921c0754d771f7352971a8f05db5e00352dbfc9eda349eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.6 outdated-0.2.2\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "torch_version = str(torch.__version__)\n",
        "scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "!pip install torch-scatter -f $scatter_src\n",
        "!pip install torch-sparse -f $sparse_src\n",
        "!pip install torch-geometric\n",
        "!pip install ogb\n",
        "!pip install faiss-gpu\n",
        "import torch_geometric\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import GCNConv, SAGEConv\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "import numpy as np\n",
        "import pickle\n",
        "device = f'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = torch.device(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Link the Colab to your Google Drive\n",
        "\n",
        "We use Google Drive to load pre trained LM embeddings and logits. Instructions to download these embeddings are provided later in the Colab\n"
      ],
      "metadata": {
        "id": "_YUCOZKlHqPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ubd42LTCHpy8",
        "outputId": "865b2ff6-ab86-4add-aa6d-8b2a80eaf7b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add code to load the OGBN Arxiv dataset\n"
      ],
      "metadata": {
        "id": "10BSGFn4xAau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = \"ogbn-arxiv\"\n",
        "class LoadData:\n",
        "    \"\"\"\n",
        "    A class used to load and process graph data using the PyTorch Geometric (PyG) library.\n",
        "    \"\"\"\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"\n",
        "        Loads the graph dataset, applies necessary transformations, and retrieves split indices.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        data: PyG data object\n",
        "            The transformed graph data.\n",
        "        split_idx: dict\n",
        "            A dictionary containing the train, validation, and test split indices.\n",
        "        num_classes: int\n",
        "            The number of classes in the dataset.\n",
        "        \"\"\"\n",
        "        dataset = PygNodePropPredDataset(name=DATASET)\n",
        "        data = dataset[0]\n",
        "        transform = T.Compose([T.ToUndirected(), T.ToSparseTensor()])\n",
        "        data = transform(data)\n",
        "        split_idx = dataset.get_idx_split()\n",
        "        return data, split_idx, dataset.num_classes"
      ],
      "metadata": {
        "id": "-HO3dfR6xJYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the model training and loss function\n",
        "\n"
      ],
      "metadata": {
        "id": "XqzkxMDZ0-QG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss:\n",
        "    \"\"\"\n",
        "    A class for defining loss computation in neural network training.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def get_loss(self, out, labels, train_idx):\n",
        "        \"\"\"\n",
        "        Computes the cross-entropy loss between the output predictions and the true labels.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        out : torch.Tensor\n",
        "            The output predictions from the neural network model, typically the logits.\n",
        "        labels : torch.Tensor\n",
        "            The true labels for the training data.\n",
        "        train_idx : torch.Tensor or list\n",
        "            The indices of the training data samples.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        torch.Tensor\n",
        "            The computed cross-entropy loss.\n",
        "        \"\"\"\n",
        "        return F.cross_entropy(out, labels[train_idx])\n",
        "\n",
        "\n",
        "def train(model, data, train_idx, optimizer, loss_obj):\n",
        "    \"\"\"\n",
        "    Trains a neural network model for one epoch.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : torch.nn.Module\n",
        "        The neural network model to be trained.\n",
        "    data : object\n",
        "        The data object containing features and adjacency information.\n",
        "    train_idx : torch.Tensor or list\n",
        "        The indices of the training data.\n",
        "    optimizer : torch.optim.Optimizer\n",
        "        The optimizer used for updating model weights.\n",
        "    loss_obj : Loss\n",
        "        An instance of the Loss class to compute the loss.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float\n",
        "        The loss value computed for this training epoch.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.adj_t, train_idx)[train_idx]\n",
        "    loss = loss_obj.get_loss(out, data.y.squeeze(1), train_idx)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()"
      ],
      "metadata": {
        "id": "yYWYuh_W1F5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the model testing function\n"
      ],
      "metadata": {
        "id": "naBo4IQ01pmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def test(model, data, split_idx, evaluator):\n",
        "    \"\"\"\n",
        "    Evaluates the performance of a trained model on training, validation, and test datasets.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : torch.nn.Module\n",
        "        The neural network model to be evaluated.\n",
        "    data : object\n",
        "        The data object containing features, adjacency information, and labels.\n",
        "    split_idx : dict\n",
        "        A dictionary with keys 'train', 'valid', and 'test' mapping to the respective data indices.\n",
        "    evaluator : object\n",
        "        An object used to evaluate the model's predictions. It must have an 'eval' method.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple: (train_acc, valid_acc, test_acc)\n",
        "        A tuple containing the accuracy on the training, validation, and test sets.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    out = model(data.x, data.adj_t)\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True).cpu()\n",
        "\n",
        "    # Evaluate accuracy on training, validation, and test sets\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['train']],\n",
        "        'y_pred': y_pred[split_idx['train']],\n",
        "    })['acc']\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['valid']],\n",
        "        'y_pred': y_pred[split_idx['valid']],\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['test']],\n",
        "        'y_pred': y_pred[split_idx['test']],\n",
        "    })['acc']\n",
        "\n",
        "    return train_acc, valid_acc, test_acc\n"
      ],
      "metadata": {
        "id": "oPdpg7lm1WYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Hyperparameters\n",
        "\n",
        "Our code support two models\n",
        "\n",
        "\n",
        "1.   GraphSAGE (torch_geometric.nn.models.GraphSAGE)\n",
        "2.   GCN (torch_geometric.nn.models.GCN)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3kAJ-BLv5d8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparams = {'model' : 'GraphSAGE', 'hidden_layer_size' : 128, 'num_layers' : 3, 'dropout' : 0.5, 'learning_rate' : 1e-3, 'epochs' : 1000}"
      ],
      "metadata": {
        "id": "dCZ9Sgsd5cv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create an end-to-end training loop\n",
        "\n"
      ],
      "metadata": {
        "id": "QCXVcxlI4AX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(load_data_obj, loss_obj, hyperparams):\n",
        "  \"\"\"\n",
        "    Executes the training loop for a graph neural network using specified hyperparameters.\n",
        "\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    load_data_obj : LoadData\n",
        "        An instance of the LoadData class that is used to load the dataset.\n",
        "    loss_obj : Loss\n",
        "        An instance of the Loss class that defines the loss function to be used during training.\n",
        "    hyperparams : dict\n",
        "        A dictionary containing hyperparameters for the model. Expected keys are:\n",
        "        'model' (str), 'hidden_layer_size' (int), 'num_layers' (int), 'out_channels' (int),\n",
        "        'dropout' (float), 'learning_rate' (float), 'epochs' (int).\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple:\n",
        "        best_test_acc (float): The highest test accuracy achieved during training.\n",
        "        model: The trained model instance.\n",
        "  \"\"\"\n",
        "  data, split_idx, num_classes = load_data_obj.load_data()\n",
        "  if hyperparams['model'] == 'GraphSAGE':\n",
        "    model = torch_geometric.nn.models.GraphSAGE(data.x.shape[1], hidden_channels=hyperparams['hidden_layer_size'],\n",
        "                                              num_layers=hyperparams['num_layers'], out_channels=num_classes, dropout=hyperparams['dropout']).to(device)\n",
        "  else:\n",
        "    model = torch_geometric.nn.models.GCN(data.x.shape[1], hidden_channels=hyperparams['hidden_layer_size'],\n",
        "                                              num_layers=hyperparams['num_layers'], out_channels=num_classes, dropout=hyperparams['dropout']).to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=hyperparams['learning_rate'])\n",
        "  evaluator = Evaluator(name=DATASET)\n",
        "  train_idx = split_idx['train']\n",
        "  model.to(device)\n",
        "  data.to(device)\n",
        "  valid_accs=[]\n",
        "  test_accs=[]\n",
        "  for epoch in range(hyperparams['epochs']):\n",
        "    loss = train(model, data, train_idx, optimizer, loss_obj)\n",
        "    result = test(model, data, split_idx, evaluator)\n",
        "    train_acc, valid_acc, test_acc = result\n",
        "    valid_accs.append(valid_acc)\n",
        "    test_accs.append(test_acc)\n",
        "    print(f'Epoch: {epoch:02d}, '\n",
        "          f'Loss: {loss:.4f}, '\n",
        "          f'Train: {100 * train_acc:.2f}%, '\n",
        "          f'Valid: {100 * valid_acc:.2f}% '\n",
        "          f'Test: {100 * test_acc:.2f}%')\n",
        "  best_test_acc = test_accs[np.argmax(valid_accs)]\n",
        "  print(f'Best Test accuracy is {best_test_acc}')\n",
        "  return best_test_acc, model"
      ],
      "metadata": {
        "id": "YHNzhwxC4GYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the training loop using the Base Model"
      ],
      "metadata": {
        "id": "yWbO32opnrBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_data=LoadData()\n",
        "loss_obj=Loss()\n",
        "standard_acc, base_model = train_loop(load_data, loss_obj, hyperparams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DoPDc6jW956G",
        "outputId": "af5bfdcb-b411-40ac-e846-2d004dda0a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.08 GB: 100%|██████████| 81/81 [00:02<00:00, 28.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 924.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 465.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 00, Loss: 3.7137, Train: 1.29%, Valid: 1.05% Test: 1.11%\n",
            "Epoch: 01, Loss: 3.6599, Train: 18.48%, Valid: 9.68% Test: 8.05%\n",
            "Epoch: 02, Loss: 3.6089, Train: 17.91%, Valid: 7.63% Test: 5.86%\n",
            "Epoch: 03, Loss: 3.5538, Train: 17.91%, Valid: 7.63% Test: 5.86%\n",
            "Epoch: 04, Loss: 3.4896, Train: 17.91%, Valid: 7.63% Test: 5.86%\n",
            "Epoch: 05, Loss: 3.4161, Train: 17.91%, Valid: 7.63% Test: 5.86%\n",
            "Epoch: 06, Loss: 3.3412, Train: 17.91%, Valid: 7.63% Test: 5.86%\n",
            "Epoch: 07, Loss: 3.2654, Train: 17.91%, Valid: 7.63% Test: 5.86%\n",
            "Epoch: 08, Loss: 3.2070, Train: 17.91%, Valid: 7.63% Test: 5.86%\n",
            "Epoch: 09, Loss: 3.1718, Train: 17.91%, Valid: 7.63% Test: 5.86%\n",
            "Epoch: 10, Loss: 3.1653, Train: 17.91%, Valid: 7.63% Test: 5.86%\n",
            "Epoch: 11, Loss: 3.1604, Train: 17.91%, Valid: 7.63% Test: 5.86%\n",
            "Epoch: 12, Loss: 3.1383, Train: 17.91%, Valid: 7.63% Test: 5.86%\n",
            "Epoch: 13, Loss: 3.1118, Train: 17.91%, Valid: 7.63% Test: 5.86%\n",
            "Epoch: 14, Loss: 3.0848, Train: 24.24%, Valid: 22.73% Test: 20.56%\n",
            "Epoch: 15, Loss: 3.0668, Train: 28.19%, Valid: 30.29% Test: 27.15%\n",
            "Epoch: 16, Loss: 3.0487, Train: 27.75%, Valid: 30.11% Test: 26.95%\n",
            "Epoch: 17, Loss: 3.0364, Train: 27.88%, Valid: 30.15% Test: 27.00%\n",
            "Epoch: 18, Loss: 3.0227, Train: 28.21%, Valid: 30.30% Test: 27.15%\n",
            "Epoch: 19, Loss: 3.0022, Train: 27.79%, Valid: 29.95% Test: 26.93%\n",
            "Epoch: 20, Loss: 2.9846, Train: 26.50%, Valid: 27.78% Test: 25.36%\n",
            "Epoch: 21, Loss: 2.9633, Train: 23.65%, Valid: 20.95% Test: 18.82%\n",
            "Epoch: 22, Loss: 2.9419, Train: 20.81%, Valid: 13.80% Test: 11.75%\n",
            "Epoch: 23, Loss: 2.9212, Train: 19.56%, Valid: 11.02% Test: 8.92%\n",
            "Epoch: 24, Loss: 2.9034, Train: 19.74%, Valid: 11.36% Test: 9.22%\n",
            "Epoch: 25, Loss: 2.8841, Train: 21.57%, Valid: 15.43% Test: 13.38%\n",
            "Epoch: 26, Loss: 2.8625, Train: 24.83%, Valid: 23.62% Test: 21.45%\n",
            "Epoch: 27, Loss: 2.8372, Train: 26.96%, Valid: 28.77% Test: 26.10%\n",
            "Epoch: 28, Loss: 2.8090, Train: 27.91%, Valid: 30.03% Test: 26.98%\n",
            "Epoch: 29, Loss: 2.7792, Train: 28.29%, Valid: 30.31% Test: 27.20%\n",
            "Epoch: 30, Loss: 2.7593, Train: 28.40%, Valid: 30.34% Test: 27.17%\n",
            "Epoch: 31, Loss: 2.7325, Train: 28.64%, Valid: 30.39% Test: 27.21%\n",
            "Epoch: 32, Loss: 2.7070, Train: 29.04%, Valid: 30.54% Test: 27.33%\n",
            "Epoch: 33, Loss: 2.6749, Train: 29.36%, Valid: 30.64% Test: 27.47%\n",
            "Epoch: 34, Loss: 2.6404, Train: 29.58%, Valid: 30.75% Test: 27.54%\n",
            "Epoch: 35, Loss: 2.6126, Train: 29.85%, Valid: 30.96% Test: 27.80%\n",
            "Epoch: 36, Loss: 2.5846, Train: 30.55%, Valid: 31.57% Test: 28.45%\n",
            "Epoch: 37, Loss: 2.5540, Train: 31.80%, Valid: 32.41% Test: 29.46%\n",
            "Epoch: 38, Loss: 2.5212, Train: 33.27%, Valid: 33.49% Test: 30.43%\n",
            "Epoch: 39, Loss: 2.4928, Train: 34.47%, Valid: 34.27% Test: 31.04%\n",
            "Epoch: 40, Loss: 2.4584, Train: 35.15%, Valid: 34.70% Test: 31.48%\n",
            "Epoch: 41, Loss: 2.4329, Train: 35.58%, Valid: 34.95% Test: 31.64%\n",
            "Epoch: 42, Loss: 2.4024, Train: 35.91%, Valid: 35.09% Test: 31.78%\n",
            "Epoch: 43, Loss: 2.3713, Train: 36.30%, Valid: 35.49% Test: 32.15%\n",
            "Epoch: 44, Loss: 2.3443, Train: 37.12%, Valid: 36.24% Test: 32.97%\n",
            "Epoch: 45, Loss: 2.3170, Train: 38.58%, Valid: 37.85% Test: 34.52%\n",
            "Epoch: 46, Loss: 2.2885, Train: 40.41%, Valid: 39.61% Test: 36.68%\n",
            "Epoch: 47, Loss: 2.2607, Train: 42.29%, Valid: 41.64% Test: 39.19%\n",
            "Epoch: 48, Loss: 2.2357, Train: 43.48%, Valid: 43.31% Test: 41.14%\n",
            "Epoch: 49, Loss: 2.2117, Train: 44.22%, Valid: 44.31% Test: 42.39%\n",
            "Epoch: 50, Loss: 2.1880, Train: 44.64%, Valid: 44.82% Test: 43.16%\n",
            "Epoch: 51, Loss: 2.1592, Train: 45.11%, Valid: 45.43% Test: 43.85%\n",
            "Epoch: 52, Loss: 2.1442, Train: 45.71%, Valid: 46.06% Test: 44.49%\n",
            "Epoch: 53, Loss: 2.1192, Train: 46.51%, Valid: 46.93% Test: 45.34%\n",
            "Epoch: 54, Loss: 2.0978, Train: 47.15%, Valid: 47.62% Test: 46.03%\n",
            "Epoch: 55, Loss: 2.0775, Train: 47.75%, Valid: 48.28% Test: 46.68%\n",
            "Epoch: 56, Loss: 2.0578, Train: 48.13%, Valid: 48.74% Test: 47.18%\n",
            "Epoch: 57, Loss: 2.0396, Train: 48.36%, Valid: 49.06% Test: 47.56%\n",
            "Epoch: 58, Loss: 2.0180, Train: 48.56%, Valid: 49.26% Test: 47.97%\n",
            "Epoch: 59, Loss: 2.0048, Train: 48.82%, Valid: 49.72% Test: 48.37%\n",
            "Epoch: 60, Loss: 1.9855, Train: 49.18%, Valid: 50.38% Test: 49.00%\n",
            "Epoch: 61, Loss: 1.9678, Train: 49.51%, Valid: 50.92% Test: 49.47%\n",
            "Epoch: 62, Loss: 1.9562, Train: 49.96%, Valid: 51.58% Test: 50.26%\n",
            "Epoch: 63, Loss: 1.9355, Train: 50.33%, Valid: 52.13% Test: 50.84%\n",
            "Epoch: 64, Loss: 1.9230, Train: 50.59%, Valid: 52.45% Test: 51.17%\n",
            "Epoch: 65, Loss: 1.9072, Train: 50.90%, Valid: 52.74% Test: 51.50%\n",
            "Epoch: 66, Loss: 1.8888, Train: 51.24%, Valid: 53.03% Test: 51.75%\n",
            "Epoch: 67, Loss: 1.8753, Train: 51.60%, Valid: 53.43% Test: 52.00%\n",
            "Epoch: 68, Loss: 1.8608, Train: 51.94%, Valid: 53.71% Test: 52.38%\n",
            "Epoch: 69, Loss: 1.8471, Train: 52.21%, Valid: 53.97% Test: 52.63%\n",
            "Epoch: 70, Loss: 1.8320, Train: 52.55%, Valid: 54.28% Test: 53.15%\n",
            "Epoch: 71, Loss: 1.8217, Train: 52.94%, Valid: 54.89% Test: 53.76%\n",
            "Epoch: 72, Loss: 1.8076, Train: 53.36%, Valid: 55.28% Test: 54.18%\n",
            "Epoch: 73, Loss: 1.7921, Train: 53.74%, Valid: 55.61% Test: 54.53%\n",
            "Epoch: 74, Loss: 1.7795, Train: 54.07%, Valid: 55.76% Test: 54.85%\n",
            "Epoch: 75, Loss: 1.7679, Train: 54.43%, Valid: 56.16% Test: 55.33%\n",
            "Epoch: 76, Loss: 1.7515, Train: 54.82%, Valid: 56.51% Test: 55.78%\n",
            "Epoch: 77, Loss: 1.7427, Train: 55.21%, Valid: 56.90% Test: 56.22%\n",
            "Epoch: 78, Loss: 1.7321, Train: 55.45%, Valid: 57.11% Test: 56.54%\n",
            "Epoch: 79, Loss: 1.7148, Train: 55.70%, Valid: 57.34% Test: 56.72%\n",
            "Epoch: 80, Loss: 1.7061, Train: 55.98%, Valid: 57.53% Test: 56.96%\n",
            "Epoch: 81, Loss: 1.6982, Train: 56.37%, Valid: 57.91% Test: 57.32%\n",
            "Epoch: 82, Loss: 1.6839, Train: 56.66%, Valid: 58.13% Test: 57.64%\n",
            "Epoch: 83, Loss: 1.6740, Train: 56.91%, Valid: 58.33% Test: 57.80%\n",
            "Epoch: 84, Loss: 1.6628, Train: 57.08%, Valid: 58.48% Test: 57.94%\n",
            "Epoch: 85, Loss: 1.6492, Train: 57.30%, Valid: 58.78% Test: 58.12%\n",
            "Epoch: 86, Loss: 1.6430, Train: 57.63%, Valid: 59.05% Test: 58.52%\n",
            "Epoch: 87, Loss: 1.6305, Train: 57.85%, Valid: 59.28% Test: 58.77%\n",
            "Epoch: 88, Loss: 1.6228, Train: 58.05%, Valid: 59.56% Test: 58.96%\n",
            "Epoch: 89, Loss: 1.6165, Train: 58.21%, Valid: 59.69% Test: 58.99%\n",
            "Epoch: 90, Loss: 1.6043, Train: 58.34%, Valid: 59.84% Test: 59.08%\n",
            "Epoch: 91, Loss: 1.5973, Train: 58.59%, Valid: 60.04% Test: 59.40%\n",
            "Epoch: 92, Loss: 1.5911, Train: 58.71%, Valid: 60.24% Test: 59.69%\n",
            "Epoch: 93, Loss: 1.5866, Train: 58.79%, Valid: 60.34% Test: 59.69%\n",
            "Epoch: 94, Loss: 1.5746, Train: 58.92%, Valid: 60.40% Test: 59.70%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-eac65accbb96>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLoadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mloss_obj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstandard_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-9b209c6fe9a0>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(load_data_obj, loss_obj, hyperparams)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mvalid_accs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-5eba104ceb3f>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, data, split_idx, evaluator)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madj_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Evaluate accuracy on training, validation, and test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataset with LM initialized embeddings\n",
        "\n",
        "We finetune an MPNet model on the ogbn-arxiv task using the title+abstract as an input feature. We then use the embeddings generated by this model as our initial node features for the GNN.\n",
        "\n",
        "The embeddings are available at https://drive.google.com/file/d/184qquWQuXbSog2PDZMG5xuWZU043hn3u/view?usp=sharing, please create a copy of this file in your Google Drive account and update the filepath in the code accordingly\n",
        "\n"
      ],
      "metadata": {
        "id": "894s0kfsAwTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filepath= \"/content/drive/Shareddrives/CS224W Project/graph_embeddings/finetuned/mpnet_arxiv.pkl\"\n",
        "embs = pickle.load(open(filepath, \"rb\"))\n"
      ],
      "metadata": {
        "id": "eRBezUs8Hfkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LoadDataLMInit(LoadData):\n",
        "    \"\"\"\n",
        "    A class extending LoadData to load graph data with initial node embeddings.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    embs : array-like\n",
        "        An array of node embeddings used to initialize the node features in the graph dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embs):\n",
        "        \"\"\"\n",
        "        The constructor for LoadDataLMInit class.\n",
        "\n",
        "        Initializes the LoadDataLMInit instance with the provided node embeddings.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        embs : array-like\n",
        "            An array-like structure containing node embeddings. Each element in the array\n",
        "            represents the embedding of a node in the graph.\n",
        "        \"\"\"\n",
        "        self.embs = embs\n",
        "        super(LoadDataLMInit, self).__init__()\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"\n",
        "        Loads the graph dataset with initial node embeddings, applies transformations, and retrieves split indices.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        data : PyG data object\n",
        "            The graph data object with initialized node features.\n",
        "        split_idx : dict\n",
        "            A dictionary containing the indices for train, validation, and test splits.\n",
        "        num_classes : int\n",
        "            The number of classes in the dataset.\n",
        "        \"\"\"\n",
        "        dataset = PygNodePropPredDataset(name=DATASET)\n",
        "        data = dataset[0]\n",
        "        embs = torch.nn.functional.normalize(torch.tensor(self.embs), dim=-1)\n",
        "        data.x = torch.tensor(self.embs)\n",
        "        split_idx = dataset.get_idx_split()\n",
        "        transform = T.Compose([T.ToUndirected(), T.ToSparseTensor()])\n",
        "        data = transform(data)\n",
        "        num_classes = dataset.num_classes\n",
        "        return data, split_idx, num_classes\n"
      ],
      "metadata": {
        "id": "orA5gxchAduL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the pipeline with LM initialized embeddings"
      ],
      "metadata": {
        "id": "GED4eoFrIZY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_data=LoadDataLMInit(embs)\n",
        "lminit_acc, lminit_model = train_loop(load_data, loss_obj, hyperparams)"
      ],
      "metadata": {
        "id": "q__q0nOdIeIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a428e9e-39fa-4de2-c713-6922d0bcde2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 00, Loss: 3.6924, Train: 1.83%, Valid: 3.60% Test: 4.25%\n",
            "Epoch: 01, Loss: 3.6616, Train: 4.53%, Valid: 9.02% Test: 9.91%\n",
            "Epoch: 02, Loss: 3.6309, Train: 30.15%, Valid: 39.30% Test: 37.35%\n",
            "Epoch: 03, Loss: 3.5958, Train: 46.82%, Valid: 51.64% Test: 50.88%\n",
            "Epoch: 04, Loss: 3.5544, Train: 50.87%, Valid: 54.65% Test: 54.58%\n",
            "Epoch: 05, Loss: 3.5045, Train: 52.80%, Valid: 56.15% Test: 56.55%\n",
            "Epoch: 06, Loss: 3.4433, Train: 52.78%, Valid: 56.52% Test: 57.17%\n",
            "Epoch: 07, Loss: 3.3713, Train: 52.14%, Valid: 56.24% Test: 57.15%\n",
            "Epoch: 08, Loss: 3.2855, Train: 51.21%, Valid: 55.72% Test: 56.90%\n",
            "Epoch: 09, Loss: 3.1847, Train: 50.23%, Valid: 55.10% Test: 56.39%\n",
            "Epoch: 10, Loss: 3.0684, Train: 49.26%, Valid: 54.51% Test: 56.00%\n",
            "Epoch: 11, Loss: 2.9403, Train: 48.30%, Valid: 54.01% Test: 55.61%\n",
            "Epoch: 12, Loss: 2.8038, Train: 47.47%, Valid: 53.41% Test: 55.29%\n",
            "Epoch: 13, Loss: 2.6633, Train: 46.81%, Valid: 52.72% Test: 54.75%\n",
            "Epoch: 14, Loss: 2.5287, Train: 46.13%, Valid: 51.73% Test: 53.99%\n",
            "Epoch: 15, Loss: 2.4108, Train: 45.43%, Valid: 50.53% Test: 52.82%\n",
            "Epoch: 16, Loss: 2.3195, Train: 44.88%, Valid: 49.63% Test: 51.59%\n",
            "Epoch: 17, Loss: 2.2556, Train: 44.73%, Valid: 49.02% Test: 50.42%\n",
            "Epoch: 18, Loss: 2.2093, Train: 45.00%, Valid: 48.59% Test: 49.11%\n",
            "Epoch: 19, Loss: 2.1702, Train: 45.56%, Valid: 48.16% Test: 47.76%\n",
            "Epoch: 20, Loss: 2.1333, Train: 46.59%, Valid: 48.42% Test: 47.44%\n",
            "Epoch: 21, Loss: 2.0910, Train: 48.04%, Valid: 49.53% Test: 48.23%\n",
            "Epoch: 22, Loss: 2.0434, Train: 50.02%, Valid: 51.33% Test: 50.01%\n",
            "Epoch: 23, Loss: 1.9928, Train: 52.42%, Valid: 53.45% Test: 52.31%\n",
            "Epoch: 24, Loss: 1.9393, Train: 54.81%, Valid: 55.89% Test: 55.01%\n",
            "Epoch: 25, Loss: 1.8812, Train: 57.42%, Valid: 58.51% Test: 57.70%\n",
            "Epoch: 26, Loss: 1.8262, Train: 59.50%, Valid: 60.56% Test: 60.10%\n",
            "Epoch: 27, Loss: 1.7806, Train: 60.90%, Valid: 62.08% Test: 61.66%\n",
            "Epoch: 28, Loss: 1.7358, Train: 62.02%, Valid: 63.24% Test: 62.83%\n",
            "Epoch: 29, Loss: 1.6931, Train: 63.09%, Valid: 64.15% Test: 63.63%\n",
            "Epoch: 30, Loss: 1.6558, Train: 64.10%, Valid: 64.86% Test: 64.25%\n",
            "Epoch: 31, Loss: 1.6163, Train: 64.74%, Valid: 65.28% Test: 64.69%\n",
            "Epoch: 32, Loss: 1.5874, Train: 65.17%, Valid: 65.62% Test: 64.98%\n",
            "Epoch: 33, Loss: 1.5461, Train: 65.58%, Valid: 65.97% Test: 65.23%\n",
            "Epoch: 34, Loss: 1.5143, Train: 65.97%, Valid: 66.24% Test: 65.52%\n",
            "Epoch: 35, Loss: 1.4817, Train: 66.45%, Valid: 66.52% Test: 65.77%\n",
            "Epoch: 36, Loss: 1.4459, Train: 66.79%, Valid: 66.85% Test: 66.00%\n",
            "Epoch: 37, Loss: 1.4152, Train: 67.09%, Valid: 67.16% Test: 66.31%\n",
            "Epoch: 38, Loss: 1.3895, Train: 67.36%, Valid: 67.46% Test: 66.65%\n",
            "Epoch: 39, Loss: 1.3587, Train: 67.61%, Valid: 67.69% Test: 66.90%\n",
            "Epoch: 40, Loss: 1.3354, Train: 67.88%, Valid: 67.87% Test: 67.21%\n",
            "Epoch: 41, Loss: 1.3099, Train: 68.11%, Valid: 68.14% Test: 67.54%\n",
            "Epoch: 42, Loss: 1.2862, Train: 68.38%, Valid: 68.39% Test: 67.76%\n",
            "Epoch: 43, Loss: 1.2621, Train: 68.77%, Valid: 68.61% Test: 67.98%\n",
            "Epoch: 44, Loss: 1.2393, Train: 69.14%, Valid: 68.88% Test: 68.27%\n",
            "Epoch: 45, Loss: 1.2211, Train: 69.48%, Valid: 69.17% Test: 68.50%\n",
            "Epoch: 46, Loss: 1.2021, Train: 69.76%, Valid: 69.44% Test: 68.73%\n",
            "Epoch: 47, Loss: 1.1839, Train: 70.05%, Valid: 69.70% Test: 68.87%\n",
            "Epoch: 48, Loss: 1.1679, Train: 70.35%, Valid: 69.94% Test: 69.08%\n",
            "Epoch: 49, Loss: 1.1542, Train: 70.65%, Valid: 70.12% Test: 69.26%\n",
            "Epoch: 50, Loss: 1.1407, Train: 70.95%, Valid: 70.38% Test: 69.54%\n",
            "Epoch: 51, Loss: 1.1238, Train: 71.23%, Valid: 70.65% Test: 69.82%\n",
            "Epoch: 52, Loss: 1.1137, Train: 71.56%, Valid: 70.89% Test: 70.06%\n",
            "Epoch: 53, Loss: 1.0981, Train: 71.83%, Valid: 71.14% Test: 70.29%\n",
            "Epoch: 54, Loss: 1.0873, Train: 72.36%, Valid: 71.50% Test: 70.53%\n",
            "Epoch: 55, Loss: 1.0776, Train: 72.82%, Valid: 71.75% Test: 70.79%\n",
            "Epoch: 56, Loss: 1.0698, Train: 73.13%, Valid: 71.88% Test: 70.93%\n",
            "Epoch: 57, Loss: 1.0556, Train: 73.42%, Valid: 71.99% Test: 71.06%\n",
            "Epoch: 58, Loss: 1.0479, Train: 73.59%, Valid: 72.02% Test: 71.20%\n",
            "Epoch: 59, Loss: 1.0399, Train: 73.74%, Valid: 72.14% Test: 71.33%\n",
            "Epoch: 60, Loss: 1.0323, Train: 73.90%, Valid: 72.23% Test: 71.42%\n",
            "Epoch: 61, Loss: 1.0238, Train: 74.01%, Valid: 72.30% Test: 71.48%\n",
            "Epoch: 62, Loss: 1.0163, Train: 74.13%, Valid: 72.41% Test: 71.62%\n",
            "Epoch: 63, Loss: 1.0047, Train: 74.25%, Valid: 72.51% Test: 71.71%\n",
            "Epoch: 64, Loss: 1.0003, Train: 74.34%, Valid: 72.59% Test: 71.79%\n",
            "Epoch: 65, Loss: 0.9951, Train: 74.45%, Valid: 72.65% Test: 71.88%\n",
            "Epoch: 66, Loss: 0.9886, Train: 74.55%, Valid: 72.71% Test: 71.94%\n",
            "Epoch: 67, Loss: 0.9826, Train: 74.67%, Valid: 72.76% Test: 72.03%\n",
            "Epoch: 68, Loss: 0.9767, Train: 74.77%, Valid: 72.89% Test: 72.12%\n",
            "Epoch: 69, Loss: 0.9710, Train: 74.88%, Valid: 72.96% Test: 72.20%\n",
            "Epoch: 70, Loss: 0.9677, Train: 74.99%, Valid: 73.05% Test: 72.27%\n",
            "Epoch: 71, Loss: 0.9594, Train: 75.12%, Valid: 73.10% Test: 72.35%\n",
            "Epoch: 72, Loss: 0.9558, Train: 75.26%, Valid: 73.17% Test: 72.45%\n",
            "Epoch: 73, Loss: 0.9503, Train: 75.36%, Valid: 73.26% Test: 72.50%\n",
            "Epoch: 74, Loss: 0.9447, Train: 75.48%, Valid: 73.32% Test: 72.61%\n",
            "Epoch: 75, Loss: 0.9412, Train: 75.56%, Valid: 73.39% Test: 72.72%\n",
            "Epoch: 76, Loss: 0.9388, Train: 75.61%, Valid: 73.49% Test: 72.82%\n",
            "Epoch: 77, Loss: 0.9331, Train: 75.68%, Valid: 73.53% Test: 72.93%\n",
            "Epoch: 78, Loss: 0.9292, Train: 75.79%, Valid: 73.64% Test: 72.97%\n",
            "Epoch: 79, Loss: 0.9270, Train: 75.89%, Valid: 73.70% Test: 73.07%\n",
            "Epoch: 80, Loss: 0.9230, Train: 75.97%, Valid: 73.72% Test: 73.12%\n",
            "Epoch: 81, Loss: 0.9167, Train: 76.06%, Valid: 73.79% Test: 73.21%\n",
            "Epoch: 82, Loss: 0.9138, Train: 76.12%, Valid: 73.84% Test: 73.32%\n",
            "Epoch: 83, Loss: 0.9112, Train: 76.19%, Valid: 73.93% Test: 73.38%\n",
            "Epoch: 84, Loss: 0.9045, Train: 76.27%, Valid: 73.95% Test: 73.43%\n",
            "Epoch: 85, Loss: 0.9030, Train: 76.35%, Valid: 74.00% Test: 73.48%\n",
            "Epoch: 86, Loss: 0.8969, Train: 76.43%, Valid: 74.06% Test: 73.53%\n",
            "Epoch: 87, Loss: 0.8968, Train: 76.51%, Valid: 74.12% Test: 73.60%\n",
            "Epoch: 88, Loss: 0.8899, Train: 76.58%, Valid: 74.19% Test: 73.67%\n",
            "Epoch: 89, Loss: 0.8873, Train: 76.65%, Valid: 74.27% Test: 73.73%\n",
            "Epoch: 90, Loss: 0.8851, Train: 76.71%, Valid: 74.32% Test: 73.80%\n",
            "Epoch: 91, Loss: 0.8792, Train: 76.77%, Valid: 74.35% Test: 73.86%\n",
            "Epoch: 92, Loss: 0.8742, Train: 76.83%, Valid: 74.37% Test: 73.93%\n",
            "Epoch: 93, Loss: 0.8778, Train: 76.90%, Valid: 74.45% Test: 73.97%\n",
            "Epoch: 94, Loss: 0.8735, Train: 76.95%, Valid: 74.54% Test: 73.99%\n",
            "Epoch: 95, Loss: 0.8695, Train: 77.00%, Valid: 74.60% Test: 74.00%\n",
            "Epoch: 96, Loss: 0.8677, Train: 77.07%, Valid: 74.61% Test: 74.04%\n",
            "Epoch: 97, Loss: 0.8644, Train: 77.12%, Valid: 74.69% Test: 74.09%\n",
            "Epoch: 98, Loss: 0.8608, Train: 77.18%, Valid: 74.79% Test: 74.12%\n",
            "Epoch: 99, Loss: 0.8579, Train: 77.23%, Valid: 74.84% Test: 74.16%\n",
            "Epoch: 100, Loss: 0.8533, Train: 77.29%, Valid: 74.89% Test: 74.19%\n",
            "Epoch: 101, Loss: 0.8511, Train: 77.36%, Valid: 74.89% Test: 74.25%\n",
            "Epoch: 102, Loss: 0.8482, Train: 77.41%, Valid: 74.92% Test: 74.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add virtual edges to the Dataset\n",
        "\n",
        "We attempt data augmentation by adding virtual edges, to capture relationships between nodes that have similar titles and abstracts. We add edges between each node and its k nearest neighbours based on the LLM embeddings of the MPNet model explained above. We use the [FAISS](https://github.com/facebookresearch/faiss) library for fast calculation of nearest neighbours."
      ],
      "metadata": {
        "id": "aJab3L_WTQTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "class LoadDataVirtualEdges(LoadData):\n",
        "  \"\"\"\n",
        "    A class extending LoadData to load graph data with additional virtual edges.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    embs : array-like\n",
        "        An array of node embeddings used to find nearest neighbors and create virtual edges.\n",
        "    k : int\n",
        "        The number of nearest neighbors to consider for creating virtual edges.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, embs, k):\n",
        "    self.embs=embs\n",
        "    self.k=k\n",
        "    super(LoadDataVirtualEdges, self).__init__()\n",
        "\n",
        "  def get_nearest_neighbours(self, embs, k):\n",
        "      \"\"\"\n",
        "      Computes the k-nearest neighbors for each node in the graph based on embeddings.\n",
        "\n",
        "      Parameters:\n",
        "      -----------\n",
        "      embs : array-like\n",
        "          An array-like structure containing node embeddings.\n",
        "      k : int\n",
        "          The number of nearest neighbors to find for each node.\n",
        "\n",
        "      Returns:\n",
        "      --------\n",
        "      numpy.ndarray\n",
        "          An array of indices representing the k-nearest neighbors for each node.\n",
        "      \"\"\"\n",
        "      res = faiss.StandardGpuResources()\n",
        "      index = faiss.IndexFlatL2(embs.shape[1])   # build the index\n",
        "      gpu_index_flat = faiss.index_cpu_to_gpu(res, 0, index)\n",
        "      gpu_index_flat.add(embs)                  # add vectors to the index\n",
        "      D, I = gpu_index_flat.search(embs, k+1)     # actual search\n",
        "      return I\n",
        "\n",
        "  def convert_to_edge_index(self, I, k):\n",
        "    \"\"\"\n",
        "    Converts nearest neighbor information into PyTorch Geometric edge indices.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    I : numpy.ndarray\n",
        "        An array of indices representing the nearest neighbors for each node.\n",
        "    k : int\n",
        "        The number of nearest neighbors for each node.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple: (src_nodes, dst_nodes)\n",
        "        Two tensors representing the source and destination nodes of each virtual edge.\n",
        "    \"\"\"\n",
        "    num_nodes = I.shape[0]\n",
        "    src_nodes = torch.arange(num_nodes).repeat_interleave(k).to(device)\n",
        "    dst_nodes = torch.tensor(I[:,1:]).flatten().to(device)\n",
        "    return src_nodes, dst_nodes\n",
        "\n",
        "  def load_data(self):\n",
        "    \"\"\"\n",
        "    Loads the graph dataset, adds virtual edges based on nearest neighbors, applies transformations,\n",
        "    and retrieves split indices.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    data : PyG data object\n",
        "        The graph data object with virtual edges added.\n",
        "    split_idx : dict\n",
        "        A dictionary containing the indices for train, validation, and test splits.\n",
        "    num_classes : int\n",
        "        The number of classes in the dataset.\n",
        "    \"\"\"\n",
        "    dataset = PygNodePropPredDataset(name='ogbn-arxiv')\n",
        "    data = dataset[0]\n",
        "    I=self.get_nearest_neighbours(self.embs,self.k)\n",
        "    S,D=self.convert_to_edge_index(I,k)\n",
        "    data.edge_index=torch.cat((data.edge_index,torch.stack((S,D)).to('cpu')),dim=1)\n",
        "    transform=T.Compose([T.ToUndirected(), T.ToSparseTensor()])\n",
        "    data=transform(data)\n",
        "    num_classes = dataset.num_classes\n",
        "    split_idx = dataset.get_idx_split()\n",
        "    return data, split_idx, num_classes"
      ],
      "metadata": {
        "id": "G9XSkQwHUtbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the pipeline with Virtual Edges added"
      ],
      "metadata": {
        "id": "ryrzlxSaWDCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k=4\n",
        "load_data=LoadDataVirtualEdges(embs,k)\n",
        "virtual_edge_acc, virtual_edge_model = train_loop(load_data, loss_obj, hyperparams)"
      ],
      "metadata": {
        "id": "IkDoMDTHV_9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add KL Divergence Regularization\n",
        "\n",
        "KLLoss extends the basic loss computation (like cross-entropy) by adding a regularization term\n",
        "    based on KL divergence. This is useful in scenarios where one wishes to penalize the divergence\n",
        "    between the model's output distribution and a target distribution, which in this case is provided\n",
        "    by 'lm_logits'. The lambda (lmbda) parameter controls the weight of this regularization term.\n",
        "\n",
        "\n",
        "The MPNET logits are available at https://drive.google.com/file/d/13yY-y7FEpFhe2lOOVX-LMZy58oEFxTS4/view?usp=sharing, please create a copy of this file in your Google Drive account and update the filepath in the code accordingly\n"
      ],
      "metadata": {
        "id": "sN79xLSueMbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filepath_logits='/content/drive/Shareddrives/CS224W Project/graph_embeddings/finetuned/mpnet_logits_arxiv.pkl'\n",
        "lm_logits=pickle.load(open(filepath_logits, \"rb\"))"
      ],
      "metadata": {
        "id": "jlN3AA6IeiyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KLLoss(Loss):\n",
        "    \"\"\"\n",
        "    A class extending Loss to incorporate Kullback-Leibler (KL) divergence as a regularization term.\n",
        "\n",
        "    Attributes:\n",
        "    -----------\n",
        "    lm_logits : torch.Tensor\n",
        "        The logits from a language model or a pre-defined target distribution.\n",
        "    lmbda : float\n",
        "        The weight (lambda) of the KL divergence regularization term in the overall loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lm_logits, lmbda):\n",
        "        \"\"\"\n",
        "        The constructor for KLLoss class.\n",
        "\n",
        "        Initializes the KLLoss instance with provided language model logits and lambda value.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        lm_logits : torch.Tensor\n",
        "            The logits from a language model or a pre-defined target distribution.\n",
        "        lmbda : float\n",
        "            The weight (lambda) of the KL divergence regularization term in the overall loss.\n",
        "        \"\"\"\n",
        "        self.lmbda = lmbda\n",
        "        self.lm_logits = lm_logits\n",
        "        super(KLLoss, self).__init__()\n",
        "\n",
        "    def get_loss(self, out, labels, train_idx):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        -----------\n",
        "        out : torch.Tensor\n",
        "            The output predictions from the neural network model, typically the logits.\n",
        "        labels : torch.Tensor\n",
        "            The true labels for the training data.\n",
        "        train_idx : torch.Tensor or list\n",
        "            The indices of the training data samples.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        torch.Tensor\n",
        "            The computed loss, combining cross-entropy and KL divergence.\n",
        "        \"\"\"\n",
        "        loss = F.cross_entropy(out, labels[train_idx])\n",
        "        reg_penalty = torch.nn.KLDivLoss(log_target=True)\n",
        "        reg = reg_penalty(F.log_softmax(out, dim=1), F.log_softmax(torch.tensor(self.lm_logits[train_idx]).to(device), dim=1))\n",
        "        loss += reg * self.lmbda\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "yyJFavYJeZb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lmbda=1\n",
        "load_data=LoadData()\n",
        "kl_loss_obj=KLLoss(lm_logits,lmbda)\n",
        "kl_best_acc = train_loop(load_data, kl_loss_obj, hyperparams)"
      ],
      "metadata": {
        "id": "pYdJ2QhafNQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensembling\n",
        "\n",
        "We demonstrate an ensemble approach combining a base graph neural network model with scaled logits from a language model.\n",
        "\n"
      ],
      "metadata": {
        "id": "DvdH3QLEiSFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data,split_idx,_=LoadData().load_data()\n",
        "data.to(device)\n",
        "base_model.to(device)\n",
        "final_out = base_model(data.x, data.adj_t)\n",
        "lm_logits = torch.tensor(lm_logits).to(device)\n",
        "max_test_acc=0\n",
        "max_i=0\n",
        "for i in [0.2,0.4,0.6,0.8,1]:\n",
        "  final_out = final_out + i*lm_logits\n",
        "  y_pred = final_out.argmax(dim=-1, keepdim=True)\n",
        "  evaluator = Evaluator(name=DATASET)\n",
        "  test_acc = evaluator.eval({\n",
        "          'y_true': data.y[split_idx['test']],\n",
        "          'y_pred': y_pred[split_idx['test']],\n",
        "      })['acc']\n",
        "  if test_acc>max_test_acc:\n",
        "    max_test_acc=test_acc\n",
        "    max_i=i\n",
        "print(f'Max test acc: {max_test_acc}')"
      ],
      "metadata": {
        "id": "W5oWwEBMiVSZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}