{"cells":[{"cell_type":"markdown","source":["# Dynamic Ensembling for Knowledge Graph Completion"],"metadata":{"id":"lno37WJ5D98u"},"id":"lno37WJ5D98u"},{"cell_type":"markdown","source":["This notebook can be used to obtain results for dynamic ensembling of SimKGC and RotatE on the WN18RR and CoDex-M datasets."],"metadata":{"id":"8Y3kzh7UEHEN"},"id":"8Y3kzh7UEHEN"},{"cell_type":"markdown","source":["## Import PyG and other required libraries"],"metadata":{"id":"94xt7tiYEc71"},"id":"94xt7tiYEc71"},{"cell_type":"code","execution_count":null,"id":"0f0d9d2c","metadata":{"id":"0f0d9d2c"},"outputs":[],"source":["import os.path as osp\n","import torch\n","import torch.optim as optim\n","torch_version = str(torch.__version__)\n","scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n","sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n","!pip install torch-scatter -f $scatter_src\n","!pip install torch-sparse -f $sparse_src\n","!pip install torch-geometric\n","!pip install ogb\n","!pip install faiss-gpu\n","from torch_geometric.datasets import WordNet18RR\n","from torch_geometric.nn import RotatE\n","from tqdm import tqdm\n","import pickle as pkl\n","from torch_geometric.data import Data\n","import torch.nn as nn\n","from torch_geometric.utils import index_sort\n","import json\n","from itertools import chain\n","import pickle"]},{"cell_type":"markdown","source":["## Link the Colab to your Google Drive\n","\n","We use Google Drive to load datasets and embeddings."],"metadata":{"id":"_YUCOZKlHqPy"},"id":"_YUCOZKlHqPy"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive/\")"],"metadata":{"id":"Ubd42LTCHpy8"},"execution_count":null,"outputs":[],"id":"Ubd42LTCHpy8"},{"cell_type":"markdown","source":["## Load all Global Variables"],"metadata":{"id":"3sLgxglHEvn5"},"id":"3sLgxglHEvn5"},{"cell_type":"markdown","source":["For this colab, we need to load trained SimKGC embeddings, entity and relation mappings for SimKGC, as well as the trained RotatE model.\n","\n","1. Please change DATA_ROOT to the path to the SimKGC version of your dataset. For WN18RR, this is available at https://drive.google.com/drive/folders/1tnYam16BA1IHYDCQgzqcoRLuBOkizSu2?usp=sharing. For CoDex, this is available at https://drive.google.com/drive/folders/1MUnSq7ENTIV7nah3IiCqfV8Wb3McAgWc?usp=drive_link.\n","2. Please change ROTATE_ROOT to the path to your trained RotatE model.\n","3. Please change SIMKGC_DUMP to the path to the SimKGC embeddings. For WN18RR, this is available at https://drive.google.com/drive/folders/1R0c5W0ofiznQQ8mjsJj0SUohFWrpOc_C?usp=sharing. For CoDex, this is available at https://drive.google.com/drive/folders/1PfXRgT8xzxXXhXoc81M86f1D9q67JFNc?usp=sharing."],"metadata":{"id":"UKT71L41E6rd"},"id":"UKT71L41E6rd"},{"cell_type":"code","execution_count":null,"id":"cfaafcdd","metadata":{"id":"cfaafcdd"},"outputs":[],"source":["DATASET = 'WN18RR' # Either CodexM or WN18RR\n","DATA_ROOT = \"/content/drive/Shareddrives/CS224W Project/WN18RR/\" # Path to the SimKGC version of the dataset\n","ROTATE_ROOT = \"/content/drive/Shareddrives/CS224W Project/kgc_models/wn18rr.mdl\" # Path to the dumped RotatE model\n","SIMKGC_DUMP = \"/content/drive/Shareddrives/CS224W Project/WN18RR_Vectors\" # Path to the dumped SimKGC embeddings\n","CHANNEL = 500 # Hidden dimensionality for RotatE\n","MARGIN = 9.0 # Margin for RotatE score computation\n","EPOCHS = 2 # Number of epochs of training for the dynamic ensemble\n","LR = 5.0e-4 # Learning rate for training the dynamic ensemble\n","BATCH_SIZE = 64 # Batch size for training the dynamic ensemble\n","NUM_NEGATIVES = 2000 # Number of negative samples per training example"]},{"cell_type":"markdown","source":["## Dataset Loading"],"metadata":{"id":"G3AoTp8fHMft"},"id":"G3AoTp8fHMft"},{"cell_type":"code","execution_count":null,"id":"6196e3ae","metadata":{"id":"6196e3ae"},"outputs":[],"source":["# The WN18RR relation mappings for PyG\n","\n","WN18RR_E2ID = {\n","        '_also_see': 0,\n","        '_derivationally_related_form': 1,\n","        '_has_part': 2,\n","        '_hypernym': 3,\n","        '_instance_hypernym': 4,\n","        '_member_meronym': 5,\n","        '_member_of_domain_region': 6,\n","        '_member_of_domain_usage': 7,\n","        '_similar_to': 8,\n","        '_synset_domain_topic_of': 9,\n","        '_verb_group': 10,\n","    }"]},{"cell_type":"code","execution_count":null,"id":"665e503e","metadata":{"id":"665e503e"},"outputs":[],"source":["'''\n","For this setting, we load both CoDex-M and WN18RR manually, without relying on PyG.\n","This is because we need the entity id and relation id mappings to align the SimKGC embeddings to the RotatE embeddings\n","'''\n","\n","device = f'cuda' if torch.cuda.is_available() else 'cpu'\n","device = torch.device(device)\n","\n","node2id, rel2id, idx, relidx = {}, {}, 0, 0\n","\n","srcs, dsts, edge_types = [], [], []\n","for file in [\"train.txt\", \"valid.txt\", \"test.txt\"]:\n","    with open(DATA_ROOT + file, 'r') as f:\n","        data = f.read().split()\n","\n","        src = data[::3]\n","        dst = data[2::3]\n","        edge_type = data[1::3]\n","\n","        for i in chain(src, dst):\n","            if i not in node2id:\n","                node2id[i] = idx\n","                idx += 1\n","\n","        if (DATASET == 'CodexM'):\n","            for i in edge_type:\n","                if i not in rel2id:\n","                    rel2id[i] = relidx\n","                    relidx += 1\n","        else:\n","            rel2id = WN18RR_E2ID\n","\n","        src = [node2id[i] for i in src]\n","        dst = [node2id[i] for i in dst]\n","        edge_type = [rel2id[i] for i in edge_type]\n","\n","        srcs.append(torch.tensor(src, dtype=torch.long))\n","        dsts.append(torch.tensor(dst, dtype=torch.long))\n","        edge_types.append(torch.tensor(edge_type, dtype=torch.long))\n","\n","src = torch.cat(srcs, dim=0)\n","dst = torch.cat(dsts, dim=0)\n","edge_type = torch.cat(edge_types, dim=0)\n","\n","train_mask = torch.zeros(src.size(0), dtype=torch.bool)\n","train_mask[:srcs[0].size(0)] = True\n","val_mask = torch.zeros(src.size(0), dtype=torch.bool)\n","val_mask[srcs[0].size(0):srcs[0].size(0) + srcs[1].size(0)] = True\n","test_mask = torch.zeros(src.size(0), dtype=torch.bool)\n","test_mask[srcs[0].size(0) + srcs[1].size(0):] = True\n","\n","num_nodes = max(int(src.max()), int(dst.max())) + 1\n","_, perm = index_sort(num_nodes * src + dst)\n","\n","edge_index = torch.stack([src[perm], dst[perm]], dim=0)\n","edge_type = edge_type[perm]\n","train_mask = train_mask[perm]\n","val_mask = val_mask[perm]\n","test_mask = test_mask[perm]\n","\n","data = Data(edge_index=edge_index, edge_type=edge_type,\n","            train_mask=train_mask, val_mask=val_mask,\n","            test_mask=test_mask, num_nodes=num_nodes)\n","\n","data"]},{"cell_type":"code","execution_count":null,"id":"aa77d30c","metadata":{"id":"aa77d30c"},"outputs":[],"source":["# To compute the evaluation metrics in the filtered setting, we need to compute neighbours for all nodes in the knowledge graph.\n","\n","neighbours = [[set() for _ in range(2*data.num_edge_types)] for _ in range(data.num_nodes)]\n","\n","for idx in tqdm(range(len(data.edge_type))):\n","    '''\n","    We will add a (t,r^{-1},h) edge for all (h,r,t) in the graph later. The relation ID for r^{-1} is data.num_edge_types + the relation ID for r.\n","    Therefore, if t is connected to h through the relation r, h will be connected to t through the relation r^{-1}.\n","    '''\n","    neighbours[data.edge_index[0, idx].item()][data.edge_type[idx].item()].add(data.edge_index[1, idx].item())\n","    neighbours[data.edge_index[1, idx].item()][data.num_edge_types + data.edge_type[idx].item()].add(data.edge_index[0, idx].item())"]},{"cell_type":"markdown","source":["## Load the RotatE model from disk"],"metadata":{"id":"GldZP6u5IEte"},"id":"GldZP6u5IEte"},{"cell_type":"code","execution_count":null,"id":"9b853dd7","metadata":{"id":"9b853dd7"},"outputs":[],"source":["'''\n","We will model (?,r,t) queries as (t,r^{-1},?) queries.\n","Since we have the inverse relation r^{-1} for each edge r, we double the number of relations below.\n","'''\n","\n","model = RotatE(\n","            num_nodes=data.num_nodes,\n","            num_relations=2*data.num_edge_types,\n","            hidden_channels=CHANNEL,\n","            margin=MARGIN\n","        ).to(device)\n","\n","model.load_state_dict(torch.load(ROTATE_ROOT))\n","model.eval()"]},{"cell_type":"markdown","source":["## Create mapping of SimKGC and RotatE entity and relation IDs"],"metadata":{"id":"gb47mWEoJBzv"},"id":"gb47mWEoJBzv"},{"cell_type":"code","execution_count":null,"id":"fdb85b28","metadata":{"id":"fdb85b28"},"outputs":[],"source":["'''\n","SimKGC and RotatE are trained on different versions of the datasets.\n","Therefore, we create a mapping between entity and relation IDs from these different versions here\n","'''\n","\n","ent = json.load(open(DATA_ROOT + 'entities.json', 'r'))\n","siment = [_[\"entity_id\"] for _ in ent]\n","siment = {siment[_]:_ for _ in range(len(siment))}\n","nbf2rnnent = {}\n","for _ in node2id:\n","    nbf2rnnent[node2id[_]] = siment[_]\n","\n","rel = json.load(open(DATA_ROOT + 'relations.json', 'r'))\n","simrel = list(rel.keys())\n","simrel = {simrel[_]:_ for _ in range(len(simrel))}\n","nbf2rnnrel = {}\n","dump_idxs = len(rel2id)\n","for _ in simrel:\n","    if _ in rel2id:\n","        nbf2rnnrel[rel2id[_]] = simrel[_]\n","        nbf2rnnrel[len(simrel) + rel2id[_]] = len(simrel) + simrel[_]\n","    else:\n","        nbf2rnnrel[dump_idxs] = simrel[_]\n","        nbf2rnnrel[len(simrel) + dump_idxs] = len(simrel) + simrel[_]\n","        dump_idxs += 1"]},{"cell_type":"markdown","source":["## Load SimKGC embeddings"],"metadata":{"id":"9QNTrFroKHBM"},"id":"9QNTrFroKHBM"},{"cell_type":"code","execution_count":null,"id":"62a5ba79","metadata":{"id":"62a5ba79"},"outputs":[],"source":["# Instantiate SimKGC class and load embeddings\n","\n","class SimKGC():\n","    \"\"\"\n","    A class for loading SimKGC embeddings and computing SimKGC scores\n","\n","    \"\"\"\n","    def __init__(self, nbf2rnnent, nbf2rnnrel):\n","        # Initialize the class using the id mappings created above\n","\n","        self.h_embs = []\n","        self.re_index = [nbf2rnnent[_] for _ in range(len(nbf2rnnent))]\n","        self.re_rel = [nbf2rnnrel[_] for _ in range(len(nbf2rnnrel))]\n","\n","        # Load tail embeddings\n","        t_file = f'{SIMKGC_DUMP}/SimKGC_t_rep.pkl'\n","        t_emb = pickle.load(open(t_file, 'rb'))\n","        t_emb = t_emb[self.re_index, :]\n","        self.t_emb = t_emb.t().cpu()\n","        print(self.t_emb.shape)\n","        print(f'Loaded embeddings for tail')\n","\n","        # Load (head,relation) embeddings\n","        for _ in self.re_rel:\n","            print(f'Loading for relation {_}')\n","            h_file = f'{SIMKGC_DUMP}/SimKGC_h_{_}_rep.pkl'\n","            h_emb = pickle.load(open(h_file, 'rb'))\n","            h_emb = h_emb[self.re_index, :]\n","            self.h_embs.append(h_emb.cpu())\n","            print(f'Loaded embeddings for relation {_}')\n","        self.h_embs = torch.stack(self.h_embs).cpu()\n","        print(self.h_embs.shape)\n","\n","    def get_h(self, h, r):\n","        \"\"\"\n","        Computes the SimKGC score from a vector of head and relation IDs for all entities in the dataset as tails\n","\n","        Parameters:\n","        -----------\n","        h : torch.Tensor\n","            Head IDs\n","        r : torch.Tensor\n","            Relation IDs\n","\n","        Returns:\n","        --------\n","        torch.Tensor\n","            Batched score distribution for each query.\n","        \"\"\"\n","        head = h\n","        rel = r\n","        h_req = self.h_embs[rel, head]\n","        t_req = self.t_emb\n","        result = torch.matmul(h_req, t_req)\n","        return result\n","\n","simkgc = SimKGC(nbf2rnnent, nbf2rnnrel)"]},{"cell_type":"markdown","source":["## Create Class for Dynamic Ensemble Computation"],"metadata":{"id":"hATfhji8L7ul"},"id":"hATfhji8L7ul"},{"cell_type":"code","execution_count":null,"id":"efd82d1d","metadata":{"id":"efd82d1d"},"outputs":[],"source":["import json\n","import numpy as np\n","\n","class Selector(nn.Module):\n","    \"\"\"\n","    A class for learning a dynamic ensemble of base KGC models.\n","\n","    \"\"\"\n","    def __init__(self, simkgc, rotate, num_layers, hidden_dims, input_dim):\n","        # Initialize the class with the base models and the hyperparameters for the MLP\n","        super(Selector, self).__init__()\n","\n","        self.simkgc = simkgc\n","        self.rotate = rotate\n","        self.rotate.requires_grad_(False)\n","\n","        mlp = []\n","        # Input is a tensor containing the distribution of scores from the base model over all candidate tail entities\n","        mlp.append(nn.Linear(input_dim, hidden_dims[0]))\n","        for i in range(num_layers - 1):\n","            layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n","            mlp.append(layer)\n","            mlp.append(nn.ReLU())\n","        # Output is the ensemble weight\n","        mlp.append(nn.Linear(hidden_dims[-1], 1))\n","        self.mlp = nn.Sequential(*mlp)\n","\n","    def get_features_and_normalize(self, score):\n","        # Max-min normalize the score distribution\n","        pre_mins = torch.amin(score, dim=1).unsqueeze(dim = -1)\n","        score = (score - pre_mins)\n","        maxs = torch.amax(score, dim=1).unsqueeze(dim = -1)\n","        score = score/maxs\n","        # Compute the mean of the distribution\n","        means = torch.mean(score, dim=1).unsqueeze(dim = -1)\n","        # Compute the mean of the top 10 scores in the distribution\n","        topk = torch.mean(torch.topk(score, dim=1, k=10, largest=True).values, dim=1).unsqueeze(dim = -1)\n","        return score, [mdiffs, topk]\n","\n","    def get_rotate_scores(self, h_index, r_index):\n","        # Run the PyG RotatE model on the given (h,r) pairs to get the score distribution\n","        arange = range(h_index.numel())\n","        tot_scores = []\n","\n","        for i in arange:\n","            h, r = h_index[i], r_index[i]\n","            scores = []\n","            tail_indices = torch.arange(len(nbf2rnnent), device=h.device)\n","            for ts in tail_indices.split(20_000):\n","                scores.append(self.rotate(h.expand_as(ts), r.expand_as(ts), ts))\n","            scores = torch.cat(scores)\n","            tot_scores.append(scores.unsqueeze(0))\n","\n","        tot_scores = torch.cat(tot_scores, dim=0)\n","        return tot_scores\n","\n","    def forward(self, h_index, r_index):\n","        # Accumulate the distribution features from all base models\n","        features = []\n","\n","        # Get the SimKGC scores and features\n","        simkgc_score = self.simkgc.get_h(h_index.cpu(), r_index.cpu()).to(h_index.device)\n","        simkgc_score, feature = self.get_features_and_normalize(simkgc_score)\n","        features += feature\n","\n","        # Get the RotatE scores and features\n","        rotate_score = self.get_rotate_scores(h_index, r_index)\n","        rotate_score, feature = self.get_features_and_normalize(rotate_score)\n","        features += feature\n","\n","        # Concatenate features to get input tensor for MLP\n","        mlp_in = torch.cat(tuple(features), dim = 1)\n","        weight = self.mlp(mlp_in.to(h_index.device))\n","\n","        # Return the score distribution after dynamic ensembling\n","        return simkgc_score + weight*rotate_score"]},{"cell_type":"code","source":["# Instantiate the Selector class with the default hyperparameters\n","\n","selector = Selector(simkgc, model, 2, [32,32], 4)"],"metadata":{"id":"23TIcRPbNKSS"},"id":"23TIcRPbNKSS","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create Dataloader"],"metadata":{"id":"Z7CY2-5-N44b"},"id":"Z7CY2-5-N44b"},{"cell_type":"code","execution_count":null,"id":"c48c3524","metadata":{"id":"c48c3524"},"outputs":[],"source":["# Create Dataloader\n","\n","# (t,r^{-1},h) edges are added for all (h,r,t) in the graph\n","\n","loader = model.loader(\n","            head_index=torch.cat([data.edge_index[0, data.val_mask],data.edge_index[1, data.val_mask]], dim=0),\n","            rel_type=torch.cat([data.edge_type[data.val_mask], data.num_edge_types+data.edge_type[data.val_mask]], dim=0),\n","            tail_index=torch.cat([data.edge_index[1, data.val_mask], data.edge_index[0, data.val_mask]], dim=0),\n","            batch_size=BATCH_SIZE,\n","            shuffle=True\n","        )"]},{"cell_type":"markdown","source":["## Function to run the training loop"],"metadata":{"id":"Vy0d_kBjORE3"},"id":"Vy0d_kBjORE3"},{"cell_type":"code","execution_count":null,"id":"8bec40f3","metadata":{"id":"8bec40f3"},"outputs":[],"source":["def train(selector, loader, optimizer):\n","    # Run the training loop for one epoch\n","    selector.train()\n","\n","    for head_index, rel_type, tail_index in tqdm(loader):\n","        optimizer.zero_grad()\n","        pred = selector(head_index.to(device), rel_type.to(device))\n","\n","        req_scores = pred.gather(1, tail_index.unsqueeze(0).to(device)).squeeze().unsqueeze(1)\n","        # Negative sampling\n","        sampled_negatives = torch.randperm(pred.size(dim=-1))[:NUM_NEGATIVES].to(device)\n","        sampled_scores = pred[:, sampled_negatives]\n","\n","        # Compute loss and backpropagate\n","        loss = nn.CrossEntropyLoss()\n","        loss_input = torch.cat([req_scores, sampled_scores], dim = 1)\n","        loss_target = torch.zeros(pred.size(dim=0)).long()\n","        final_loss = loss(loss_input, loss_target.to(device))\n","        final_loss.backward()\n","        optimizer.step()"]},{"cell_type":"markdown","source":["## Function to run testing"],"metadata":{"id":"ch4IMB7ZOjyP"},"id":"ch4IMB7ZOjyP"},{"cell_type":"code","execution_count":null,"id":"86f41664","metadata":{"id":"86f41664"},"outputs":[],"source":["@torch.no_grad()\n","def test(selector, data):\n","    '''\n","    Testing in the filtered setting according to (Bordes et al, 2013)[https://proceedings.neurips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf\n","    '''\n","\n","    selector.eval()\n","    # Augment the testing data with inverse relations\n","    head_index=torch.cat([data.edge_index[0, data.test_mask],data.edge_index[1, data.test_mask]], dim=0).to(device)\n","    rel_type=torch.cat([data.edge_type[data.test_mask], data.num_edge_types+data.edge_type[data.test_mask]], dim=0).to(device)\n","    tail_index=torch.cat([data.edge_index[1, data.test_mask], data.edge_index[0, data.test_mask]], dim=0).to(device)\n","    arange = range(head_index.numel())\n","    arange = tqdm(arange)\n","\n","    mean_ranks, reciprocal_ranks, hits_at_1, hits_at_10 = [], [], [], []\n","    for i in arange:\n","        h, r, t = head_index[i], rel_type[i], tail_index[i]\n","        # Get dynamic ensembling scores for all entities in the dataset as tails\n","        flattened_scores = selector(torch.tensor([h]).to(device),torch.tensor([r]).to(device)).squeeze()\n","\n","        # Filter out neighbours from candidate tails\n","        curr_neighbours = list(neighbours[h.item()][r.item()])\n","        mask_indices = []\n","        for e_id in curr_neighbours:\n","            if e_id == t.item():\n","                continue\n","            mask_indices.append(e_id)\n","        mask_indices = torch.LongTensor(mask_indices).to(device)\n","        flattened_scores.index_fill_(0, mask_indices, -1).cpu()\n","\n","        # Compute rank, mrr and hits@k\n","        rank = int((flattened_scores.argsort(\n","                descending=True) == t).nonzero().view(-1))\n","        mean_ranks.append(rank)\n","        reciprocal_ranks.append(1 / (rank + 1))\n","        hits_at_1.append(rank < 1)\n","        hits_at_10.append(rank < 10)\n","\n","    # Accumulate results from all queries\n","    mean_rank = float(torch.tensor(mean_ranks, dtype=torch.float).mean())\n","    mrr = float(torch.tensor(reciprocal_ranks, dtype=torch.float).mean())\n","    hits_at_1 = int(torch.tensor(hits_at_1).sum()) / len(hits_at_1)\n","    hits_at_10 = int(torch.tensor(hits_at_10).sum()) / len(hits_at_10)\n","    print(mean_rank, mrr, hits_at_1, hits_at_10)"]},{"cell_type":"markdown","source":["## Run the training loop!"],"metadata":{"id":"D2UdMVaTO_Kg"},"id":"D2UdMVaTO_Kg"},{"cell_type":"code","source":["# Initialize the Optimizer\n","\n","selector.to(device)\n","optimizer = optim.Adam(selector.parameters(), lr=LR)"],"metadata":{"id":"juCRaX1oONy9"},"id":"juCRaX1oONy9","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"249a207c","metadata":{"id":"249a207c"},"outputs":[],"source":["# Training loop\n","\n","for ep in range(EPOCHS):\n","    train(selector, loader, optimizer)\n","    test(selector, data)"]}],"metadata":{"kernelspec":{"display_name":"Python [conda env:ts] *","language":"python","name":"conda-env-ts-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}